{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-BcG1g4QbE_"
      },
      "source": [
        "# Cloning the Repository (pc-gym)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxWOGAsaECz2",
        "outputId": "899c63bc-c202-4dbc-eb5e-9de200616a0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Volume in drive C is Windows \n",
            " Volume Serial Number is F0B8-32A3\n",
            "\n",
            " Directory of c:\\Users\\Usuario\\Desktop\\imperial_projects\\VSCode\\pcgym2\\train\\base\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "File Not Found\n"
          ]
        }
      ],
      "source": [
        "!dir pc-gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVv4nhKnJZyQ",
        "outputId": "0c6c2f7d-37db-4485-a590-41299d74c51c"
      },
      "outputs": [],
      "source": [
        "# %cd pc-gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pXeY_oDVTPZ"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhR6YfBEbVRM",
        "outputId": "a67140ae-cfff-4a16-fbd8-dc7974929319"
      },
      "outputs": [],
      "source": [
        "# %cd src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %cd pcgym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Usuario\\Desktop\\imperial_projects\\VSCode\\pcgym2\\pc-gym\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\Desktop\\imperial_projects\\VSCode\\pcgym2\\new_venv\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "%cd C:/Users/Usuario/Desktop/imperial_projects/VSCode/pcgym2/pc-gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0xr48fylffH",
        "outputId": "504b6be1-dc60-4c93-f087-158249487b25"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "from stable_baselines3 import PPO,SAC,DDPG,TD3\n",
        "import pcgym\n",
        "from pcgym import make_env\n",
        "import jax.numpy as jnp\n",
        "#Global params\n",
        "T = 26\n",
        "nsteps =60\n",
        "# Global seed for reproducibility\n",
        "seed = 1990"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMPAn1SRd32f"
      },
      "source": [
        "# Saving and loading\n",
        "\n",
        "Saving and loading stable-baselines models is straightforward: you can directly call `.save()` and `.load()` on the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mph7ddv8iCjY",
        "outputId": "fc007c61-f2cd-4545-d8ee-15f900a0eb6c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Create save dir\n",
        "save_dir = \"./max/ddpg\"\n",
        "os.makedirs(save_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPlCXGwWFPTI"
      },
      "source": [
        "# Monitoring experiments with W&B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8sM9B5fOG_w",
        "outputId": "6c14e127-7339-4421-9a3b-f4c37cd16d6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Usuario\\.netrc\n"
          ]
        }
      ],
      "source": [
        "!wandb login 84af17cc9914cf1736f3a8e2733a2f361e4750bb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sWhTer-Wg7X"
      },
      "source": [
        "# 1.1 Reactor Case Study"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZklHwMtzkOAE",
        "outputId": "209ef879-e2cb-48ac-ce44-0c2300707462"
      },
      "outputs": [],
      "source": [
        "# @title Function to log the performance data\n",
        "def log_performance(performance, test_label, file_path):\n",
        "    with open(file_path, \"a\") as file:\n",
        "        file.write(f\"{test_label}: \\n\")\n",
        "        file.write(f\"scalarised_performance: {performance}\\n\\n\")\n",
        "\n",
        "file_path = f\"{save_dir}/lcb_metric_safe.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OtCnIncfray"
      },
      "source": [
        "### RL training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch as th\n",
        "from torch import nn  # Import torch.nn for activation functions\n",
        "\n",
        "# Learning rate decay schedule\n",
        "def cosine_annealing_schedule(progress_remaining: float, num_cycles=1, min_lr=0.005, max_lr=0.01):\n",
        "    progress = 1.0 - progress_remaining\n",
        "    lr = min_lr + (max_lr - min_lr) / 2 * (1 + np.cos(np.pi * num_cycles * progress))\n",
        "    return lr\n",
        "\n",
        "# Configuration for reinforcement learning model\n",
        "config = {\n",
        "    \"policy\": 'MlpPolicy',  # default: MlpPolicy\n",
        "    \"seed\": 1990,\n",
        "    \"check_freq\": 100,  # base: 12000 (~100 episodes)\n",
        "    \"n_eval_episodes\": 10,  # evaluate the agent over 100 episodes in the evaluation environment\n",
        "    \"positive_definiteness_penalty_weight\": 0,  # Set to 0 initially\n",
        "    \"derivative_penalty_weight\": 0,  # Set to 0 initially\n",
        "    'use_direct_penalty': False,  # choose between applying a penalty directly to the critic loss or adjusting the Q target values (derivative penalty)\n",
        "    'allowed_increase_factor': 1,  # max increase value for both methods (derivative penalty)\n",
        "    # 'total_timesteps': 500000,\n",
        "    # 'action_noise_sigma': 0.34670933515233754,\n",
        "}\n",
        "\n",
        "# The best hyperparameters found in previous runs\n",
        "best_params = {\n",
        "    'min_lr': 0.0005334897747162678,\n",
        "    'max_lr': 0.0003132010556972375,\n",
        "    'pi_layer_0_units': 3,\n",
        "    'pi_layer_1_units': 4,\n",
        "    'qf_layer_0_units': 3,\n",
        "    'qf_layer_1_units': 6,\n",
        "    'activation_fn': 'LeakyReLU',\n",
        "    'buffer_size': 200000,\n",
        "    'batch_size': 512,\n",
        "    'gamma': 0.9194039413768046,\n",
        "    'tau': 0.012320756403750887,\n",
        "    'learning_starts': 2082,\n",
        "    'train_freq': 1,\n",
        "    'action_noise_sigma': 0.34670933515233754,\n",
        "    'total_timesteps': 50000,\n",
        "}\n",
        "\n",
        "# Update the config dictionary with the best parameters\n",
        "config.update(best_params)\n",
        "\n",
        "# Set the activation function directly based on best_params\n",
        "if best_params['activation_fn'] == 'Tanh':\n",
        "    activation_fn = th.nn.Tanh\n",
        "elif best_params['activation_fn'] == 'ReLU':\n",
        "    activation_fn = th.nn.ReLU\n",
        "elif best_params['activation_fn'] == 'LeakyReLU':\n",
        "    activation_fn = th.nn.LeakyReLU\n",
        "else:\n",
        "    raise ValueError(\"Unsupported activation function\")\n",
        "\n",
        "# # Create policy_kwargs with the fixed network architecture and activation function from best_params\n",
        "policy_kwargs = dict(\n",
        "    activation_fn=activation_fn,  # Use the activation function directly\n",
        "    net_arch=dict(\n",
        "        pi=[2 ** best_params['pi_layer_0_units'], 2 ** best_params['pi_layer_1_units']],\n",
        "        qf=[2 ** best_params['qf_layer_0_units'], 2 ** best_params['qf_layer_1_units']]\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9qGhyfYRQMZ",
        "outputId": "0ef37754-e355-49a2-9785-2604e4a68a71"
      },
      "outputs": [],
      "source": [
        "import torch as th\n",
        "import numpy as np\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.policies import ActorCriticPolicy\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "##################################################################################\n",
        "# Environment and RL Definition\n",
        "##################################################################################\n",
        "\n",
        "# Enter required setpoints for each state. Enter None for states without setpoints.\n",
        "SP = {\n",
        "    'T': [325.0 for _ in range(nsteps)],\n",
        "}\n",
        "\n",
        "# Continuous box action space\n",
        "action_space = {\n",
        "    'low': np.array([295]),\n",
        "    'high': np.array([302])\n",
        "}\n",
        "\n",
        "# Continuous box observation space ([CA, T, CA_Setpoint, T_Setpoint])\n",
        "observation_space = {\n",
        "    'low': np.array([0.0, 300, 300]),\n",
        "    'high': np.array([1, 450, 400])\n",
        "}\n",
        "\n",
        "r_scale = {\n",
        "    'T': 1e-6  # Reward scale for each state,\n",
        "}\n",
        "\n",
        "# Define disturbance bounds\n",
        "disturbance_bounds = {\n",
        "    'low': np.array([330]),\n",
        "    'high': np.array([370])\n",
        "}\n",
        "\n",
        "# Environment parameters\n",
        "env_params_template = {\n",
        "    'Nx': 2,\n",
        "    'N': 60,\n",
        "    'tsim': 26,\n",
        "    'Nu': 1,\n",
        "    'SP': SP,\n",
        "    'o_space': observation_space,\n",
        "    'a_space': action_space,\n",
        "    'x0': np.array([0.87725294608097, 324.475443431599, 324.475443431599]),\n",
        "    'model': 'cstr',\n",
        "    'r_scale': r_scale,\n",
        "    'normalise_a': True,\n",
        "    'normalise_o': True,\n",
        "    'noise': True,\n",
        "    'integration_method': 'casadi',\n",
        "    'noise_percentage': 0.001,  # 0.001,\n",
        "    'disturbance_bounds': disturbance_bounds\n",
        "}\n",
        "\n",
        "# Add noise_percentage from env_params to config\n",
        "config['noise_percentage'] = env_params_template['noise_percentage']\n",
        "\n",
        "# Seed everything for reproducibility\n",
        "def set_global_seeds(seed):\n",
        "    np.random.seed(seed)\n",
        "    th.manual_seed(seed)\n",
        "    if th.cuda.is_available():\n",
        "        th.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "# Function to create random disturbances\n",
        "def create_random_disturbances(seed, nsteps, low=345, high=355):\n",
        "    # Set the global seed for reproducibility\n",
        "    set_global_seeds(seed)\n",
        "    value = np.random.uniform(low, high, 1)[0]  # Generate a single random disturbance value within the specified range\n",
        "    disturbances = {'Ti': [350] * (nsteps // 3) + [value] * (nsteps // 3) + [350] * (nsteps // 3)}  # Repeat each disturbance value for nsteps/3 times\n",
        "    return disturbances\n",
        "\n",
        "# Create multiple environments with different disturbances\n",
        "def create_parallel_envs(n_envs, seed):\n",
        "    set_global_seeds(seed)\n",
        "    envs = []\n",
        "    disturbances_list = []\n",
        "    for i in range(n_envs):\n",
        "        env_params = env_params_template.copy()\n",
        "        disturbances = create_random_disturbances(seed + i, nsteps)\n",
        "        env_params.update({'disturbances': disturbances})\n",
        "        disturbances_list.append(disturbances)\n",
        "        envs.append(lambda: make_env(env_params))\n",
        "    return DummyVecEnv(envs), disturbances_list\n",
        "\n",
        "# Create evaluation environment using DummyVecEnv\n",
        "def create_eval_env(seed, n_envs=1):\n",
        "    set_global_seeds(seed)\n",
        "    envs = []\n",
        "    for i in range(n_envs):\n",
        "        env_params = env_params_template.copy()\n",
        "        disturbances = create_random_disturbances(seed + i, nsteps)\n",
        "        env_params.update({'disturbances': disturbances})\n",
        "        envs.append(lambda: make_env(env_params))\n",
        "    return DummyVecEnv(envs)\n",
        "\n",
        "eval_env = create_eval_env(seed=config['seed'], n_envs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "29c77ed8695a4287956ec27672fe7b7a",
            "5b907d9cb030400c8cd9308246d761c1",
            "bf4a59f3065346d6af96d1fc1a2e6385",
            "c341e0ddb94f4c0b9066e8dc99136430",
            "ac046599192d45cfbf5ab150f9fb240d",
            "483c79836eed4cfab4cfc7320151ef66",
            "e8afd67546254f249e488008aa76f0a8",
            "6446e95478cf4113aad04353ba1c2030"
          ]
        },
        "id": "ZhPjWJ7AfjMs",
        "outputId": "eef1f821-6644-4a6c-9131-889a6f18b697"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First disturbance used in training: {'Ti': [320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 325.8296588623832, 325.8296588623832, 325.8296588623832, 325.8296588623832, 325.8296588623832, 325.8296588623832, 325.8296588623832, 325.8296588623832, 325.8296588623832, 325.8296588623832, 325.8296588623832, 325.8296588623832, 325.8296588623832, 325.8296588623832, 325.8296588623832, 325.8296588623832, 325.8296588623832, 325.8296588623832, 325.8296588623832, 325.8296588623832, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320]}\n",
            "Using cpu device\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 421      |\n",
            "|    time_elapsed    | 1        |\n",
            "|    total_timesteps | 590      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0524   |\n",
            "|    critic_loss     | 4.78e-06 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 48       |\n",
            "---------------------------------\n",
            "----------------------------\n",
            "| time/              |     |\n",
            "|    episodes        | 8   |\n",
            "|    fps             | 421 |\n",
            "|    time_elapsed    | 1   |\n",
            "|    total_timesteps | 590 |\n",
            "----------------------------\n",
            "Eval num_timesteps=1000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00316 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0517   |\n",
            "|    critic_loss     | 1.97e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 89       |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 335      |\n",
            "|    time_elapsed    | 3        |\n",
            "|    total_timesteps | 1180     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0511   |\n",
            "|    critic_loss     | 6.04e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 107      |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 16   |\n",
            "|    fps             | 335  |\n",
            "|    time_elapsed    | 3    |\n",
            "|    total_timesteps | 1180 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 20   |\n",
            "|    fps             | 335  |\n",
            "|    time_elapsed    | 3    |\n",
            "|    total_timesteps | 1180 |\n",
            "-----------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 24       |\n",
            "|    fps             | 358      |\n",
            "|    time_elapsed    | 4        |\n",
            "|    total_timesteps | 1770     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0571   |\n",
            "|    critic_loss     | 6.63e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 166      |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 28   |\n",
            "|    fps             | 358  |\n",
            "|    time_elapsed    | 4    |\n",
            "|    total_timesteps | 1770 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=2000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00323 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.049    |\n",
            "|    critic_loss     | 4.29e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 189      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 32       |\n",
            "|    fps             | 329      |\n",
            "|    time_elapsed    | 7        |\n",
            "|    total_timesteps | 2360     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0532   |\n",
            "|    critic_loss     | 1.81e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 225      |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 36   |\n",
            "|    fps             | 329  |\n",
            "|    time_elapsed    | 7    |\n",
            "|    total_timesteps | 2360 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 40   |\n",
            "|    fps             | 329  |\n",
            "|    time_elapsed    | 7    |\n",
            "|    total_timesteps | 2360 |\n",
            "-----------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 44       |\n",
            "|    fps             | 335      |\n",
            "|    time_elapsed    | 8        |\n",
            "|    total_timesteps | 2950     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0556   |\n",
            "|    critic_loss     | 2.27e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 284      |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 48   |\n",
            "|    fps             | 335  |\n",
            "|    time_elapsed    | 8    |\n",
            "|    total_timesteps | 2950 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=3000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00324 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0552   |\n",
            "|    critic_loss     | 3.58e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 289      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 52       |\n",
            "|    fps             | 316      |\n",
            "|    time_elapsed    | 11       |\n",
            "|    total_timesteps | 3540     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.053    |\n",
            "|    critic_loss     | 1.72e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 343      |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 56   |\n",
            "|    fps             | 316  |\n",
            "|    time_elapsed    | 11   |\n",
            "|    total_timesteps | 3540 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 60   |\n",
            "|    fps             | 316  |\n",
            "|    time_elapsed    | 11   |\n",
            "|    total_timesteps | 3540 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=4000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00322 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0538   |\n",
            "|    critic_loss     | 5.47e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 389      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 64       |\n",
            "|    fps             | 292      |\n",
            "|    time_elapsed    | 14       |\n",
            "|    total_timesteps | 4130     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0523   |\n",
            "|    critic_loss     | 6.64e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 402      |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 68   |\n",
            "|    fps             | 292  |\n",
            "|    time_elapsed    | 14   |\n",
            "|    total_timesteps | 4130 |\n",
            "-----------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 72       |\n",
            "|    fps             | 298      |\n",
            "|    time_elapsed    | 15       |\n",
            "|    total_timesteps | 4720     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0526   |\n",
            "|    critic_loss     | 3.79e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 461      |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 76   |\n",
            "|    fps             | 298  |\n",
            "|    time_elapsed    | 15   |\n",
            "|    total_timesteps | 4720 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 80   |\n",
            "|    fps             | 297  |\n",
            "|    time_elapsed    | 15   |\n",
            "|    total_timesteps | 4720 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=5000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00321 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0518   |\n",
            "|    critic_loss     | 5.02e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 489      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 84       |\n",
            "|    fps             | 287      |\n",
            "|    time_elapsed    | 18       |\n",
            "|    total_timesteps | 5310     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0507   |\n",
            "|    critic_loss     | 5.03e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 520      |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 88   |\n",
            "|    fps             | 287  |\n",
            "|    time_elapsed    | 18   |\n",
            "|    total_timesteps | 5310 |\n",
            "-----------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 92       |\n",
            "|    fps             | 287      |\n",
            "|    time_elapsed    | 20       |\n",
            "|    total_timesteps | 5900     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0529   |\n",
            "|    critic_loss     | 2.85e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 579      |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 96   |\n",
            "|    fps             | 287  |\n",
            "|    time_elapsed    | 20   |\n",
            "|    total_timesteps | 5900 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 100  |\n",
            "|    fps             | 287  |\n",
            "|    time_elapsed    | 20   |\n",
            "|    total_timesteps | 5900 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=6000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00314 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 6000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0462   |\n",
            "|    critic_loss     | 8.94e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 589      |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 104      |\n",
            "|    fps             | 276      |\n",
            "|    time_elapsed    | 23       |\n",
            "|    total_timesteps | 6490     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0511   |\n",
            "|    critic_loss     | 8.59e-06 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 638      |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 108  |\n",
            "|    fps             | 276  |\n",
            "|    time_elapsed    | 23   |\n",
            "|    total_timesteps | 6490 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=7000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00315 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 7000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0507   |\n",
            "|    critic_loss     | 9.53e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 689      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 112      |\n",
            "|    fps             | 272      |\n",
            "|    time_elapsed    | 25       |\n",
            "|    total_timesteps | 7080     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0494   |\n",
            "|    critic_loss     | 6.17e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 697      |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 116  |\n",
            "|    fps             | 272  |\n",
            "|    time_elapsed    | 25   |\n",
            "|    total_timesteps | 7080 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 120  |\n",
            "|    fps             | 272  |\n",
            "|    time_elapsed    | 25   |\n",
            "|    total_timesteps | 7080 |\n",
            "-----------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 124      |\n",
            "|    fps             | 278      |\n",
            "|    time_elapsed    | 27       |\n",
            "|    total_timesteps | 7670     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0471   |\n",
            "|    critic_loss     | 5.55e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 756      |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 128  |\n",
            "|    fps             | 278  |\n",
            "|    time_elapsed    | 27   |\n",
            "|    total_timesteps | 7670 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=8000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00315 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0484   |\n",
            "|    critic_loss     | 4.32e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 789      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 132      |\n",
            "|    fps             | 274      |\n",
            "|    time_elapsed    | 30       |\n",
            "|    total_timesteps | 8260     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0477   |\n",
            "|    critic_loss     | 6.72e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 815      |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 136  |\n",
            "|    fps             | 274  |\n",
            "|    time_elapsed    | 30   |\n",
            "|    total_timesteps | 8260 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 140  |\n",
            "|    fps             | 274  |\n",
            "|    time_elapsed    | 30   |\n",
            "|    total_timesteps | 8260 |\n",
            "-----------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 144      |\n",
            "|    fps             | 278      |\n",
            "|    time_elapsed    | 31       |\n",
            "|    total_timesteps | 8850     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0449   |\n",
            "|    critic_loss     | 9.39e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 874      |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 148  |\n",
            "|    fps             | 278  |\n",
            "|    time_elapsed    | 31   |\n",
            "|    total_timesteps | 8850 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=9000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00317 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 9000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0474   |\n",
            "|    critic_loss     | 4.09e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 889      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 152      |\n",
            "|    fps             | 274      |\n",
            "|    time_elapsed    | 34       |\n",
            "|    total_timesteps | 9440     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0495   |\n",
            "|    critic_loss     | 2.48e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 933      |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 156  |\n",
            "|    fps             | 274  |\n",
            "|    time_elapsed    | 34   |\n",
            "|    total_timesteps | 9440 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 160  |\n",
            "|    fps             | 274  |\n",
            "|    time_elapsed    | 34   |\n",
            "|    total_timesteps | 9440 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=10000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00318 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 10000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0473   |\n",
            "|    critic_loss     | 5.73e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 989      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 164      |\n",
            "|    fps             | 271      |\n",
            "|    time_elapsed    | 36       |\n",
            "|    total_timesteps | 10030    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0485   |\n",
            "|    critic_loss     | 4.85e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 992      |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 168   |\n",
            "|    fps             | 271   |\n",
            "|    time_elapsed    | 36    |\n",
            "|    total_timesteps | 10030 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 172      |\n",
            "|    fps             | 275      |\n",
            "|    time_elapsed    | 38       |\n",
            "|    total_timesteps | 10620    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0485   |\n",
            "|    critic_loss     | 1.95e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1051     |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 176   |\n",
            "|    fps             | 275   |\n",
            "|    time_elapsed    | 38    |\n",
            "|    total_timesteps | 10620 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 180   |\n",
            "|    fps             | 275   |\n",
            "|    time_elapsed    | 38    |\n",
            "|    total_timesteps | 10620 |\n",
            "------------------------------\n",
            "Eval num_timesteps=11000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00319 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 11000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0448   |\n",
            "|    critic_loss     | 3.78e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1089     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 184      |\n",
            "|    fps             | 272      |\n",
            "|    time_elapsed    | 41       |\n",
            "|    total_timesteps | 11210    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.047    |\n",
            "|    critic_loss     | 2.47e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1110     |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 188   |\n",
            "|    fps             | 272   |\n",
            "|    time_elapsed    | 41    |\n",
            "|    total_timesteps | 11210 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 192      |\n",
            "|    fps             | 275      |\n",
            "|    time_elapsed    | 42       |\n",
            "|    total_timesteps | 11800    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0485   |\n",
            "|    critic_loss     | 4.63e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1169     |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 196   |\n",
            "|    fps             | 275   |\n",
            "|    time_elapsed    | 42    |\n",
            "|    total_timesteps | 11800 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 200   |\n",
            "|    fps             | 275   |\n",
            "|    time_elapsed    | 42    |\n",
            "|    total_timesteps | 11800 |\n",
            "------------------------------\n",
            "Eval num_timesteps=12000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00321 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 12000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0491   |\n",
            "|    critic_loss     | 3.19e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1189     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 204      |\n",
            "|    fps             | 274      |\n",
            "|    time_elapsed    | 45       |\n",
            "|    total_timesteps | 12390    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0464   |\n",
            "|    critic_loss     | 2.55e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1228     |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 208   |\n",
            "|    fps             | 274   |\n",
            "|    time_elapsed    | 45    |\n",
            "|    total_timesteps | 12390 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 212      |\n",
            "|    fps             | 278      |\n",
            "|    time_elapsed    | 46       |\n",
            "|    total_timesteps | 12980    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0466   |\n",
            "|    critic_loss     | 4.1e-05  |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1287     |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 216   |\n",
            "|    fps             | 278   |\n",
            "|    time_elapsed    | 46    |\n",
            "|    total_timesteps | 12980 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 220   |\n",
            "|    fps             | 278   |\n",
            "|    time_elapsed    | 46    |\n",
            "|    total_timesteps | 12980 |\n",
            "------------------------------\n",
            "Eval num_timesteps=13000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00319 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 13000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0444   |\n",
            "|    critic_loss     | 4.1e-05  |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1289     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 224      |\n",
            "|    fps             | 277      |\n",
            "|    time_elapsed    | 48       |\n",
            "|    total_timesteps | 13570    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0476   |\n",
            "|    critic_loss     | 3.22e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1346     |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 228   |\n",
            "|    fps             | 277   |\n",
            "|    time_elapsed    | 48    |\n",
            "|    total_timesteps | 13570 |\n",
            "------------------------------\n",
            "Eval num_timesteps=14000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00316 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 14000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0438   |\n",
            "|    critic_loss     | 3.72e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1389     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 232      |\n",
            "|    fps             | 275      |\n",
            "|    time_elapsed    | 51       |\n",
            "|    total_timesteps | 14160    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0457   |\n",
            "|    critic_loss     | 3.19e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1405     |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 236   |\n",
            "|    fps             | 275   |\n",
            "|    time_elapsed    | 51    |\n",
            "|    total_timesteps | 14160 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 240   |\n",
            "|    fps             | 275   |\n",
            "|    time_elapsed    | 51    |\n",
            "|    total_timesteps | 14160 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 244      |\n",
            "|    fps             | 277      |\n",
            "|    time_elapsed    | 53       |\n",
            "|    total_timesteps | 14750    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.041    |\n",
            "|    critic_loss     | 4.55e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1464     |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 248   |\n",
            "|    fps             | 277   |\n",
            "|    time_elapsed    | 53    |\n",
            "|    total_timesteps | 14750 |\n",
            "------------------------------\n",
            "Eval num_timesteps=15000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00317 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 15000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0444   |\n",
            "|    critic_loss     | 3.87e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1489     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 252      |\n",
            "|    fps             | 273      |\n",
            "|    time_elapsed    | 56       |\n",
            "|    total_timesteps | 15340    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0459   |\n",
            "|    critic_loss     | 4.62e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1523     |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 256   |\n",
            "|    fps             | 273   |\n",
            "|    time_elapsed    | 56    |\n",
            "|    total_timesteps | 15340 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 260   |\n",
            "|    fps             | 273   |\n",
            "|    time_elapsed    | 56    |\n",
            "|    total_timesteps | 15340 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 264      |\n",
            "|    fps             | 277      |\n",
            "|    time_elapsed    | 57       |\n",
            "|    total_timesteps | 15930    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0452   |\n",
            "|    critic_loss     | 2.88e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1582     |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 268   |\n",
            "|    fps             | 277   |\n",
            "|    time_elapsed    | 57    |\n",
            "|    total_timesteps | 15930 |\n",
            "------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00317 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 16000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0436   |\n",
            "|    critic_loss     | 2.94e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1589     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 272      |\n",
            "|    fps             | 277      |\n",
            "|    time_elapsed    | 59       |\n",
            "|    total_timesteps | 16520    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0407   |\n",
            "|    critic_loss     | 2.48e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1641     |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 276   |\n",
            "|    fps             | 277   |\n",
            "|    time_elapsed    | 59    |\n",
            "|    total_timesteps | 16520 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 280   |\n",
            "|    fps             | 277   |\n",
            "|    time_elapsed    | 59    |\n",
            "|    total_timesteps | 16520 |\n",
            "------------------------------\n",
            "Eval num_timesteps=17000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00317 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 17000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0421   |\n",
            "|    critic_loss     | 2.94e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1689     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 284      |\n",
            "|    fps             | 275      |\n",
            "|    time_elapsed    | 62       |\n",
            "|    total_timesteps | 17110    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0423   |\n",
            "|    critic_loss     | 3.14e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1700     |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 288   |\n",
            "|    fps             | 275   |\n",
            "|    time_elapsed    | 62    |\n",
            "|    total_timesteps | 17110 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 292      |\n",
            "|    fps             | 277      |\n",
            "|    time_elapsed    | 63       |\n",
            "|    total_timesteps | 17700    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0443   |\n",
            "|    critic_loss     | 2.99e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1759     |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 296   |\n",
            "|    fps             | 277   |\n",
            "|    time_elapsed    | 63    |\n",
            "|    total_timesteps | 17700 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 300   |\n",
            "|    fps             | 277   |\n",
            "|    time_elapsed    | 63    |\n",
            "|    total_timesteps | 17700 |\n",
            "------------------------------\n",
            "Eval num_timesteps=18000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00316 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 18000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0454   |\n",
            "|    critic_loss     | 1.97e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1789     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 304      |\n",
            "|    fps             | 274      |\n",
            "|    time_elapsed    | 66       |\n",
            "|    total_timesteps | 18290    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0407   |\n",
            "|    critic_loss     | 3.86e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1818     |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 308   |\n",
            "|    fps             | 274   |\n",
            "|    time_elapsed    | 66    |\n",
            "|    total_timesteps | 18290 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 312      |\n",
            "|    fps             | 277      |\n",
            "|    time_elapsed    | 68       |\n",
            "|    total_timesteps | 18880    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0383   |\n",
            "|    critic_loss     | 1.35e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1877     |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 316   |\n",
            "|    fps             | 277   |\n",
            "|    time_elapsed    | 68    |\n",
            "|    total_timesteps | 18880 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 320   |\n",
            "|    fps             | 277   |\n",
            "|    time_elapsed    | 68    |\n",
            "|    total_timesteps | 18880 |\n",
            "------------------------------\n",
            "Eval num_timesteps=19000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00316 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 19000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0439   |\n",
            "|    critic_loss     | 5.11e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1889     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 324      |\n",
            "|    fps             | 276      |\n",
            "|    time_elapsed    | 70       |\n",
            "|    total_timesteps | 19470    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0421   |\n",
            "|    critic_loss     | 2.59e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1936     |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 328   |\n",
            "|    fps             | 276   |\n",
            "|    time_elapsed    | 70    |\n",
            "|    total_timesteps | 19470 |\n",
            "------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00316 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 20000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0386   |\n",
            "|    critic_loss     | 4.26e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1989     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 332      |\n",
            "|    fps             | 276      |\n",
            "|    time_elapsed    | 72       |\n",
            "|    total_timesteps | 20060    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0411   |\n",
            "|    critic_loss     | 3.37e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 1995     |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 336   |\n",
            "|    fps             | 276   |\n",
            "|    time_elapsed    | 72    |\n",
            "|    total_timesteps | 20060 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 340   |\n",
            "|    fps             | 276   |\n",
            "|    time_elapsed    | 72    |\n",
            "|    total_timesteps | 20060 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 344      |\n",
            "|    fps             | 278      |\n",
            "|    time_elapsed    | 74       |\n",
            "|    total_timesteps | 20650    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0405   |\n",
            "|    critic_loss     | 1.24e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 2054     |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 348   |\n",
            "|    fps             | 278   |\n",
            "|    time_elapsed    | 74    |\n",
            "|    total_timesteps | 20650 |\n",
            "------------------------------\n",
            "Eval num_timesteps=21000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00318 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 21000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0398   |\n",
            "|    critic_loss     | 2.42e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 2089     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 352      |\n",
            "|    fps             | 276      |\n",
            "|    time_elapsed    | 76       |\n",
            "|    total_timesteps | 21240    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0394   |\n",
            "|    critic_loss     | 3.33e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 2113     |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 356   |\n",
            "|    fps             | 276   |\n",
            "|    time_elapsed    | 76    |\n",
            "|    total_timesteps | 21240 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 360   |\n",
            "|    fps             | 276   |\n",
            "|    time_elapsed    | 76    |\n",
            "|    total_timesteps | 21240 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 364      |\n",
            "|    fps             | 279      |\n",
            "|    time_elapsed    | 78       |\n",
            "|    total_timesteps | 21830    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0406   |\n",
            "|    critic_loss     | 3.35e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 2172     |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 368   |\n",
            "|    fps             | 279   |\n",
            "|    time_elapsed    | 78    |\n",
            "|    total_timesteps | 21830 |\n",
            "------------------------------\n",
            "Eval num_timesteps=22000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00316 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 22000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0429   |\n",
            "|    critic_loss     | 4.51e-05 |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 2189     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 372      |\n",
            "|    fps             | 278      |\n",
            "|    time_elapsed    | 80       |\n",
            "|    total_timesteps | 22420    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.039    |\n",
            "|    critic_loss     | 2.4e-05  |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 2231     |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 376   |\n",
            "|    fps             | 278   |\n",
            "|    time_elapsed    | 80    |\n",
            "|    total_timesteps | 22420 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 380   |\n",
            "|    fps             | 278   |\n",
            "|    time_elapsed    | 80    |\n",
            "|    total_timesteps | 22420 |\n",
            "------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 60\u001b[0m\n\u001b[0;32m     50\u001b[0m eval_callback \u001b[38;5;241m=\u001b[39m EvalCallback(\n\u001b[0;32m     51\u001b[0m     eval_env,\n\u001b[0;32m     52\u001b[0m     best_model_save_path\u001b[38;5;241m=\u001b[39msave_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     58\u001b[0m )\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Train the model with the callbacks\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal_timesteps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m     63\u001b[0m model\u001b[38;5;241m.\u001b[39msave(save_dir)\n",
            "File \u001b[1;32mc:\\Users\\mfb22\\AppData\\Local\\anaconda3\\envs\\pcgym_test\\Lib\\site-packages\\stable_baselines3\\ddpg\\ddpg.py:123\u001b[0m, in \u001b[0;36mDDPG.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDDPG,\n\u001b[0;32m    116\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    122\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDDPG:\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\mfb22\\AppData\\Local\\anaconda3\\envs\\pcgym_test\\Lib\\site-packages\\stable_baselines3\\td3\\td3.py:222\u001b[0m, in \u001b[0;36mTD3.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfTD3,\n\u001b[0;32m    215\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    221\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfTD3:\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\mfb22\\AppData\\Local\\anaconda3\\envs\\pcgym_test\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:328\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 328\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\mfb22\\AppData\\Local\\anaconda3\\envs\\pcgym_test\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:568\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[0;32m    566\u001b[0m callback\u001b[38;5;241m.\u001b[39mupdate_locals(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[0;32m    567\u001b[0m \u001b[38;5;66;03m# Only stop training if return value is False, not when it is None.\u001b[39;00m\n\u001b[1;32m--> 568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RolloutReturn(num_collected_steps \u001b[38;5;241m*\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs, num_collected_episodes, continue_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    571\u001b[0m \u001b[38;5;66;03m# Retrieve reward and episode length if using Monitor wrapper\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\mfb22\\AppData\\Local\\anaconda3\\envs\\pcgym_test\\Lib\\site-packages\\stable_baselines3\\common\\callbacks.py:114\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnum_timesteps\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\mfb22\\AppData\\Local\\anaconda3\\envs\\pcgym_test\\Lib\\site-packages\\stable_baselines3\\common\\callbacks.py:464\u001b[0m, in \u001b[0;36mEvalCallback._on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;66;03m# Reset success rate buffer\u001b[39;00m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_success_buffer \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 464\u001b[0m episode_rewards, episode_lengths \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_episode_rewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_success_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(episode_rewards, \u001b[38;5;28mlist\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\mfb22\\AppData\\Local\\anaconda3\\envs\\pcgym_test\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:88\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[1;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[0;32m     86\u001b[0m episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((env\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (episode_counts \u001b[38;5;241m<\u001b[39m episode_count_targets)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m---> 88\u001b[0m     actions, states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisode_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m     new_observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n\u001b[0;32m     95\u001b[0m     current_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n",
            "File \u001b[1;32mc:\\Users\\mfb22\\AppData\\Local\\anaconda3\\envs\\pcgym_test\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:556\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    538\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[0;32m    543\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\mfb22\\AppData\\Local\\anaconda3\\envs\\pcgym_test\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:368\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 368\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# Convert to numpy, and reshape to the original action shape\u001b[39;00m\n\u001b[0;32m    370\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape))  \u001b[38;5;66;03m# type: ignore[misc, assignment]\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\mfb22\\AppData\\Local\\anaconda3\\envs\\pcgym_test\\Lib\\site-packages\\stable_baselines3\\td3\\policies.py:242\u001b[0m, in \u001b[0;36mTD3Policy._predict\u001b[1;34m(self, observation, deterministic)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation: PyTorchObs, deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# Note: the deterministic deterministic parameter is ignored in the case of TD3.\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;66;03m#   Predictions are always deterministic.\u001b[39;00m\n\u001b[1;32m--> 242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\mfb22\\AppData\\Local\\anaconda3\\envs\\pcgym_test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\mfb22\\AppData\\Local\\anaconda3\\envs\\pcgym_test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\mfb22\\AppData\\Local\\anaconda3\\envs\\pcgym_test\\Lib\\site-packages\\stable_baselines3\\td3\\policies.py:78\u001b[0m, in \u001b[0;36mActor.forward\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;66;03m# assert deterministic, 'The TD3 actor only outputs deterministic actions'\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_features(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_extractor)\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\mfb22\\AppData\\Local\\anaconda3\\envs\\pcgym_test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\mfb22\\AppData\\Local\\anaconda3\\envs\\pcgym_test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\mfb22\\AppData\\Local\\anaconda3\\envs\\pcgym_test\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\mfb22\\AppData\\Local\\anaconda3\\envs\\pcgym_test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\mfb22\\AppData\\Local\\anaconda3\\envs\\pcgym_test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\mfb22\\AppData\\Local\\anaconda3\\envs\\pcgym_test\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.noise import OrnsteinUhlenbeckActionNoise\n",
        "\n",
        "# Set the global seed for reproducibility\n",
        "set_global_seeds(config['seed'])\n",
        "\n",
        "# Number of parallel environments\n",
        "n_envs = 10\n",
        "\n",
        "# Create the parallel environments and get the disturbances\n",
        "env, disturbances_list = create_parallel_envs(n_envs, config['seed'])\n",
        "\n",
        "# Set up Ornstein-Uhlenbeck action noise after creating the environment\n",
        "n_actions = env.action_space.shape\n",
        "action_noise = OrnsteinUhlenbeckActionNoise(\n",
        "    mean=np.zeros(n_actions), \n",
        "    sigma=config['action_noise_sigma'] * np.ones(n_actions)\n",
        ")\n",
        "\n",
        "# Print the first disturbances\n",
        "print(\"First disturbance used in training:\", disturbances_list[0])\n",
        "\n",
        "model = DDPG(\n",
        "    config['policy'],\n",
        "    env,\n",
        "    learning_rate=0.0001,\n",
        "    # learning_rate=lambda progress: cosine_annealing_schedule(progress, \n",
        "    #     min_lr=config['min_lr'], max_lr=config['max_lr']),  # Cosine schedule\n",
        "    # batch_size=config['batch_size'],\n",
        "    # gamma=config['gamma'],\n",
        "    # tau=config['tau'],\n",
        "    # buffer_size=config['buffer_size'],\n",
        "    # learning_starts=config['learning_starts'],\n",
        "    # train_freq=config['train_freq'],\n",
        "    # action_noise=action_noise,\n",
        "    # policy_kwargs=policy_kwargs,  # Use custom policy network structure\n",
        "    seed=seed,\n",
        "    verbose=1,\n",
        "    # Adding the optimized DDPG-specific parameters to the model\n",
        "\n",
        "\n",
        ")\n",
        "\n",
        "# Create the evaluation environment\n",
        "eval_env = create_eval_env(seed)\n",
        "\n",
        "\n",
        "eval_callback = EvalCallback(\n",
        "    eval_env,\n",
        "    best_model_save_path=save_dir,\n",
        "    log_path=save_dir,\n",
        "    eval_freq=config['check_freq'],\n",
        "    n_eval_episodes=config['n_eval_episodes'],\n",
        "    deterministic=True,\n",
        "    render=False\n",
        ")\n",
        "# Train the model with the callbacks\n",
        "model.learn(total_timesteps=config['total_timesteps'], callback=eval_callback)\n",
        "\n",
        "# Save the model\n",
        "model.save(save_dir)\n",
        "\n",
        "# Save the disturbances to a file for future reference\n",
        "disturbances_file_path = os.path.join(save_dir, \"disturbances_used_in_training.txt\")\n",
        "with open(disturbances_file_path, \"w\") as f:\n",
        "    for i, disturbance in enumerate(disturbances_list):\n",
        "        f.write(f\"Disturbance {i+1}: {disturbance}\\n\")\n",
        "\n",
        "# Finish the W&B run\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "6x1u30y1uN2_"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "pcgym_test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02ab575e267f4264a325dcea50170518": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d397a3c58bdb4f09b7e774b30727d8ae",
              "IPY_MODEL_5e12a8eb338042f8b42b6f9a35813403"
            ],
            "layout": "IPY_MODEL_cc9fd941a0004a3ebb1d5d66a1013aaf"
          }
        },
        "15627f8991b54e8b961444ef8481086b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19a083985d3e466cb8130c07d692901c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29c77ed8695a4287956ec27672fe7b7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b907d9cb030400c8cd9308246d761c1",
              "IPY_MODEL_bf4a59f3065346d6af96d1fc1a2e6385"
            ],
            "layout": "IPY_MODEL_c341e0ddb94f4c0b9066e8dc99136430"
          }
        },
        "32fd32bd040841959acbae2d2e1b8365": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1a6dbe39e8e492baac9782ed9b7230e",
            "placeholder": "​",
            "style": "IPY_MODEL_b74942f112ea4933b0c5ba6176c6b142",
            "value": "0.013 MB of 0.013 MB uploaded\r"
          }
        },
        "35e1b6be6af8413d95e14d6ce68f3f98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32fd32bd040841959acbae2d2e1b8365",
              "IPY_MODEL_52dbfe8dcd8d4a968987b49e4643feeb"
            ],
            "layout": "IPY_MODEL_15627f8991b54e8b961444ef8481086b"
          }
        },
        "483c79836eed4cfab4cfc7320151ef66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "518bd8accca940b284dfc562ca9b823c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "52dbfe8dcd8d4a968987b49e4643feeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2ce0aa2308849fb9a56cbd94aad8fa9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b5526159b14423dbcc1228e4a1f7699",
            "value": 1
          }
        },
        "5b5526159b14423dbcc1228e4a1f7699": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b907d9cb030400c8cd9308246d761c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac046599192d45cfbf5ab150f9fb240d",
            "placeholder": "​",
            "style": "IPY_MODEL_483c79836eed4cfab4cfc7320151ef66",
            "value": "0.674 MB of 0.674 MB uploaded\r"
          }
        },
        "5e12a8eb338042f8b42b6f9a35813403": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae728950ce914f39a035a67662e99bad",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_518bd8accca940b284dfc562ca9b823c",
            "value": 1
          }
        },
        "6446e95478cf4113aad04353ba1c2030": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac046599192d45cfbf5ab150f9fb240d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae728950ce914f39a035a67662e99bad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b74942f112ea4933b0c5ba6176c6b142": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf4a59f3065346d6af96d1fc1a2e6385": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8afd67546254f249e488008aa76f0a8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6446e95478cf4113aad04353ba1c2030",
            "value": 1
          }
        },
        "c2ce0aa2308849fb9a56cbd94aad8fa9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c341e0ddb94f4c0b9066e8dc99136430": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc9fd941a0004a3ebb1d5d66a1013aaf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfb98e5737c9486fb426884c14e5ae1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1a6dbe39e8e492baac9782ed9b7230e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d397a3c58bdb4f09b7e774b30727d8ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19a083985d3e466cb8130c07d692901c",
            "placeholder": "​",
            "style": "IPY_MODEL_cfb98e5737c9486fb426884c14e5ae1f",
            "value": "0.013 MB of 0.013 MB uploaded\r"
          }
        },
        "e8afd67546254f249e488008aa76f0a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

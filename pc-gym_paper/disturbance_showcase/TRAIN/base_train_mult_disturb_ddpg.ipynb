{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-BcG1g4QbE_"
      },
      "source": [
        "# Cloning the Repository (pc-gym)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxWOGAsaECz2",
        "outputId": "899c63bc-c202-4dbc-eb5e-9de200616a0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Volume in drive C is Windows \n",
            " Volume Serial Number is F0B8-32A3\n",
            "\n",
            " Directory of c:\\Users\\Usuario\\Desktop\\imperial_projects\\VSCode\\pcgym2\\train\\base\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "File Not Found\n"
          ]
        }
      ],
      "source": [
        "!dir pc-gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVv4nhKnJZyQ",
        "outputId": "0c6c2f7d-37db-4485-a590-41299d74c51c"
      },
      "outputs": [],
      "source": [
        "# %cd pc-gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pXeY_oDVTPZ"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhR6YfBEbVRM",
        "outputId": "a67140ae-cfff-4a16-fbd8-dc7974929319"
      },
      "outputs": [],
      "source": [
        "# %cd src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %cd pcgym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Usuario\\Desktop\\imperial_projects\\VSCode\\pcgym2\\pc-gym\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\Desktop\\imperial_projects\\VSCode\\pcgym2\\new_venv\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "%cd C:/Users/Usuario/Desktop/imperial_projects/VSCode/pcgym2/pc-gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0xr48fylffH",
        "outputId": "504b6be1-dc60-4c93-f087-158249487b25"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "from stable_baselines3 import PPO,SAC,DDPG,TD3\n",
        "import pcgym\n",
        "from pcgym import make_env\n",
        "import jax.numpy as jnp\n",
        "#Global params\n",
        "T = 26\n",
        "nsteps =60\n",
        "# Global seed for reproducibility\n",
        "seed = 1990"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMPAn1SRd32f"
      },
      "source": [
        "# Saving and loading\n",
        "\n",
        "Saving and loading stable-baselines models is straightforward: you can directly call `.save()` and `.load()` on the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mph7ddv8iCjY",
        "outputId": "fc007c61-f2cd-4545-d8ee-15f900a0eb6c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Create save dir\n",
        "save_dir = \"./max/ddpg\"\n",
        "os.makedirs(save_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPlCXGwWFPTI"
      },
      "source": [
        "# Monitoring experiments with W&B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8sM9B5fOG_w",
        "outputId": "6c14e127-7339-4421-9a3b-f4c37cd16d6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Usuario\\.netrc\n"
          ]
        }
      ],
      "source": [
        "!wandb login 84af17cc9914cf1736f3a8e2733a2f361e4750bb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sWhTer-Wg7X"
      },
      "source": [
        "# 1.1 Reactor Case Study"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZklHwMtzkOAE",
        "outputId": "209ef879-e2cb-48ac-ce44-0c2300707462"
      },
      "outputs": [],
      "source": [
        "# @title Function to log the performance data\n",
        "def log_performance(performance, test_label, file_path):\n",
        "    with open(file_path, \"a\") as file:\n",
        "        file.write(f\"{test_label}: \\n\")\n",
        "        file.write(f\"scalarised_performance: {performance}\\n\\n\")\n",
        "\n",
        "file_path = f\"{save_dir}/lcb_metric_safe.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OtCnIncfray"
      },
      "source": [
        "### RL training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch as th\n",
        "from torch import nn  # Import torch.nn for activation functions\n",
        "\n",
        "# Learning rate decay schedule\n",
        "def cosine_annealing_schedule(progress_remaining: float, num_cycles=1, min_lr=0.005, max_lr=0.01):\n",
        "    progress = 1.0 - progress_remaining\n",
        "    lr = min_lr + (max_lr - min_lr) / 2 * (1 + np.cos(np.pi * num_cycles * progress))\n",
        "    return lr\n",
        "\n",
        "# Configuration for reinforcement learning model\n",
        "config = {\n",
        "    \"policy\": 'MlpPolicy',  # default: MlpPolicy\n",
        "    \"seed\": 1990,\n",
        "    \"check_freq\": 100,  # base: 12000 (~100 episodes)\n",
        "    \"n_eval_episodes\": 10,  # evaluate the agent over 100 episodes in the evaluation environment\n",
        "    \"positive_definiteness_penalty_weight\": 0,  # Set to 0 initially\n",
        "    \"derivative_penalty_weight\": 0,  # Set to 0 initially\n",
        "    'use_direct_penalty': False,  # choose between applying a penalty directly to the critic loss or adjusting the Q target values (derivative penalty)\n",
        "    'allowed_increase_factor': 1,  # max increase value for both methods (derivative penalty)\n",
        "    # 'total_timesteps': 500000,\n",
        "    # 'action_noise_sigma': 0.34670933515233754,\n",
        "}\n",
        "\n",
        "# The best hyperparameters found in previous runs\n",
        "best_params = {\n",
        "    'min_lr': 0.0005334897747162678,\n",
        "    'max_lr': 0.0003132010556972375,\n",
        "    'pi_layer_0_units': 3,\n",
        "    'pi_layer_1_units': 4,\n",
        "    'qf_layer_0_units': 3,\n",
        "    'qf_layer_1_units': 6,\n",
        "    'activation_fn': 'LeakyReLU',\n",
        "    'buffer_size': 200000,\n",
        "    'batch_size': 512,\n",
        "    'gamma': 0.9194039413768046,\n",
        "    'tau': 0.012320756403750887,\n",
        "    'learning_starts': 2082,\n",
        "    'train_freq': 1,\n",
        "    'action_noise_sigma': 0.34670933515233754,\n",
        "    'total_timesteps': 50000,\n",
        "}\n",
        "\n",
        "# Update the config dictionary with the best parameters\n",
        "config.update(best_params)\n",
        "\n",
        "# Set the activation function directly based on best_params\n",
        "if best_params['activation_fn'] == 'Tanh':\n",
        "    activation_fn = th.nn.Tanh\n",
        "elif best_params['activation_fn'] == 'ReLU':\n",
        "    activation_fn = th.nn.ReLU\n",
        "elif best_params['activation_fn'] == 'LeakyReLU':\n",
        "    activation_fn = th.nn.LeakyReLU\n",
        "else:\n",
        "    raise ValueError(\"Unsupported activation function\")\n",
        "\n",
        "# # Create policy_kwargs with the fixed network architecture and activation function from best_params\n",
        "policy_kwargs = dict(\n",
        "    activation_fn=activation_fn,  # Use the activation function directly\n",
        "    net_arch=dict(\n",
        "        pi=[2 ** best_params['pi_layer_0_units'], 2 ** best_params['pi_layer_1_units']],\n",
        "        qf=[2 ** best_params['qf_layer_0_units'], 2 ** best_params['qf_layer_1_units']]\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9qGhyfYRQMZ",
        "outputId": "0ef37754-e355-49a2-9785-2604e4a68a71"
      },
      "outputs": [],
      "source": [
        "import torch as th\n",
        "import numpy as np\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.policies import ActorCriticPolicy\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "##################################################################################\n",
        "# Environment and RL Definition\n",
        "##################################################################################\n",
        "\n",
        "# Enter required setpoints for each state. Enter None for states without setpoints.\n",
        "SP = {\n",
        "    'T': [340.0 for _ in range(nsteps)],\n",
        "}\n",
        "\n",
        "# Continuous box action space\n",
        "action_space = {\n",
        "    'low': np.array([250]),\n",
        "    'high': np.array([350])\n",
        "}\n",
        "\n",
        "# Continuous box observation space ([CA, T, CA_Setpoint, T_Setpoint])\n",
        "observation_space = {\n",
        "    'low': np.array([0.0, 300, 300]),\n",
        "    'high': np.array([1, 450, 400])\n",
        "}\n",
        "\n",
        "r_scale = {\n",
        "    'T': 1e-6  # Reward scale for each state,\n",
        "}\n",
        "\n",
        "# Define disturbance bounds\n",
        "disturbance_bounds = {\n",
        "    'low': np.array([330]),\n",
        "    'high': np.array([370])\n",
        "}\n",
        "\n",
        "# Environment parameters\n",
        "env_params_template = {\n",
        "    'Nx': 2,\n",
        "    'N': 60,\n",
        "    'tsim': 26,\n",
        "    'Nu': 1,\n",
        "    'SP': SP,\n",
        "    'o_space': observation_space,\n",
        "    'a_space': action_space,\n",
        "    'x0': np.array([0.87725294608097, 324.475443431599, 324.475443431599]),\n",
        "    'model': 'cstr',\n",
        "    'r_scale': r_scale,\n",
        "    'normalise_a': True,\n",
        "    'normalise_o': True,\n",
        "    'noise': True,\n",
        "    'integration_method': 'casadi',\n",
        "    'noise_percentage': 0.001,  # 0.001,\n",
        "    'disturbance_bounds': disturbance_bounds\n",
        "}\n",
        "\n",
        "# Add noise_percentage from env_params to config\n",
        "config['noise_percentage'] = env_params_template['noise_percentage']\n",
        "\n",
        "# Seed everything for reproducibility\n",
        "def set_global_seeds(seed):\n",
        "    np.random.seed(seed)\n",
        "    th.manual_seed(seed)\n",
        "    if th.cuda.is_available():\n",
        "        th.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "# Function to create random disturbances\n",
        "def create_random_disturbances(seed, nsteps, low=330, high=370):\n",
        "    # Set the global seed for reproducibility\n",
        "    set_global_seeds(seed)\n",
        "    value = np.random.uniform(low, high, 1)[0]  # Generate a single random disturbance value within the specified range\n",
        "    disturbances = {'Ti': [350] * (nsteps // 3) + [value] * (nsteps // 3) + [350] * (nsteps // 3)}  # Repeat each disturbance value for nsteps/3 times\n",
        "    return disturbances\n",
        "\n",
        "# Create multiple environments with different disturbances\n",
        "def create_parallel_envs(n_envs, seed):\n",
        "    set_global_seeds(seed)\n",
        "    envs = []\n",
        "    disturbances_list = []\n",
        "    for i in range(n_envs):\n",
        "        env_params = env_params_template.copy()\n",
        "        disturbances = create_random_disturbances(seed + i, nsteps)\n",
        "        env_params.update({'disturbances': disturbances})\n",
        "        disturbances_list.append(disturbances)\n",
        "        envs.append(lambda: make_env(env_params))\n",
        "    return DummyVecEnv(envs), disturbances_list\n",
        "\n",
        "# Create evaluation environment using DummyVecEnv\n",
        "def create_eval_env(seed, n_envs=1):\n",
        "    set_global_seeds(seed)\n",
        "    envs = []\n",
        "    for i in range(n_envs):\n",
        "        env_params = env_params_template.copy()\n",
        "        disturbances = create_random_disturbances(seed + i, nsteps)\n",
        "        env_params.update({'disturbances': disturbances})\n",
        "        envs.append(lambda: make_env(env_params))\n",
        "    return DummyVecEnv(envs)\n",
        "\n",
        "eval_env = create_eval_env(seed=config['seed'], n_envs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "29c77ed8695a4287956ec27672fe7b7a",
            "5b907d9cb030400c8cd9308246d761c1",
            "bf4a59f3065346d6af96d1fc1a2e6385",
            "c341e0ddb94f4c0b9066e8dc99136430",
            "ac046599192d45cfbf5ab150f9fb240d",
            "483c79836eed4cfab4cfc7320151ef66",
            "e8afd67546254f249e488008aa76f0a8",
            "6446e95478cf4113aad04353ba1c2030"
          ]
        },
        "id": "ZhPjWJ7AfjMs",
        "outputId": "eef1f821-6644-4a6c-9131-889a6f18b697"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First disturbance used in training: {'Ti': [350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350]}\n",
            "Using cuda device\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 4    |\n",
            "|    fps             | 1524 |\n",
            "|    time_elapsed    | 0    |\n",
            "|    total_timesteps | 590  |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 8    |\n",
            "|    fps             | 1520 |\n",
            "|    time_elapsed    | 0    |\n",
            "|    total_timesteps | 590  |\n",
            "-----------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\mfb22\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval num_timesteps=1000, episode_reward=-0.13 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.132   |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1000     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 12   |\n",
            "|    fps             | 679  |\n",
            "|    time_elapsed    | 1    |\n",
            "|    total_timesteps | 1180 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 16   |\n",
            "|    fps             | 679  |\n",
            "|    time_elapsed    | 1    |\n",
            "|    total_timesteps | 1180 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 20   |\n",
            "|    fps             | 679  |\n",
            "|    time_elapsed    | 1    |\n",
            "|    total_timesteps | 1180 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 24   |\n",
            "|    fps             | 795  |\n",
            "|    time_elapsed    | 2    |\n",
            "|    total_timesteps | 1770 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 28   |\n",
            "|    fps             | 795  |\n",
            "|    time_elapsed    | 2    |\n",
            "|    total_timesteps | 1770 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=2000, episode_reward=-0.13 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.131   |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2000     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 32       |\n",
            "|    fps             | 370      |\n",
            "|    time_elapsed    | 6        |\n",
            "|    total_timesteps | 2360     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00227  |\n",
            "|    critic_loss     | 2.56e-05 |\n",
            "|    learning_rate   | 0.000314 |\n",
            "|    n_updates       | 270      |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 36   |\n",
            "|    fps             | 370  |\n",
            "|    time_elapsed    | 6    |\n",
            "|    total_timesteps | 2360 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 40   |\n",
            "|    fps             | 370  |\n",
            "|    time_elapsed    | 6    |\n",
            "|    total_timesteps | 2360 |\n",
            "-----------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 44       |\n",
            "|    fps             | 201      |\n",
            "|    time_elapsed    | 14       |\n",
            "|    total_timesteps | 2950     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000396 |\n",
            "|    critic_loss     | 1.38e-05 |\n",
            "|    learning_rate   | 0.000315 |\n",
            "|    n_updates       | 860      |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 48   |\n",
            "|    fps             | 201  |\n",
            "|    time_elapsed    | 14   |\n",
            "|    total_timesteps | 2950 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=3000, episode_reward=-0.07 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.0735  |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00117  |\n",
            "|    critic_loss     | 1.33e-05 |\n",
            "|    learning_rate   | 0.000315 |\n",
            "|    n_updates       | 910      |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 52       |\n",
            "|    fps             | 132      |\n",
            "|    time_elapsed    | 26       |\n",
            "|    total_timesteps | 3540     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000807 |\n",
            "|    critic_loss     | 7.56e-06 |\n",
            "|    learning_rate   | 0.000316 |\n",
            "|    n_updates       | 1450     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 56   |\n",
            "|    fps             | 132  |\n",
            "|    time_elapsed    | 26   |\n",
            "|    total_timesteps | 3540 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 60   |\n",
            "|    fps             | 132  |\n",
            "|    time_elapsed    | 26   |\n",
            "|    total_timesteps | 3540 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=4000, episode_reward=-0.04 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.04    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000212 |\n",
            "|    critic_loss     | 5.52e-06 |\n",
            "|    learning_rate   | 0.000317 |\n",
            "|    n_updates       | 1910     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 64       |\n",
            "|    fps             | 106      |\n",
            "|    time_elapsed    | 38       |\n",
            "|    total_timesteps | 4130     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000351 |\n",
            "|    critic_loss     | 5.08e-06 |\n",
            "|    learning_rate   | 0.000317 |\n",
            "|    n_updates       | 2040     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 68   |\n",
            "|    fps             | 106  |\n",
            "|    time_elapsed    | 38   |\n",
            "|    total_timesteps | 4130 |\n",
            "-----------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 72       |\n",
            "|    fps             | 101      |\n",
            "|    time_elapsed    | 46       |\n",
            "|    total_timesteps | 4720     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000125 |\n",
            "|    critic_loss     | 4.3e-06  |\n",
            "|    learning_rate   | 0.000318 |\n",
            "|    n_updates       | 2630     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 76   |\n",
            "|    fps             | 101  |\n",
            "|    time_elapsed    | 46   |\n",
            "|    total_timesteps | 4720 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 80   |\n",
            "|    fps             | 101  |\n",
            "|    time_elapsed    | 46   |\n",
            "|    total_timesteps | 4720 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=5000, episode_reward=-0.04 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.0368  |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000803 |\n",
            "|    critic_loss     | 4.33e-06 |\n",
            "|    learning_rate   | 0.000319 |\n",
            "|    n_updates       | 2910     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 84       |\n",
            "|    fps             | 99       |\n",
            "|    time_elapsed    | 53       |\n",
            "|    total_timesteps | 5310     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000999 |\n",
            "|    critic_loss     | 3.51e-06 |\n",
            "|    learning_rate   | 0.000319 |\n",
            "|    n_updates       | 3220     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 88   |\n",
            "|    fps             | 99   |\n",
            "|    time_elapsed    | 53   |\n",
            "|    total_timesteps | 5310 |\n",
            "-----------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 92       |\n",
            "|    fps             | 96       |\n",
            "|    time_elapsed    | 61       |\n",
            "|    total_timesteps | 5900     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000879 |\n",
            "|    critic_loss     | 2.71e-06 |\n",
            "|    learning_rate   | 0.000321 |\n",
            "|    n_updates       | 3810     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 96   |\n",
            "|    fps             | 96   |\n",
            "|    time_elapsed    | 61   |\n",
            "|    total_timesteps | 5900 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 100  |\n",
            "|    fps             | 96   |\n",
            "|    time_elapsed    | 61   |\n",
            "|    total_timesteps | 5900 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=6000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00465 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 6000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000869 |\n",
            "|    critic_loss     | 3.21e-06 |\n",
            "|    learning_rate   | 0.000321 |\n",
            "|    n_updates       | 3910     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 104      |\n",
            "|    fps             | 90       |\n",
            "|    time_elapsed    | 71       |\n",
            "|    total_timesteps | 6490     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00053  |\n",
            "|    critic_loss     | 2.47e-06 |\n",
            "|    learning_rate   | 0.000322 |\n",
            "|    n_updates       | 4400     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 108  |\n",
            "|    fps             | 90   |\n",
            "|    time_elapsed    | 71   |\n",
            "|    total_timesteps | 6490 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=7000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00583 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 7000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000532 |\n",
            "|    critic_loss     | 2.47e-06 |\n",
            "|    learning_rate   | 0.000324 |\n",
            "|    n_updates       | 4910     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 112      |\n",
            "|    fps             | 86       |\n",
            "|    time_elapsed    | 82       |\n",
            "|    total_timesteps | 7080     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000365 |\n",
            "|    critic_loss     | 2.08e-06 |\n",
            "|    learning_rate   | 0.000324 |\n",
            "|    n_updates       | 4990     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 116  |\n",
            "|    fps             | 86   |\n",
            "|    time_elapsed    | 82   |\n",
            "|    total_timesteps | 7080 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 120  |\n",
            "|    fps             | 86   |\n",
            "|    time_elapsed    | 82   |\n",
            "|    total_timesteps | 7080 |\n",
            "-----------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 124      |\n",
            "|    fps             | 85       |\n",
            "|    time_elapsed    | 89       |\n",
            "|    total_timesteps | 7670     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000394 |\n",
            "|    critic_loss     | 1.64e-06 |\n",
            "|    learning_rate   | 0.000326 |\n",
            "|    n_updates       | 5580     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 128  |\n",
            "|    fps             | 85   |\n",
            "|    time_elapsed    | 89   |\n",
            "|    total_timesteps | 7670 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=8000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00529 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000298 |\n",
            "|    critic_loss     | 1.6e-06  |\n",
            "|    learning_rate   | 0.000327 |\n",
            "|    n_updates       | 5910     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 132      |\n",
            "|    fps             | 81       |\n",
            "|    time_elapsed    | 100      |\n",
            "|    total_timesteps | 8260     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000454 |\n",
            "|    critic_loss     | 1.72e-06 |\n",
            "|    learning_rate   | 0.000328 |\n",
            "|    n_updates       | 6170     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 136  |\n",
            "|    fps             | 81   |\n",
            "|    time_elapsed    | 100  |\n",
            "|    total_timesteps | 8260 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 140  |\n",
            "|    fps             | 81   |\n",
            "|    time_elapsed    | 100  |\n",
            "|    total_timesteps | 8260 |\n",
            "-----------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 144      |\n",
            "|    fps             | 80       |\n",
            "|    time_elapsed    | 109      |\n",
            "|    total_timesteps | 8850     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000214 |\n",
            "|    critic_loss     | 1.4e-06  |\n",
            "|    learning_rate   | 0.00033  |\n",
            "|    n_updates       | 6760     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 148  |\n",
            "|    fps             | 80   |\n",
            "|    time_elapsed    | 109  |\n",
            "|    total_timesteps | 8850 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=9000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.0032  |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 9000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.25e-05 |\n",
            "|    critic_loss     | 1.5e-06  |\n",
            "|    learning_rate   | 0.00033  |\n",
            "|    n_updates       | 6910     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 152       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 116       |\n",
            "|    total_timesteps | 9440      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -5.82e-05 |\n",
            "|    critic_loss     | 1.2e-06   |\n",
            "|    learning_rate   | 0.000332  |\n",
            "|    n_updates       | 7350      |\n",
            "----------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 156  |\n",
            "|    fps             | 80   |\n",
            "|    time_elapsed    | 116  |\n",
            "|    total_timesteps | 9440 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 160  |\n",
            "|    fps             | 80   |\n",
            "|    time_elapsed    | 116  |\n",
            "|    total_timesteps | 9440 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=10000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00337 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 10000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 2.56e-05 |\n",
            "|    critic_loss     | 1.22e-06 |\n",
            "|    learning_rate   | 0.000334 |\n",
            "|    n_updates       | 7910     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 164       |\n",
            "|    fps             | 79        |\n",
            "|    time_elapsed    | 125       |\n",
            "|    total_timesteps | 10030     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.36e-05 |\n",
            "|    critic_loss     | 1.59e-06  |\n",
            "|    learning_rate   | 0.000334  |\n",
            "|    n_updates       | 7940      |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 168   |\n",
            "|    fps             | 79    |\n",
            "|    time_elapsed    | 125   |\n",
            "|    total_timesteps | 10030 |\n",
            "------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 172       |\n",
            "|    fps             | 79        |\n",
            "|    time_elapsed    | 133       |\n",
            "|    total_timesteps | 10620     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -5.82e-06 |\n",
            "|    critic_loss     | 1.09e-06  |\n",
            "|    learning_rate   | 0.000337  |\n",
            "|    n_updates       | 8530      |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 176   |\n",
            "|    fps             | 79    |\n",
            "|    time_elapsed    | 133   |\n",
            "|    total_timesteps | 10620 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 180   |\n",
            "|    fps             | 79    |\n",
            "|    time_elapsed    | 133   |\n",
            "|    total_timesteps | 10620 |\n",
            "------------------------------\n",
            "Eval num_timesteps=11000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00293 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 11000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00014  |\n",
            "|    critic_loss     | 1.43e-06 |\n",
            "|    learning_rate   | 0.000338 |\n",
            "|    n_updates       | 8910     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 184      |\n",
            "|    fps             | 79       |\n",
            "|    time_elapsed    | 140      |\n",
            "|    total_timesteps | 11210    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 6.41e-05 |\n",
            "|    critic_loss     | 1.44e-06 |\n",
            "|    learning_rate   | 0.000339 |\n",
            "|    n_updates       | 9120     |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 188   |\n",
            "|    fps             | 79    |\n",
            "|    time_elapsed    | 140   |\n",
            "|    total_timesteps | 11210 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 192      |\n",
            "|    fps             | 80       |\n",
            "|    time_elapsed    | 146      |\n",
            "|    total_timesteps | 11800    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000106 |\n",
            "|    critic_loss     | 1.11e-06 |\n",
            "|    learning_rate   | 0.000342 |\n",
            "|    n_updates       | 9710     |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 196   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 146   |\n",
            "|    total_timesteps | 11800 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 200   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 146   |\n",
            "|    total_timesteps | 11800 |\n",
            "------------------------------\n",
            "Eval num_timesteps=12000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00728 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 12000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 9.62e-05 |\n",
            "|    critic_loss     | 8.93e-07 |\n",
            "|    learning_rate   | 0.000343 |\n",
            "|    n_updates       | 9910     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 204       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 154       |\n",
            "|    total_timesteps | 12390     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.08e-05 |\n",
            "|    critic_loss     | 9.87e-07  |\n",
            "|    learning_rate   | 0.000345  |\n",
            "|    n_updates       | 10300     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 208   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 154   |\n",
            "|    total_timesteps | 12390 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 212      |\n",
            "|    fps             | 80       |\n",
            "|    time_elapsed    | 160      |\n",
            "|    total_timesteps | 12980    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000158 |\n",
            "|    critic_loss     | 1.04e-06 |\n",
            "|    learning_rate   | 0.000348 |\n",
            "|    n_updates       | 10890    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 216   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 160   |\n",
            "|    total_timesteps | 12980 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 220   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 160   |\n",
            "|    total_timesteps | 12980 |\n",
            "------------------------------\n",
            "Eval num_timesteps=13000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.00864  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 13000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -4.18e-05 |\n",
            "|    critic_loss     | 1.12e-06  |\n",
            "|    learning_rate   | 0.000348  |\n",
            "|    n_updates       | 10910     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 224      |\n",
            "|    fps             | 80       |\n",
            "|    time_elapsed    | 167      |\n",
            "|    total_timesteps | 13570    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00022  |\n",
            "|    critic_loss     | 1.02e-06 |\n",
            "|    learning_rate   | 0.000351 |\n",
            "|    n_updates       | 11480    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 228   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 167   |\n",
            "|    total_timesteps | 13570 |\n",
            "------------------------------\n",
            "Eval num_timesteps=14000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.0103  |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 14000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000424 |\n",
            "|    critic_loss     | 1.11e-06 |\n",
            "|    learning_rate   | 0.000353 |\n",
            "|    n_updates       | 11910    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 232      |\n",
            "|    fps             | 80       |\n",
            "|    time_elapsed    | 175      |\n",
            "|    total_timesteps | 14160    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000447 |\n",
            "|    critic_loss     | 8.4e-07  |\n",
            "|    learning_rate   | 0.000354 |\n",
            "|    n_updates       | 12070    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 236   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 175   |\n",
            "|    total_timesteps | 14160 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 240   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 175   |\n",
            "|    total_timesteps | 14160 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 244      |\n",
            "|    fps             | 81       |\n",
            "|    time_elapsed    | 181      |\n",
            "|    total_timesteps | 14750    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000711 |\n",
            "|    critic_loss     | 9.35e-07 |\n",
            "|    learning_rate   | 0.000357 |\n",
            "|    n_updates       | 12660    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 248   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 181   |\n",
            "|    total_timesteps | 14750 |\n",
            "------------------------------\n",
            "Eval num_timesteps=15000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00687 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 15000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000771 |\n",
            "|    critic_loss     | 6.56e-07 |\n",
            "|    learning_rate   | 0.000359 |\n",
            "|    n_updates       | 12910    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 252      |\n",
            "|    fps             | 81       |\n",
            "|    time_elapsed    | 189      |\n",
            "|    total_timesteps | 15340    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000771 |\n",
            "|    critic_loss     | 7.72e-07 |\n",
            "|    learning_rate   | 0.00036  |\n",
            "|    n_updates       | 13250    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 256   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 189   |\n",
            "|    total_timesteps | 15340 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 260   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 189   |\n",
            "|    total_timesteps | 15340 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 264      |\n",
            "|    fps             | 81       |\n",
            "|    time_elapsed    | 195      |\n",
            "|    total_timesteps | 15930    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000708 |\n",
            "|    critic_loss     | 6.52e-07 |\n",
            "|    learning_rate   | 0.000364 |\n",
            "|    n_updates       | 13840    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 268   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 195   |\n",
            "|    total_timesteps | 15930 |\n",
            "------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00536 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 16000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000687 |\n",
            "|    critic_loss     | 6.77e-07 |\n",
            "|    learning_rate   | 0.000364 |\n",
            "|    n_updates       | 13910    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 272      |\n",
            "|    fps             | 81       |\n",
            "|    time_elapsed    | 203      |\n",
            "|    total_timesteps | 16520    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000565 |\n",
            "|    critic_loss     | 6.18e-07 |\n",
            "|    learning_rate   | 0.000367 |\n",
            "|    n_updates       | 14430    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 276   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 203   |\n",
            "|    total_timesteps | 16520 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 280   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 203   |\n",
            "|    total_timesteps | 16520 |\n",
            "------------------------------\n",
            "Eval num_timesteps=17000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00253 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 17000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000481 |\n",
            "|    critic_loss     | 7.39e-07 |\n",
            "|    learning_rate   | 0.00037  |\n",
            "|    n_updates       | 14910    |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 284      |\n",
            "|    fps             | 80       |\n",
            "|    time_elapsed    | 212      |\n",
            "|    total_timesteps | 17110    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000416 |\n",
            "|    critic_loss     | 4.62e-07 |\n",
            "|    learning_rate   | 0.000371 |\n",
            "|    n_updates       | 15020    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 288   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 212   |\n",
            "|    total_timesteps | 17110 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 292      |\n",
            "|    fps             | 80       |\n",
            "|    time_elapsed    | 219      |\n",
            "|    total_timesteps | 17700    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000278 |\n",
            "|    critic_loss     | 5.96e-07 |\n",
            "|    learning_rate   | 0.000375 |\n",
            "|    n_updates       | 15610    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 296   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 219   |\n",
            "|    total_timesteps | 17700 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 300   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 219   |\n",
            "|    total_timesteps | 17700 |\n",
            "------------------------------\n",
            "Eval num_timesteps=18000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00235 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 18000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000242 |\n",
            "|    critic_loss     | 6.66e-07 |\n",
            "|    learning_rate   | 0.000376 |\n",
            "|    n_updates       | 15910    |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 304      |\n",
            "|    fps             | 80       |\n",
            "|    time_elapsed    | 228      |\n",
            "|    total_timesteps | 18290    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.000176 |\n",
            "|    critic_loss     | 6.45e-07 |\n",
            "|    learning_rate   | 0.000378 |\n",
            "|    n_updates       | 16200    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 308   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 228   |\n",
            "|    total_timesteps | 18290 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 312      |\n",
            "|    fps             | 80       |\n",
            "|    time_elapsed    | 235      |\n",
            "|    total_timesteps | 18880    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.44e-05 |\n",
            "|    critic_loss     | 4.99e-07 |\n",
            "|    learning_rate   | 0.000382 |\n",
            "|    n_updates       | 16790    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 316   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 235   |\n",
            "|    total_timesteps | 18880 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 320   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 235   |\n",
            "|    total_timesteps | 18880 |\n",
            "------------------------------\n",
            "Eval num_timesteps=19000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00315 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 19000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.4e-05  |\n",
            "|    critic_loss     | 6.74e-07 |\n",
            "|    learning_rate   | 0.000383 |\n",
            "|    n_updates       | 16910    |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 324       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 242       |\n",
            "|    total_timesteps | 19470     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.04e-05 |\n",
            "|    critic_loss     | 6.3e-07   |\n",
            "|    learning_rate   | 0.000386  |\n",
            "|    n_updates       | 17380     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 328   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 242   |\n",
            "|    total_timesteps | 19470 |\n",
            "------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00257 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 20000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.00011 |\n",
            "|    critic_loss     | 4.44e-07 |\n",
            "|    learning_rate   | 0.000389 |\n",
            "|    n_updates       | 17910    |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 332       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 249       |\n",
            "|    total_timesteps | 20060     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.73e-05 |\n",
            "|    critic_loss     | 5.56e-07  |\n",
            "|    learning_rate   | 0.00039   |\n",
            "|    n_updates       | 17970     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 336   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 249   |\n",
            "|    total_timesteps | 20060 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 340   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 249   |\n",
            "|    total_timesteps | 20060 |\n",
            "------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 344       |\n",
            "|    fps             | 81        |\n",
            "|    time_elapsed    | 254       |\n",
            "|    total_timesteps | 20650     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -8.22e-05 |\n",
            "|    critic_loss     | 5.9e-07   |\n",
            "|    learning_rate   | 0.000394  |\n",
            "|    n_updates       | 18560     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 348   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 254   |\n",
            "|    total_timesteps | 20650 |\n",
            "------------------------------\n",
            "Eval num_timesteps=21000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.00317  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 21000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -7.43e-05 |\n",
            "|    critic_loss     | 3.87e-07  |\n",
            "|    learning_rate   | 0.000396  |\n",
            "|    n_updates       | 18910     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 352       |\n",
            "|    fps             | 81        |\n",
            "|    time_elapsed    | 261       |\n",
            "|    total_timesteps | 21240     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -7.85e-05 |\n",
            "|    critic_loss     | 3.59e-07  |\n",
            "|    learning_rate   | 0.000397  |\n",
            "|    n_updates       | 19150     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 356   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 261   |\n",
            "|    total_timesteps | 21240 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 360   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 261   |\n",
            "|    total_timesteps | 21240 |\n",
            "------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 364       |\n",
            "|    fps             | 81        |\n",
            "|    time_elapsed    | 267       |\n",
            "|    total_timesteps | 21830     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -8.71e-05 |\n",
            "|    critic_loss     | 4.17e-07  |\n",
            "|    learning_rate   | 0.000401  |\n",
            "|    n_updates       | 19740     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 368   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 267   |\n",
            "|    total_timesteps | 21830 |\n",
            "------------------------------\n",
            "Eval num_timesteps=22000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00265 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 22000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 4.77e-06 |\n",
            "|    critic_loss     | 4.21e-07 |\n",
            "|    learning_rate   | 0.000403 |\n",
            "|    n_updates       | 19910    |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 372       |\n",
            "|    fps             | 81        |\n",
            "|    time_elapsed    | 273       |\n",
            "|    total_timesteps | 22420     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -9.33e-06 |\n",
            "|    critic_loss     | 3.98e-07  |\n",
            "|    learning_rate   | 0.000406  |\n",
            "|    n_updates       | 20330     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 376   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 273   |\n",
            "|    total_timesteps | 22420 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 380   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 273   |\n",
            "|    total_timesteps | 22420 |\n",
            "------------------------------\n",
            "Eval num_timesteps=23000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.00337  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 23000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.96e-05 |\n",
            "|    critic_loss     | 4.11e-07  |\n",
            "|    learning_rate   | 0.000409  |\n",
            "|    n_updates       | 20910     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 384       |\n",
            "|    fps             | 82        |\n",
            "|    time_elapsed    | 280       |\n",
            "|    total_timesteps | 23010     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.54e-05 |\n",
            "|    critic_loss     | 4.23e-07  |\n",
            "|    learning_rate   | 0.00041   |\n",
            "|    n_updates       | 20920     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 388   |\n",
            "|    fps             | 82    |\n",
            "|    time_elapsed    | 280   |\n",
            "|    total_timesteps | 23010 |\n",
            "------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 392       |\n",
            "|    fps             | 81        |\n",
            "|    time_elapsed    | 288       |\n",
            "|    total_timesteps | 23600     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.67e-05 |\n",
            "|    critic_loss     | 5.77e-07  |\n",
            "|    learning_rate   | 0.000414  |\n",
            "|    n_updates       | 21510     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 396   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 288   |\n",
            "|    total_timesteps | 23600 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 400   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 288   |\n",
            "|    total_timesteps | 23600 |\n",
            "------------------------------\n",
            "Eval num_timesteps=24000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.00466  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 24000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.86e-05 |\n",
            "|    critic_loss     | 6.03e-07  |\n",
            "|    learning_rate   | 0.000416  |\n",
            "|    n_updates       | 21910     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 404      |\n",
            "|    fps             | 81       |\n",
            "|    time_elapsed    | 296      |\n",
            "|    total_timesteps | 24190    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 2.82e-05 |\n",
            "|    critic_loss     | 4.44e-07 |\n",
            "|    learning_rate   | 0.000418 |\n",
            "|    n_updates       | 22100    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 408   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 296   |\n",
            "|    total_timesteps | 24190 |\n",
            "------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 412       |\n",
            "|    fps             | 81        |\n",
            "|    time_elapsed    | 303       |\n",
            "|    total_timesteps | 24780     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -7.77e-06 |\n",
            "|    critic_loss     | 4e-07     |\n",
            "|    learning_rate   | 0.000422  |\n",
            "|    n_updates       | 22690     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 416   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 303   |\n",
            "|    total_timesteps | 24780 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 420   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 303   |\n",
            "|    total_timesteps | 24780 |\n",
            "------------------------------\n",
            "Eval num_timesteps=25000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.00199  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 25000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.94e-05 |\n",
            "|    critic_loss     | 5.25e-07  |\n",
            "|    learning_rate   | 0.000423  |\n",
            "|    n_updates       | 22910     |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 424      |\n",
            "|    fps             | 81       |\n",
            "|    time_elapsed    | 312      |\n",
            "|    total_timesteps | 25370    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 3.1e-05  |\n",
            "|    critic_loss     | 4.51e-07 |\n",
            "|    learning_rate   | 0.000426 |\n",
            "|    n_updates       | 23280    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 428   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 312   |\n",
            "|    total_timesteps | 25370 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 432      |\n",
            "|    fps             | 81       |\n",
            "|    time_elapsed    | 319      |\n",
            "|    total_timesteps | 25960    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 1.97e-05 |\n",
            "|    critic_loss     | 4.7e-07  |\n",
            "|    learning_rate   | 0.00043  |\n",
            "|    n_updates       | 23870    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 436   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 319   |\n",
            "|    total_timesteps | 25960 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 440   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 319   |\n",
            "|    total_timesteps | 25960 |\n",
            "------------------------------\n",
            "Eval num_timesteps=26000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00404 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 26000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 3.06e-05 |\n",
            "|    critic_loss     | 3.89e-07 |\n",
            "|    learning_rate   | 0.00043  |\n",
            "|    n_updates       | 23910    |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 444       |\n",
            "|    fps             | 81        |\n",
            "|    time_elapsed    | 327       |\n",
            "|    total_timesteps | 26550     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -2.86e-05 |\n",
            "|    critic_loss     | 3.42e-07  |\n",
            "|    learning_rate   | 0.000434  |\n",
            "|    n_updates       | 24460     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 448   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 327   |\n",
            "|    total_timesteps | 26550 |\n",
            "------------------------------\n",
            "Eval num_timesteps=27000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.0027   |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 27000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000105 |\n",
            "|    critic_loss     | 4.1e-07   |\n",
            "|    learning_rate   | 0.000437  |\n",
            "|    n_updates       | 24910     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 452       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 336       |\n",
            "|    total_timesteps | 27140     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -9.44e-05 |\n",
            "|    critic_loss     | 4.06e-07  |\n",
            "|    learning_rate   | 0.000438  |\n",
            "|    n_updates       | 25050     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 456   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 336   |\n",
            "|    total_timesteps | 27140 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 460   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 336   |\n",
            "|    total_timesteps | 27140 |\n",
            "------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 464       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 342       |\n",
            "|    total_timesteps | 27730     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -6.77e-05 |\n",
            "|    critic_loss     | 3.6e-07   |\n",
            "|    learning_rate   | 0.000442  |\n",
            "|    n_updates       | 25640     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 468   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 342   |\n",
            "|    total_timesteps | 27730 |\n",
            "------------------------------\n",
            "Eval num_timesteps=28000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.00169  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 28000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000135 |\n",
            "|    critic_loss     | 4.37e-07  |\n",
            "|    learning_rate   | 0.000444  |\n",
            "|    n_updates       | 25910     |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 472       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 350       |\n",
            "|    total_timesteps | 28320     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000156 |\n",
            "|    critic_loss     | 3.11e-07  |\n",
            "|    learning_rate   | 0.000446  |\n",
            "|    n_updates       | 26230     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 476   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 350   |\n",
            "|    total_timesteps | 28320 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 480   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 350   |\n",
            "|    total_timesteps | 28320 |\n",
            "------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 484       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 358       |\n",
            "|    total_timesteps | 28910     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000278 |\n",
            "|    critic_loss     | 4.81e-07  |\n",
            "|    learning_rate   | 0.00045   |\n",
            "|    n_updates       | 26820     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 488   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 358   |\n",
            "|    total_timesteps | 28910 |\n",
            "------------------------------\n",
            "Eval num_timesteps=29000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.00229  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 29000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000199 |\n",
            "|    critic_loss     | 3.22e-07  |\n",
            "|    learning_rate   | 0.000451  |\n",
            "|    n_updates       | 26910     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 492       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 367       |\n",
            "|    total_timesteps | 29500     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000304 |\n",
            "|    critic_loss     | 5.07e-07  |\n",
            "|    learning_rate   | 0.000454  |\n",
            "|    n_updates       | 27410     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 496   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 367   |\n",
            "|    total_timesteps | 29500 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 500   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 367   |\n",
            "|    total_timesteps | 29500 |\n",
            "------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00209 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 30000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.00028 |\n",
            "|    critic_loss     | 4.79e-07 |\n",
            "|    learning_rate   | 0.000457 |\n",
            "|    n_updates       | 27910    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 504      |\n",
            "|    fps             | 80       |\n",
            "|    time_elapsed    | 376      |\n",
            "|    total_timesteps | 30090    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.00034 |\n",
            "|    critic_loss     | 2.87e-07 |\n",
            "|    learning_rate   | 0.000458 |\n",
            "|    n_updates       | 28000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 508   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 376   |\n",
            "|    total_timesteps | 30090 |\n",
            "------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 512       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 383       |\n",
            "|    total_timesteps | 30680     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000397 |\n",
            "|    critic_loss     | 4e-07     |\n",
            "|    learning_rate   | 0.000462  |\n",
            "|    n_updates       | 28590     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 516   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 383   |\n",
            "|    total_timesteps | 30680 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 520   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 383   |\n",
            "|    total_timesteps | 30680 |\n",
            "------------------------------\n",
            "Eval num_timesteps=31000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.0019   |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 31000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000406 |\n",
            "|    critic_loss     | 3.28e-07  |\n",
            "|    learning_rate   | 0.000464  |\n",
            "|    n_updates       | 28910     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 524       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 390       |\n",
            "|    total_timesteps | 31270     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000394 |\n",
            "|    critic_loss     | 4.81e-07  |\n",
            "|    learning_rate   | 0.000466  |\n",
            "|    n_updates       | 29180     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 528   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 390   |\n",
            "|    total_timesteps | 31270 |\n",
            "------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 532       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 396       |\n",
            "|    total_timesteps | 31860     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000427 |\n",
            "|    critic_loss     | 3.08e-07  |\n",
            "|    learning_rate   | 0.000469  |\n",
            "|    n_updates       | 29770     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 536   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 396   |\n",
            "|    total_timesteps | 31860 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 540   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 396   |\n",
            "|    total_timesteps | 31860 |\n",
            "------------------------------\n",
            "Eval num_timesteps=32000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.00565  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 32000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000381 |\n",
            "|    critic_loss     | 3.31e-07  |\n",
            "|    learning_rate   | 0.00047   |\n",
            "|    n_updates       | 29910     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 544       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 405       |\n",
            "|    total_timesteps | 32450     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000399 |\n",
            "|    critic_loss     | 3.39e-07  |\n",
            "|    learning_rate   | 0.000473  |\n",
            "|    n_updates       | 30360     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 548   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 405   |\n",
            "|    total_timesteps | 32450 |\n",
            "------------------------------\n",
            "Eval num_timesteps=33000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.00619  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 33000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000403 |\n",
            "|    critic_loss     | 4.15e-07  |\n",
            "|    learning_rate   | 0.000476  |\n",
            "|    n_updates       | 30910     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 552       |\n",
            "|    fps             | 79        |\n",
            "|    time_elapsed    | 414       |\n",
            "|    total_timesteps | 33040     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000439 |\n",
            "|    critic_loss     | 2.87e-07  |\n",
            "|    learning_rate   | 0.000477  |\n",
            "|    n_updates       | 30950     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 556   |\n",
            "|    fps             | 79    |\n",
            "|    time_elapsed    | 414   |\n",
            "|    total_timesteps | 33040 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 560   |\n",
            "|    fps             | 79    |\n",
            "|    time_elapsed    | 414   |\n",
            "|    total_timesteps | 33040 |\n",
            "------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 564       |\n",
            "|    fps             | 79        |\n",
            "|    time_elapsed    | 421       |\n",
            "|    total_timesteps | 33630     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000457 |\n",
            "|    critic_loss     | 3.27e-07  |\n",
            "|    learning_rate   | 0.00048   |\n",
            "|    n_updates       | 31540     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 568   |\n",
            "|    fps             | 79    |\n",
            "|    time_elapsed    | 421   |\n",
            "|    total_timesteps | 33630 |\n",
            "------------------------------\n",
            "Eval num_timesteps=34000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00591 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 34000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.00047 |\n",
            "|    critic_loss     | 2.67e-07 |\n",
            "|    learning_rate   | 0.000482 |\n",
            "|    n_updates       | 31910    |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 572       |\n",
            "|    fps             | 79        |\n",
            "|    time_elapsed    | 429       |\n",
            "|    total_timesteps | 34220     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000408 |\n",
            "|    critic_loss     | 2.57e-07  |\n",
            "|    learning_rate   | 0.000484  |\n",
            "|    n_updates       | 32130     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 576   |\n",
            "|    fps             | 79    |\n",
            "|    time_elapsed    | 429   |\n",
            "|    total_timesteps | 34220 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 580   |\n",
            "|    fps             | 79    |\n",
            "|    time_elapsed    | 429   |\n",
            "|    total_timesteps | 34220 |\n",
            "------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 584       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 434       |\n",
            "|    total_timesteps | 34810     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000426 |\n",
            "|    critic_loss     | 3.81e-07  |\n",
            "|    learning_rate   | 0.000487  |\n",
            "|    n_updates       | 32720     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 588   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 434   |\n",
            "|    total_timesteps | 34810 |\n",
            "------------------------------\n",
            "Eval num_timesteps=35000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.00537  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 35000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000438 |\n",
            "|    critic_loss     | 2.82e-07  |\n",
            "|    learning_rate   | 0.000488  |\n",
            "|    n_updates       | 32910     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 592       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 442       |\n",
            "|    total_timesteps | 35400     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000453 |\n",
            "|    critic_loss     | 3.6e-07   |\n",
            "|    learning_rate   | 0.00049   |\n",
            "|    n_updates       | 33310     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 596   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 442   |\n",
            "|    total_timesteps | 35400 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 600   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 442   |\n",
            "|    total_timesteps | 35400 |\n",
            "------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 604       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 448       |\n",
            "|    total_timesteps | 35990     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000473 |\n",
            "|    critic_loss     | 3.11e-07  |\n",
            "|    learning_rate   | 0.000493  |\n",
            "|    n_updates       | 33900     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 608   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 448   |\n",
            "|    total_timesteps | 35990 |\n",
            "------------------------------\n",
            "Eval num_timesteps=36000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.00541  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 36000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000415 |\n",
            "|    critic_loss     | 3.07e-07  |\n",
            "|    learning_rate   | 0.000494  |\n",
            "|    n_updates       | 33910     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 612       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 455       |\n",
            "|    total_timesteps | 36580     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000442 |\n",
            "|    critic_loss     | 3.36e-07  |\n",
            "|    learning_rate   | 0.000497  |\n",
            "|    n_updates       | 34490     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 616   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 455   |\n",
            "|    total_timesteps | 36580 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 620   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 455   |\n",
            "|    total_timesteps | 36580 |\n",
            "------------------------------\n",
            "Eval num_timesteps=37000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.0083   |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 37000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000462 |\n",
            "|    critic_loss     | 4.97e-07  |\n",
            "|    learning_rate   | 0.000499  |\n",
            "|    n_updates       | 34910     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 624       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 462       |\n",
            "|    total_timesteps | 37170     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000428 |\n",
            "|    critic_loss     | 2.68e-07  |\n",
            "|    learning_rate   | 0.0005    |\n",
            "|    n_updates       | 35080     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 628   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 462   |\n",
            "|    total_timesteps | 37170 |\n",
            "------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 632       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 468       |\n",
            "|    total_timesteps | 37760     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000486 |\n",
            "|    critic_loss     | 2.43e-07  |\n",
            "|    learning_rate   | 0.000502  |\n",
            "|    n_updates       | 35670     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 636   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 468   |\n",
            "|    total_timesteps | 37760 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 640   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 468   |\n",
            "|    total_timesteps | 37760 |\n",
            "------------------------------\n",
            "Eval num_timesteps=38000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.00669  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 38000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000458 |\n",
            "|    critic_loss     | 4.33e-07  |\n",
            "|    learning_rate   | 0.000504  |\n",
            "|    n_updates       | 35910     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 644       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 477       |\n",
            "|    total_timesteps | 38350     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000471 |\n",
            "|    critic_loss     | 3.69e-07  |\n",
            "|    learning_rate   | 0.000505  |\n",
            "|    n_updates       | 36260     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 648   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 477   |\n",
            "|    total_timesteps | 38350 |\n",
            "------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 652       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 483       |\n",
            "|    total_timesteps | 38940     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000469 |\n",
            "|    critic_loss     | 2.84e-07  |\n",
            "|    learning_rate   | 0.000508  |\n",
            "|    n_updates       | 36850     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 656   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 483   |\n",
            "|    total_timesteps | 38940 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 660   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 483   |\n",
            "|    total_timesteps | 38940 |\n",
            "------------------------------\n",
            "Eval num_timesteps=39000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.00947  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 39000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000465 |\n",
            "|    critic_loss     | 3.93e-07  |\n",
            "|    learning_rate   | 0.000508  |\n",
            "|    n_updates       | 36910     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 664       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 491       |\n",
            "|    total_timesteps | 39530     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000514 |\n",
            "|    critic_loss     | 3.38e-07  |\n",
            "|    learning_rate   | 0.00051   |\n",
            "|    n_updates       | 37440     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 668   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 491   |\n",
            "|    total_timesteps | 39530 |\n",
            "------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.00969  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 40000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000451 |\n",
            "|    critic_loss     | 3.91e-07  |\n",
            "|    learning_rate   | 0.000512  |\n",
            "|    n_updates       | 37910     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 672       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 499       |\n",
            "|    total_timesteps | 40120     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000457 |\n",
            "|    critic_loss     | 3.15e-07  |\n",
            "|    learning_rate   | 0.000513  |\n",
            "|    n_updates       | 38030     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 676   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 499   |\n",
            "|    total_timesteps | 40120 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 680   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 499   |\n",
            "|    total_timesteps | 40120 |\n",
            "------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 684       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 507       |\n",
            "|    total_timesteps | 40710     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000393 |\n",
            "|    critic_loss     | 2.58e-07  |\n",
            "|    learning_rate   | 0.000515  |\n",
            "|    n_updates       | 38620     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 688   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 507   |\n",
            "|    total_timesteps | 40710 |\n",
            "------------------------------\n",
            "Eval num_timesteps=41000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.00707  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 41000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000405 |\n",
            "|    critic_loss     | 2.55e-07  |\n",
            "|    learning_rate   | 0.000516  |\n",
            "|    n_updates       | 38910     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 692       |\n",
            "|    fps             | 79        |\n",
            "|    time_elapsed    | 517       |\n",
            "|    total_timesteps | 41300     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000419 |\n",
            "|    critic_loss     | 2.85e-07  |\n",
            "|    learning_rate   | 0.000517  |\n",
            "|    n_updates       | 39210     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 696   |\n",
            "|    fps             | 79    |\n",
            "|    time_elapsed    | 517   |\n",
            "|    total_timesteps | 41300 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 700   |\n",
            "|    fps             | 79    |\n",
            "|    time_elapsed    | 517   |\n",
            "|    total_timesteps | 41300 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 704      |\n",
            "|    fps             | 79       |\n",
            "|    time_elapsed    | 523      |\n",
            "|    total_timesteps | 41890    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.00036 |\n",
            "|    critic_loss     | 3.13e-07 |\n",
            "|    learning_rate   | 0.000519 |\n",
            "|    n_updates       | 39800    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 708   |\n",
            "|    fps             | 79    |\n",
            "|    time_elapsed    | 523   |\n",
            "|    total_timesteps | 41890 |\n",
            "------------------------------\n",
            "Eval num_timesteps=42000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.00718  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 42000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000381 |\n",
            "|    critic_loss     | 2.24e-07  |\n",
            "|    learning_rate   | 0.00052   |\n",
            "|    n_updates       | 39910     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 712       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 530       |\n",
            "|    total_timesteps | 42480     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000377 |\n",
            "|    critic_loss     | 3.18e-07  |\n",
            "|    learning_rate   | 0.000521  |\n",
            "|    n_updates       | 40390     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 716   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 530   |\n",
            "|    total_timesteps | 42480 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 720   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 530   |\n",
            "|    total_timesteps | 42480 |\n",
            "------------------------------\n",
            "Eval num_timesteps=43000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.0053   |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 43000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000334 |\n",
            "|    critic_loss     | 3.76e-07  |\n",
            "|    learning_rate   | 0.000523  |\n",
            "|    n_updates       | 40910     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 724       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 537       |\n",
            "|    total_timesteps | 43070     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000278 |\n",
            "|    critic_loss     | 3.6e-07   |\n",
            "|    learning_rate   | 0.000523  |\n",
            "|    n_updates       | 40980     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 728   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 537   |\n",
            "|    total_timesteps | 43070 |\n",
            "------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 732       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 542       |\n",
            "|    total_timesteps | 43660     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000308 |\n",
            "|    critic_loss     | 3.1e-07   |\n",
            "|    learning_rate   | 0.000525  |\n",
            "|    n_updates       | 41570     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 736   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 542   |\n",
            "|    total_timesteps | 43660 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 740   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 542   |\n",
            "|    total_timesteps | 43660 |\n",
            "------------------------------\n",
            "Eval num_timesteps=44000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.00665  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 44000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000352 |\n",
            "|    critic_loss     | 2.78e-07  |\n",
            "|    learning_rate   | 0.000526  |\n",
            "|    n_updates       | 41910     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 744       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 549       |\n",
            "|    total_timesteps | 44250     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000262 |\n",
            "|    critic_loss     | 3.92e-07  |\n",
            "|    learning_rate   | 0.000526  |\n",
            "|    n_updates       | 42160     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 748   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 549   |\n",
            "|    total_timesteps | 44250 |\n",
            "------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 752       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 554       |\n",
            "|    total_timesteps | 44840     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000267 |\n",
            "|    critic_loss     | 4.53e-07  |\n",
            "|    learning_rate   | 0.000528  |\n",
            "|    n_updates       | 42750     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 756   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 554   |\n",
            "|    total_timesteps | 44840 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 760   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 554   |\n",
            "|    total_timesteps | 44840 |\n",
            "------------------------------\n",
            "Eval num_timesteps=45000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.00743  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 45000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000272 |\n",
            "|    critic_loss     | 2.93e-07  |\n",
            "|    learning_rate   | 0.000528  |\n",
            "|    n_updates       | 42910     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 764       |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 561       |\n",
            "|    total_timesteps | 45430     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000212 |\n",
            "|    critic_loss     | 3.28e-07  |\n",
            "|    learning_rate   | 0.000529  |\n",
            "|    n_updates       | 43340     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 768   |\n",
            "|    fps             | 80    |\n",
            "|    time_elapsed    | 561   |\n",
            "|    total_timesteps | 45430 |\n",
            "------------------------------\n",
            "Eval num_timesteps=46000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.0106   |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 46000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000214 |\n",
            "|    critic_loss     | 3.07e-07  |\n",
            "|    learning_rate   | 0.00053   |\n",
            "|    n_updates       | 43910     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 772      |\n",
            "|    fps             | 81       |\n",
            "|    time_elapsed    | 567      |\n",
            "|    total_timesteps | 46020    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.00022 |\n",
            "|    critic_loss     | 2.51e-07 |\n",
            "|    learning_rate   | 0.00053  |\n",
            "|    n_updates       | 43930    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 776   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 567   |\n",
            "|    total_timesteps | 46020 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 780   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 567   |\n",
            "|    total_timesteps | 46020 |\n",
            "------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 784       |\n",
            "|    fps             | 81        |\n",
            "|    time_elapsed    | 573       |\n",
            "|    total_timesteps | 46610     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000181 |\n",
            "|    critic_loss     | 4.01e-07  |\n",
            "|    learning_rate   | 0.000531  |\n",
            "|    n_updates       | 44520     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 788   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 573   |\n",
            "|    total_timesteps | 46610 |\n",
            "------------------------------\n",
            "Eval num_timesteps=47000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.0119   |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 47000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000176 |\n",
            "|    critic_loss     | 2.25e-07  |\n",
            "|    learning_rate   | 0.000532  |\n",
            "|    n_updates       | 44910     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 792       |\n",
            "|    fps             | 81        |\n",
            "|    time_elapsed    | 579       |\n",
            "|    total_timesteps | 47200     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000166 |\n",
            "|    critic_loss     | 3.45e-07  |\n",
            "|    learning_rate   | 0.000532  |\n",
            "|    n_updates       | 45110     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 796   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 579   |\n",
            "|    total_timesteps | 47200 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 800   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 579   |\n",
            "|    total_timesteps | 47200 |\n",
            "------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 804       |\n",
            "|    fps             | 81        |\n",
            "|    time_elapsed    | 585       |\n",
            "|    total_timesteps | 47790     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000157 |\n",
            "|    critic_loss     | 3.66e-07  |\n",
            "|    learning_rate   | 0.000532  |\n",
            "|    n_updates       | 45700     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 808   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 585   |\n",
            "|    total_timesteps | 47790 |\n",
            "------------------------------\n",
            "Eval num_timesteps=48000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00563 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 48000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.00012 |\n",
            "|    critic_loss     | 3.99e-07 |\n",
            "|    learning_rate   | 0.000533 |\n",
            "|    n_updates       | 45910    |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 812       |\n",
            "|    fps             | 81        |\n",
            "|    time_elapsed    | 591       |\n",
            "|    total_timesteps | 48380     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.000122 |\n",
            "|    critic_loss     | 3.26e-07  |\n",
            "|    learning_rate   | 0.000533  |\n",
            "|    n_updates       | 46290     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 816   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 591   |\n",
            "|    total_timesteps | 48380 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 820   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 591   |\n",
            "|    total_timesteps | 48380 |\n",
            "------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 824       |\n",
            "|    fps             | 81        |\n",
            "|    time_elapsed    | 597       |\n",
            "|    total_timesteps | 48970     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -7.31e-05 |\n",
            "|    critic_loss     | 2.2e-07   |\n",
            "|    learning_rate   | 0.000533  |\n",
            "|    n_updates       | 46880     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 828   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 597   |\n",
            "|    total_timesteps | 48970 |\n",
            "------------------------------\n",
            "Eval num_timesteps=49000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.00904  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 49000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -6.74e-05 |\n",
            "|    critic_loss     | 3.18e-07  |\n",
            "|    learning_rate   | 0.000533  |\n",
            "|    n_updates       | 46910     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 832       |\n",
            "|    fps             | 81        |\n",
            "|    time_elapsed    | 604       |\n",
            "|    total_timesteps | 49560     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -7.97e-06 |\n",
            "|    critic_loss     | 2.82e-07  |\n",
            "|    learning_rate   | 0.000533  |\n",
            "|    n_updates       | 47470     |\n",
            "----------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 836   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 604   |\n",
            "|    total_timesteps | 49560 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 840   |\n",
            "|    fps             | 81    |\n",
            "|    time_elapsed    | 604   |\n",
            "|    total_timesteps | 49560 |\n",
            "------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.00897  |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 50000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -3.87e-05 |\n",
            "|    critic_loss     | 2.73e-07  |\n",
            "|    learning_rate   | 0.000533  |\n",
            "|    n_updates       | 47910     |\n",
            "----------------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.noise import OrnsteinUhlenbeckActionNoise\n",
        "\n",
        "# Set the global seed for reproducibility\n",
        "set_global_seeds(config['seed'])\n",
        "\n",
        "# Number of parallel environments\n",
        "n_envs = 10\n",
        "\n",
        "# Create the parallel environments and get the disturbances\n",
        "env, disturbances_list = create_parallel_envs(n_envs, config['seed'])\n",
        "\n",
        "# Set up Ornstein-Uhlenbeck action noise after creating the environment\n",
        "n_actions = env.action_space.shape\n",
        "action_noise = OrnsteinUhlenbeckActionNoise(\n",
        "    mean=np.zeros(n_actions), \n",
        "    sigma=config['action_noise_sigma'] * np.ones(n_actions)\n",
        ")\n",
        "\n",
        "# Print the first disturbances\n",
        "print(\"First disturbance used in training:\", disturbances_list[0])\n",
        "\n",
        "model = DDPG(\n",
        "    config['policy'],\n",
        "    env,\n",
        "    learning_rate=0.0001\n",
        "    # learning_rate=lambda progress: cosine_annealing_schedule(progress, \n",
        "    #     min_lr=config['min_lr'], max_lr=config['max_lr']),  # Cosine schedule\n",
        "    # batch_size=config['batch_size'],\n",
        "    # gamma=config['gamma'],\n",
        "    # tau=config['tau'],\n",
        "    # buffer_size=config['buffer_size'],\n",
        "    # learning_starts=config['learning_starts'],\n",
        "    # train_freq=config['train_freq'],\n",
        "    # action_noise=action_noise,\n",
        "    # policy_kwargs=policy_kwargs,  # Use custom policy network structure\n",
        "    seed=seed,\n",
        "    verbose=1,\n",
        "    # Adding the optimized DDPG-specific parameters to the model\n",
        "\n",
        "\n",
        ")\n",
        "\n",
        "# Create the evaluation environment\n",
        "eval_env = create_eval_env(seed)\n",
        "\n",
        "\n",
        "eval_callback = EvalCallback(\n",
        "    eval_env,\n",
        "    best_model_save_path=save_dir,\n",
        "    log_path=save_dir,\n",
        "    eval_freq=config['check_freq'],\n",
        "    n_eval_episodes=config['n_eval_episodes'],\n",
        "    deterministic=True,\n",
        "    render=False\n",
        ")\n",
        "# Train the model with the callbacks\n",
        "model.learn(total_timesteps=config['total_timesteps'], callback=eval_callback)\n",
        "\n",
        "# Save the model\n",
        "model.save(save_dir)\n",
        "\n",
        "# Save the disturbances to a file for future reference\n",
        "disturbances_file_path = os.path.join(save_dir, \"disturbances_used_in_training.txt\")\n",
        "with open(disturbances_file_path, \"w\") as f:\n",
        "    for i, disturbance in enumerate(disturbances_list):\n",
        "        f.write(f\"Disturbance {i+1}: {disturbance}\\n\")\n",
        "\n",
        "# Finish the W&B run\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "6x1u30y1uN2_"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02ab575e267f4264a325dcea50170518": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d397a3c58bdb4f09b7e774b30727d8ae",
              "IPY_MODEL_5e12a8eb338042f8b42b6f9a35813403"
            ],
            "layout": "IPY_MODEL_cc9fd941a0004a3ebb1d5d66a1013aaf"
          }
        },
        "15627f8991b54e8b961444ef8481086b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19a083985d3e466cb8130c07d692901c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29c77ed8695a4287956ec27672fe7b7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b907d9cb030400c8cd9308246d761c1",
              "IPY_MODEL_bf4a59f3065346d6af96d1fc1a2e6385"
            ],
            "layout": "IPY_MODEL_c341e0ddb94f4c0b9066e8dc99136430"
          }
        },
        "32fd32bd040841959acbae2d2e1b8365": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1a6dbe39e8e492baac9782ed9b7230e",
            "placeholder": "​",
            "style": "IPY_MODEL_b74942f112ea4933b0c5ba6176c6b142",
            "value": "0.013 MB of 0.013 MB uploaded\r"
          }
        },
        "35e1b6be6af8413d95e14d6ce68f3f98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32fd32bd040841959acbae2d2e1b8365",
              "IPY_MODEL_52dbfe8dcd8d4a968987b49e4643feeb"
            ],
            "layout": "IPY_MODEL_15627f8991b54e8b961444ef8481086b"
          }
        },
        "483c79836eed4cfab4cfc7320151ef66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "518bd8accca940b284dfc562ca9b823c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "52dbfe8dcd8d4a968987b49e4643feeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2ce0aa2308849fb9a56cbd94aad8fa9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b5526159b14423dbcc1228e4a1f7699",
            "value": 1
          }
        },
        "5b5526159b14423dbcc1228e4a1f7699": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b907d9cb030400c8cd9308246d761c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac046599192d45cfbf5ab150f9fb240d",
            "placeholder": "​",
            "style": "IPY_MODEL_483c79836eed4cfab4cfc7320151ef66",
            "value": "0.674 MB of 0.674 MB uploaded\r"
          }
        },
        "5e12a8eb338042f8b42b6f9a35813403": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae728950ce914f39a035a67662e99bad",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_518bd8accca940b284dfc562ca9b823c",
            "value": 1
          }
        },
        "6446e95478cf4113aad04353ba1c2030": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac046599192d45cfbf5ab150f9fb240d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae728950ce914f39a035a67662e99bad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b74942f112ea4933b0c5ba6176c6b142": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf4a59f3065346d6af96d1fc1a2e6385": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8afd67546254f249e488008aa76f0a8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6446e95478cf4113aad04353ba1c2030",
            "value": 1
          }
        },
        "c2ce0aa2308849fb9a56cbd94aad8fa9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c341e0ddb94f4c0b9066e8dc99136430": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc9fd941a0004a3ebb1d5d66a1013aaf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfb98e5737c9486fb426884c14e5ae1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1a6dbe39e8e492baac9782ed9b7230e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d397a3c58bdb4f09b7e774b30727d8ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19a083985d3e466cb8130c07d692901c",
            "placeholder": "​",
            "style": "IPY_MODEL_cfb98e5737c9486fb426884c14e5ae1f",
            "value": "0.013 MB of 0.013 MB uploaded\r"
          }
        },
        "e8afd67546254f249e488008aa76f0a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-BcG1g4QbE_"
      },
      "source": [
        "# Cloning the Repository (pc-gym)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxWOGAsaECz2",
        "outputId": "899c63bc-c202-4dbc-eb5e-9de200616a0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Volume in drive C is Windows \n",
            " Volume Serial Number is F0B8-32A3\n",
            "\n",
            " Directory of c:\\Users\\Usuario\\Desktop\\imperial_projects\\VSCode\\pcgym2\\train\\base\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "File Not Found\n"
          ]
        }
      ],
      "source": [
        "!dir pc-gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVv4nhKnJZyQ",
        "outputId": "0c6c2f7d-37db-4485-a590-41299d74c51c"
      },
      "outputs": [],
      "source": [
        "# %cd pc-gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pXeY_oDVTPZ"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhR6YfBEbVRM",
        "outputId": "a67140ae-cfff-4a16-fbd8-dc7974929319"
      },
      "outputs": [],
      "source": [
        "# %cd src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %cd pcgym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Usuario\\Desktop\\imperial_projects\\VSCode\\pcgym2\\pc-gym\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\Desktop\\imperial_projects\\VSCode\\pcgym2\\new_venv\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "%cd C:/Users/Usuario/Desktop/imperial_projects/VSCode/pcgym2/pc-gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0xr48fylffH",
        "outputId": "504b6be1-dc60-4c93-f087-158249487b25"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "from stable_baselines3 import PPO,SAC,DDPG,TD3\n",
        "import pcgym\n",
        "from pcgym import make_env\n",
        "import jax.numpy as jnp\n",
        "#Global params\n",
        "T = 26\n",
        "nsteps = 60\n",
        "# Global seed for reproducibility\n",
        "seed = 1990"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMPAn1SRd32f"
      },
      "source": [
        "# Saving and loading\n",
        "\n",
        "Saving and loading stable-baselines models is straightforward: you can directly call `.save()` and `.load()` on the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mph7ddv8iCjY",
        "outputId": "fc007c61-f2cd-4545-d8ee-15f900a0eb6c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Create save dir\n",
        "save_dir = \"./max/sac\"\n",
        "os.makedirs(save_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPlCXGwWFPTI"
      },
      "source": [
        "# Monitoring experiments with W&B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8sM9B5fOG_w",
        "outputId": "6c14e127-7339-4421-9a3b-f4c37cd16d6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Usuario\\.netrc\n"
          ]
        }
      ],
      "source": [
        "# !wandb login\n",
        "# api_key: 84af17cc9914cf1736f3a8e2733a2f361e4750bb\n",
        "\n",
        "!wandb login 84af17cc9914cf1736f3a8e2733a2f361e4750bb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sWhTer-Wg7X"
      },
      "source": [
        "# 1.1 Reactor Case Study"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZklHwMtzkOAE",
        "outputId": "209ef879-e2cb-48ac-ce44-0c2300707462"
      },
      "outputs": [],
      "source": [
        "# @title Function to log the performance data\n",
        "def log_performance(performance, test_label, file_path):\n",
        "    with open(file_path, \"a\") as file:\n",
        "        file.write(f\"{test_label}: \\n\")\n",
        "        file.write(f\"scalarised_performance: {performance}\\n\\n\")\n",
        "\n",
        "file_path = f\"{save_dir}/lcb_metric_safe.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OtCnIncfray"
      },
      "source": [
        "### RL training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch as th\n",
        "from torch import nn  # Import torch.nn for activation functions\n",
        "\n",
        "# Learning rate decay schedule\n",
        "def cosine_annealing_schedule(progress_remaining: float, num_cycles=1, min_lr=0.005, max_lr=0.01):\n",
        "    progress = 1.0 - progress_remaining\n",
        "    lr = min_lr + (max_lr - min_lr) / 2 * (1 + np.cos(np.pi * num_cycles * progress))\n",
        "    return lr\n",
        "\n",
        "# Configuration for reinforcement learning model\n",
        "config = {\n",
        "    \"policy\": 'MlpPolicy',  # default: MlpPolicy\n",
        "    \"seed\": 1990,\n",
        "    \"check_freq\": 1e3,  # base: 12000 (~100 episodes)\n",
        "    \"n_eval_episodes\": 1,  # evaluate the agent over 100 episodes in the evaluation environment\n",
        "    \"positive_definiteness_penalty_weight\": 0,  # Set to 0 initially\n",
        "    \"derivative_penalty_weight\": 0,  # Set to 0 initially\n",
        "    'use_direct_penalty': False,  # choose between applying a penalty directly to the critic loss or adjusting the Q target values (derivative penalty)\n",
        "    'allowed_increase_factor': 1,  # max increase value for both methods (derivative penalty)\n",
        "    'total_timesteps': 50000\n",
        "}\n",
        "\n",
        "# The best hyperparameters found in previous runs\n",
        "best_params = {\n",
        "    'min_lr': 6.298973459520807e-05,\n",
        "    'max_lr': 0.010222529480333347,\n",
        "    'pi_layer_0_units': 2,\n",
        "    'pi_layer_1_units': 3,\n",
        "    'qf_layer_0_units': 4,\n",
        "    'qf_layer_1_units': 6,\n",
        "    'activation_fn': 'ReLU',\n",
        "    'buffer_size': 500000,\n",
        "    'batch_size': 256,\n",
        "    'gamma': 0.9795136621593099,\n",
        "    'tau': 0.019991455145873808,\n",
        "    'learning_starts': 613,\n",
        "    'train_freq': 16,\n",
        "    'gradient_steps': -1,\n",
        "    'ent_coef': 'auto',\n",
        "    'total_timesteps': 50000\n",
        "    # 'total_timesteps': 500000\n",
        "}\n",
        "\n",
        "# Update the config dictionary with the best parameters\n",
        "config.update(best_params)\n",
        "\n",
        "# Set the activation function directly based on best_params\n",
        "if best_params['activation_fn'] == 'Tanh':\n",
        "    activation_fn = th.nn.Tanh\n",
        "elif best_params['activation_fn'] == 'ReLU':\n",
        "    activation_fn = th.nn.ReLU\n",
        "elif best_params['activation_fn'] == 'LeakyReLU':\n",
        "    activation_fn = th.nn.LeakyReLU\n",
        "else:\n",
        "    raise ValueError(\"Unsupported activation function\")\n",
        "\n",
        "# # Create policy_kwargs with the fixed network architecture and activation function from best_params\n",
        "policy_kwargs = dict(\n",
        "    activation_fn=activation_fn,  # Use the activation function directly\n",
        "    net_arch=dict(\n",
        "        pi=[2 ** best_params['pi_layer_0_units'], 2 ** best_params['pi_layer_1_units']],\n",
        "        qf=[2 ** best_params['qf_layer_0_units'], 2 ** best_params['qf_layer_1_units']]\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9qGhyfYRQMZ",
        "outputId": "0ef37754-e355-49a2-9785-2604e4a68a71"
      },
      "outputs": [],
      "source": [
        "import torch as th\n",
        "import numpy as np\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.policies import ActorCriticPolicy\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "##################################################################################\n",
        "# Environment and RL Definition\n",
        "##################################################################################\n",
        "\n",
        "# Enter required setpoints for each state. Enter None for states without setpoints.\n",
        "SP = {\n",
        "    'T': [340.0 for _ in range(nsteps)],\n",
        "}\n",
        "\n",
        "# Continuous box action space\n",
        "action_space = {\n",
        "    'low': np.array([250]),\n",
        "    'high': np.array([350])\n",
        "}\n",
        "\n",
        "# Continuous box observation space ([CA, T, CA_Setpoint, T_Setpoint])\n",
        "observation_space = {\n",
        "    'low': np.array([0.0, 300, 300]),\n",
        "    'high': np.array([1, 450, 400])\n",
        "}\n",
        "\n",
        "r_scale = {\n",
        "    'T': 1e-6  # Reward scale for each state,\n",
        "}\n",
        "\n",
        "# Define disturbance bounds\n",
        "disturbance_bounds = {\n",
        "    'low': np.array([310]),\n",
        "    'high': np.array([390])\n",
        "}\n",
        "\n",
        "# Environment parameters\n",
        "env_params_template = {\n",
        "    'Nx': 2,\n",
        "    'N': 60,\n",
        "    'tsim': 26,\n",
        "    'Nu': 1,\n",
        "    'SP': SP,\n",
        "    'o_space': observation_space,\n",
        "    'a_space': action_space,\n",
        "    'x0': np.array([0.87725294608097, 324.475443431599, 324.475443431599]),\n",
        "    'model': 'cstr',\n",
        "    'r_scale': r_scale,\n",
        "    'normalise_a': True,\n",
        "    'normalise_o': True,\n",
        "    'noise': True,\n",
        "    'integration_method': 'casadi',\n",
        "    'noise_percentage': 0.001,  # 0.001,\n",
        "    'disturbance_bounds': disturbance_bounds\n",
        "}\n",
        "\n",
        "# Add noise_percentage from env_params to config\n",
        "config['noise_percentage'] = env_params_template['noise_percentage']\n",
        "\n",
        "# Seed everything for reproducibility\n",
        "def set_global_seeds(seed):\n",
        "    np.random.seed(seed)\n",
        "    th.manual_seed(seed)\n",
        "    if th.cuda.is_available():\n",
        "        th.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "# Function to create random disturbances\n",
        "def create_random_disturbances(seed, nsteps, low=330, high=370):\n",
        "    # Set the global seed for reproducibility\n",
        "    set_global_seeds(seed)\n",
        "    value = np.random.uniform(low, high, 1)[0]  # Generate a single random disturbance value within the specified range\n",
        "    disturbances = {'Ti': [350] * (nsteps // 3) + [value] * (nsteps // 3) + [350] * (nsteps // 3)}  # Repeat each disturbance value for nsteps/3 times\n",
        "    return disturbances\n",
        "\n",
        "# Create multiple environments with different disturbances\n",
        "def create_parallel_envs(n_envs, seed):\n",
        "    set_global_seeds(seed)\n",
        "    envs = []\n",
        "    disturbances_list = []\n",
        "    for i in range(n_envs):\n",
        "        env_params = env_params_template.copy()\n",
        "        disturbances = create_random_disturbances(seed + i, nsteps)\n",
        "        env_params.update({'disturbances': disturbances})\n",
        "        disturbances_list.append(disturbances)\n",
        "        envs.append(lambda: make_env(env_params))\n",
        "    return DummyVecEnv(envs), disturbances_list\n",
        "\n",
        "# Create evaluation environment using DummyVecEnv\n",
        "def create_eval_env(seed, n_envs=1):\n",
        "    set_global_seeds(seed)\n",
        "    envs = []\n",
        "    for i in range(n_envs):\n",
        "        env_params = env_params_template.copy()\n",
        "        disturbances = create_random_disturbances(seed + i, nsteps)\n",
        "        env_params.update({'disturbances': disturbances})\n",
        "        envs.append(lambda: make_env(env_params))\n",
        "    return DummyVecEnv(envs)\n",
        "\n",
        "eval_env = create_eval_env(seed=config['seed'], n_envs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "29c77ed8695a4287956ec27672fe7b7a",
            "5b907d9cb030400c8cd9308246d761c1",
            "bf4a59f3065346d6af96d1fc1a2e6385",
            "c341e0ddb94f4c0b9066e8dc99136430",
            "ac046599192d45cfbf5ab150f9fb240d",
            "483c79836eed4cfab4cfc7320151ef66",
            "e8afd67546254f249e488008aa76f0a8",
            "6446e95478cf4113aad04353ba1c2030"
          ]
        },
        "id": "ZhPjWJ7AfjMs",
        "outputId": "eef1f821-6644-4a6c-9131-889a6f18b697"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First disturbance used in training: {'Ti': [350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 358.8790902996886, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350, 350]}\n",
            "Using cuda device\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 4    |\n",
            "|    fps             | 1766 |\n",
            "|    time_elapsed    | 0    |\n",
            "|    total_timesteps | 590  |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 8    |\n",
            "|    fps             | 1761 |\n",
            "|    time_elapsed    | 0    |\n",
            "|    total_timesteps | 590  |\n",
            "-----------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\mfb22\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval num_timesteps=1000, episode_reward=-0.15 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.147   |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.899   |\n",
            "|    critic_loss     | 0.00818  |\n",
            "|    ent_coef        | 0.0202   |\n",
            "|    ent_coef_loss   | -4.94    |\n",
            "|    learning_rate   | 0.0102   |\n",
            "|    n_updates       | 480      |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 89       |\n",
            "|    time_elapsed    | 13       |\n",
            "|    total_timesteps | 1180     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.838   |\n",
            "|    critic_loss     | 0.00581  |\n",
            "|    ent_coef        | 0.00717  |\n",
            "|    ent_coef_loss   | -2.95    |\n",
            "|    learning_rate   | 0.0102   |\n",
            "|    n_updates       | 640      |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 16   |\n",
            "|    fps             | 89   |\n",
            "|    time_elapsed    | 13   |\n",
            "|    total_timesteps | 1180 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 20   |\n",
            "|    fps             | 89   |\n",
            "|    time_elapsed    | 13   |\n",
            "|    total_timesteps | 1180 |\n",
            "-----------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 24       |\n",
            "|    fps             | 77       |\n",
            "|    time_elapsed    | 22       |\n",
            "|    total_timesteps | 1770     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.524   |\n",
            "|    critic_loss     | 0.00306  |\n",
            "|    ent_coef        | 0.000994 |\n",
            "|    ent_coef_loss   | -1.22    |\n",
            "|    learning_rate   | 0.0102   |\n",
            "|    n_updates       | 1280     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 28   |\n",
            "|    fps             | 77   |\n",
            "|    time_elapsed    | 22   |\n",
            "|    total_timesteps | 1770 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=2000, episode_reward=-0.08 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.0795  |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.457   |\n",
            "|    critic_loss     | 0.00297  |\n",
            "|    ent_coef        | 0.000718 |\n",
            "|    ent_coef_loss   | -1.45    |\n",
            "|    learning_rate   | 0.0102   |\n",
            "|    n_updates       | 1440     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 32       |\n",
            "|    fps             | 69       |\n",
            "|    time_elapsed    | 33       |\n",
            "|    total_timesteps | 2360     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.352   |\n",
            "|    critic_loss     | 0.0015   |\n",
            "|    ent_coef        | 0.000719 |\n",
            "|    ent_coef_loss   | 6.02     |\n",
            "|    learning_rate   | 0.0102   |\n",
            "|    n_updates       | 1760     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 36   |\n",
            "|    fps             | 69   |\n",
            "|    time_elapsed    | 33   |\n",
            "|    total_timesteps | 2360 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 40   |\n",
            "|    fps             | 69   |\n",
            "|    time_elapsed    | 33   |\n",
            "|    total_timesteps | 2360 |\n",
            "-----------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 44       |\n",
            "|    fps             | 56       |\n",
            "|    time_elapsed    | 52       |\n",
            "|    total_timesteps | 2950     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.217   |\n",
            "|    critic_loss     | 0.000646 |\n",
            "|    ent_coef        | 0.000897 |\n",
            "|    ent_coef_loss   | 0.0706   |\n",
            "|    learning_rate   | 0.0102   |\n",
            "|    n_updates       | 2400     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 48   |\n",
            "|    fps             | 56   |\n",
            "|    time_elapsed    | 52   |\n",
            "|    total_timesteps | 2950 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=3000, episode_reward=-0.09 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.094   |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3000     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 52       |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 70       |\n",
            "|    total_timesteps | 3540     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.124   |\n",
            "|    critic_loss     | 0.000244 |\n",
            "|    ent_coef        | 0.000433 |\n",
            "|    ent_coef_loss   | -0.662   |\n",
            "|    learning_rate   | 0.0102   |\n",
            "|    n_updates       | 3040     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 56   |\n",
            "|    fps             | 50   |\n",
            "|    time_elapsed    | 70   |\n",
            "|    total_timesteps | 3540 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 60   |\n",
            "|    fps             | 50   |\n",
            "|    time_elapsed    | 70   |\n",
            "|    total_timesteps | 3540 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=4000, episode_reward=-0.07 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.0666  |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.0889  |\n",
            "|    critic_loss     | 0.000146 |\n",
            "|    ent_coef        | 0.000169 |\n",
            "|    ent_coef_loss   | -1.47    |\n",
            "|    learning_rate   | 0.0102   |\n",
            "|    n_updates       | 3360     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 64       |\n",
            "|    fps             | 51       |\n",
            "|    time_elapsed    | 80       |\n",
            "|    total_timesteps | 4130     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.0753  |\n",
            "|    critic_loss     | 0.000109 |\n",
            "|    ent_coef        | 0.00011  |\n",
            "|    ent_coef_loss   | -0.607   |\n",
            "|    learning_rate   | 0.0102   |\n",
            "|    n_updates       | 3520     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 68   |\n",
            "|    fps             | 51   |\n",
            "|    time_elapsed    | 80   |\n",
            "|    total_timesteps | 4130 |\n",
            "-----------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 72       |\n",
            "|    fps             | 48       |\n",
            "|    time_elapsed    | 97       |\n",
            "|    total_timesteps | 4720     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.032   |\n",
            "|    critic_loss     | 3.39e-05 |\n",
            "|    ent_coef        | 0.000242 |\n",
            "|    ent_coef_loss   | 0.0704   |\n",
            "|    learning_rate   | 0.0102   |\n",
            "|    n_updates       | 4160     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 76   |\n",
            "|    fps             | 48   |\n",
            "|    time_elapsed    | 97   |\n",
            "|    total_timesteps | 4720 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 80   |\n",
            "|    fps             | 48   |\n",
            "|    time_elapsed    | 97   |\n",
            "|    total_timesteps | 4720 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=5000, episode_reward=-0.05 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.0523  |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.0182  |\n",
            "|    critic_loss     | 1.83e-05 |\n",
            "|    ent_coef        | 0.00016  |\n",
            "|    ent_coef_loss   | -0.191   |\n",
            "|    learning_rate   | 0.0102   |\n",
            "|    n_updates       | 4480     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 84       |\n",
            "|    fps             | 46       |\n",
            "|    time_elapsed    | 114      |\n",
            "|    total_timesteps | 5310     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.00824 |\n",
            "|    critic_loss     | 1.02e-05 |\n",
            "|    ent_coef        | 9.37e-05 |\n",
            "|    ent_coef_loss   | -1.12    |\n",
            "|    learning_rate   | 0.0102   |\n",
            "|    n_updates       | 4800     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 88   |\n",
            "|    fps             | 46   |\n",
            "|    time_elapsed    | 114  |\n",
            "|    total_timesteps | 5310 |\n",
            "-----------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 92       |\n",
            "|    fps             | 46       |\n",
            "|    time_elapsed    | 127      |\n",
            "|    total_timesteps | 5900     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0031   |\n",
            "|    critic_loss     | 5.83e-06 |\n",
            "|    ent_coef        | 0.000108 |\n",
            "|    ent_coef_loss   | -0.00137 |\n",
            "|    learning_rate   | 0.0102   |\n",
            "|    n_updates       | 5280     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 96   |\n",
            "|    fps             | 46   |\n",
            "|    time_elapsed    | 127  |\n",
            "|    total_timesteps | 5900 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 100  |\n",
            "|    fps             | 46   |\n",
            "|    time_elapsed    | 127  |\n",
            "|    total_timesteps | 5900 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=6000, episode_reward=-0.02 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.0158  |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 6000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00572  |\n",
            "|    critic_loss     | 5.65e-06 |\n",
            "|    ent_coef        | 0.000189 |\n",
            "|    ent_coef_loss   | 0.651    |\n",
            "|    learning_rate   | 0.0102   |\n",
            "|    n_updates       | 5440     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 104      |\n",
            "|    fps             | 45       |\n",
            "|    time_elapsed    | 142      |\n",
            "|    total_timesteps | 6490     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0108   |\n",
            "|    critic_loss     | 5.44e-06 |\n",
            "|    ent_coef        | 0.000152 |\n",
            "|    ent_coef_loss   | 0.617    |\n",
            "|    learning_rate   | 0.0102   |\n",
            "|    n_updates       | 5920     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 108  |\n",
            "|    fps             | 45   |\n",
            "|    time_elapsed    | 142  |\n",
            "|    total_timesteps | 6490 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=7000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.0294  |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 7000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0138   |\n",
            "|    critic_loss     | 5.52e-06 |\n",
            "|    ent_coef        | 0.000188 |\n",
            "|    ent_coef_loss   | 0.131    |\n",
            "|    learning_rate   | 0.0102   |\n",
            "|    n_updates       | 6400     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 112      |\n",
            "|    fps             | 44       |\n",
            "|    time_elapsed    | 158      |\n",
            "|    total_timesteps | 7080     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0145   |\n",
            "|    critic_loss     | 5.59e-06 |\n",
            "|    ent_coef        | 0.000145 |\n",
            "|    ent_coef_loss   | 0.242    |\n",
            "|    learning_rate   | 0.0102   |\n",
            "|    n_updates       | 6560     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 116  |\n",
            "|    fps             | 44   |\n",
            "|    time_elapsed    | 158  |\n",
            "|    total_timesteps | 7080 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 120  |\n",
            "|    fps             | 44   |\n",
            "|    time_elapsed    | 158  |\n",
            "|    total_timesteps | 7080 |\n",
            "-----------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 124      |\n",
            "|    fps             | 45       |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 7670     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0159   |\n",
            "|    critic_loss     | 6.04e-06 |\n",
            "|    ent_coef        | 0.000197 |\n",
            "|    ent_coef_loss   | 0.0539   |\n",
            "|    learning_rate   | 0.0102   |\n",
            "|    n_updates       | 7040     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 128  |\n",
            "|    fps             | 45   |\n",
            "|    time_elapsed    | 166  |\n",
            "|    total_timesteps | 7670 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=8000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.0101  |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0162   |\n",
            "|    critic_loss     | 6.6e-06  |\n",
            "|    ent_coef        | 0.000176 |\n",
            "|    ent_coef_loss   | 0.069    |\n",
            "|    learning_rate   | 0.0102   |\n",
            "|    n_updates       | 7360     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 132      |\n",
            "|    fps             | 45       |\n",
            "|    time_elapsed    | 179      |\n",
            "|    total_timesteps | 8260     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0166   |\n",
            "|    critic_loss     | 6.01e-06 |\n",
            "|    ent_coef        | 0.000199 |\n",
            "|    ent_coef_loss   | 0.276    |\n",
            "|    learning_rate   | 0.0102   |\n",
            "|    n_updates       | 7680     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 136  |\n",
            "|    fps             | 45   |\n",
            "|    time_elapsed    | 179  |\n",
            "|    total_timesteps | 8260 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 140  |\n",
            "|    fps             | 45   |\n",
            "|    time_elapsed    | 179  |\n",
            "|    total_timesteps | 8260 |\n",
            "-----------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 144      |\n",
            "|    fps             | 46       |\n",
            "|    time_elapsed    | 191      |\n",
            "|    total_timesteps | 8850     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0168   |\n",
            "|    critic_loss     | 6.59e-06 |\n",
            "|    ent_coef        | 0.000257 |\n",
            "|    ent_coef_loss   | 0.405    |\n",
            "|    learning_rate   | 0.0102   |\n",
            "|    n_updates       | 8320     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 148  |\n",
            "|    fps             | 46   |\n",
            "|    time_elapsed    | 191  |\n",
            "|    total_timesteps | 8850 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=9000, episode_reward=-0.02 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.0184  |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 9000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0167   |\n",
            "|    critic_loss     | 6.23e-06 |\n",
            "|    ent_coef        | 0.000238 |\n",
            "|    ent_coef_loss   | -0.27    |\n",
            "|    learning_rate   | 0.0102   |\n",
            "|    n_updates       | 8480     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 152      |\n",
            "|    fps             | 46       |\n",
            "|    time_elapsed    | 201      |\n",
            "|    total_timesteps | 9440     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.017    |\n",
            "|    critic_loss     | 5.9e-06  |\n",
            "|    ent_coef        | 0.000298 |\n",
            "|    ent_coef_loss   | -0.17    |\n",
            "|    learning_rate   | 0.0102   |\n",
            "|    n_updates       | 8800     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 156  |\n",
            "|    fps             | 46   |\n",
            "|    time_elapsed    | 201  |\n",
            "|    total_timesteps | 9440 |\n",
            "-----------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    episodes        | 160  |\n",
            "|    fps             | 46   |\n",
            "|    time_elapsed    | 201  |\n",
            "|    total_timesteps | 9440 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=10000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.0254  |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 10000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0168   |\n",
            "|    critic_loss     | 5.93e-06 |\n",
            "|    ent_coef        | 0.000248 |\n",
            "|    ent_coef_loss   | -0.297   |\n",
            "|    learning_rate   | 0.0102   |\n",
            "|    n_updates       | 9440     |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 164   |\n",
            "|    fps             | 46    |\n",
            "|    time_elapsed    | 214   |\n",
            "|    total_timesteps | 10030 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 168   |\n",
            "|    fps             | 46    |\n",
            "|    time_elapsed    | 214   |\n",
            "|    total_timesteps | 10030 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 172      |\n",
            "|    fps             | 46       |\n",
            "|    time_elapsed    | 226      |\n",
            "|    total_timesteps | 10620    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0163   |\n",
            "|    critic_loss     | 5.7e-06  |\n",
            "|    ent_coef        | 0.000188 |\n",
            "|    ent_coef_loss   | 0.0712   |\n",
            "|    learning_rate   | 0.0102   |\n",
            "|    n_updates       | 10080    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 176   |\n",
            "|    fps             | 46    |\n",
            "|    time_elapsed    | 226   |\n",
            "|    total_timesteps | 10620 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 180   |\n",
            "|    fps             | 46    |\n",
            "|    time_elapsed    | 226   |\n",
            "|    total_timesteps | 10620 |\n",
            "------------------------------\n",
            "Eval num_timesteps=11000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.0133  |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 11000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0156   |\n",
            "|    critic_loss     | 5.4e-06  |\n",
            "|    ent_coef        | 0.000237 |\n",
            "|    ent_coef_loss   | -0.268   |\n",
            "|    learning_rate   | 0.0101   |\n",
            "|    n_updates       | 10400    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 184      |\n",
            "|    fps             | 46       |\n",
            "|    time_elapsed    | 242      |\n",
            "|    total_timesteps | 11210    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.015    |\n",
            "|    critic_loss     | 4.65e-06 |\n",
            "|    ent_coef        | 0.000162 |\n",
            "|    ent_coef_loss   | -0.214   |\n",
            "|    learning_rate   | 0.0101   |\n",
            "|    n_updates       | 10720    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 188   |\n",
            "|    fps             | 46    |\n",
            "|    time_elapsed    | 242   |\n",
            "|    total_timesteps | 11210 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 192      |\n",
            "|    fps             | 46       |\n",
            "|    time_elapsed    | 252      |\n",
            "|    total_timesteps | 11800    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0136   |\n",
            "|    critic_loss     | 3.86e-06 |\n",
            "|    ent_coef        | 9.39e-05 |\n",
            "|    ent_coef_loss   | -0.373   |\n",
            "|    learning_rate   | 0.0101   |\n",
            "|    n_updates       | 11200    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 196   |\n",
            "|    fps             | 46    |\n",
            "|    time_elapsed    | 252   |\n",
            "|    total_timesteps | 11800 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 200   |\n",
            "|    fps             | 46    |\n",
            "|    time_elapsed    | 252   |\n",
            "|    total_timesteps | 11800 |\n",
            "------------------------------\n",
            "Eval num_timesteps=12000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.0138  |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 12000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0132   |\n",
            "|    critic_loss     | 4.01e-06 |\n",
            "|    ent_coef        | 8.21e-05 |\n",
            "|    ent_coef_loss   | 0.0651   |\n",
            "|    learning_rate   | 0.0101   |\n",
            "|    n_updates       | 11360    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 204      |\n",
            "|    fps             | 46       |\n",
            "|    time_elapsed    | 265      |\n",
            "|    total_timesteps | 12390    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0121   |\n",
            "|    critic_loss     | 3.28e-06 |\n",
            "|    ent_coef        | 9.22e-05 |\n",
            "|    ent_coef_loss   | 0.241    |\n",
            "|    learning_rate   | 0.0101   |\n",
            "|    n_updates       | 11840    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 208   |\n",
            "|    fps             | 46    |\n",
            "|    time_elapsed    | 265   |\n",
            "|    total_timesteps | 12390 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 212      |\n",
            "|    fps             | 47       |\n",
            "|    time_elapsed    | 275      |\n",
            "|    total_timesteps | 12980    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0113   |\n",
            "|    critic_loss     | 2.77e-06 |\n",
            "|    ent_coef        | 0.000114 |\n",
            "|    ent_coef_loss   | 0.106    |\n",
            "|    learning_rate   | 0.0101   |\n",
            "|    n_updates       | 12480    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 216   |\n",
            "|    fps             | 47    |\n",
            "|    time_elapsed    | 275   |\n",
            "|    total_timesteps | 12980 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 220   |\n",
            "|    fps             | 47    |\n",
            "|    time_elapsed    | 275   |\n",
            "|    total_timesteps | 12980 |\n",
            "------------------------------\n",
            "Eval num_timesteps=13000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00842 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 13000    |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 224      |\n",
            "|    fps             | 47       |\n",
            "|    time_elapsed    | 285      |\n",
            "|    total_timesteps | 13570    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0107   |\n",
            "|    critic_loss     | 2.54e-06 |\n",
            "|    ent_coef        | 9.55e-05 |\n",
            "|    ent_coef_loss   | -0.129   |\n",
            "|    learning_rate   | 0.0101   |\n",
            "|    n_updates       | 12960    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 228   |\n",
            "|    fps             | 47    |\n",
            "|    time_elapsed    | 285   |\n",
            "|    total_timesteps | 13570 |\n",
            "------------------------------\n",
            "Eval num_timesteps=14000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00865 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 14000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0102   |\n",
            "|    critic_loss     | 2.37e-06 |\n",
            "|    ent_coef        | 0.000111 |\n",
            "|    ent_coef_loss   | -0.029   |\n",
            "|    learning_rate   | 0.0101   |\n",
            "|    n_updates       | 13440    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 232      |\n",
            "|    fps             | 47       |\n",
            "|    time_elapsed    | 295      |\n",
            "|    total_timesteps | 14160    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0102   |\n",
            "|    critic_loss     | 2.5e-06  |\n",
            "|    ent_coef        | 0.000103 |\n",
            "|    ent_coef_loss   | -0.166   |\n",
            "|    learning_rate   | 0.0101   |\n",
            "|    n_updates       | 13600    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 236   |\n",
            "|    fps             | 47    |\n",
            "|    time_elapsed    | 295   |\n",
            "|    total_timesteps | 14160 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 240   |\n",
            "|    fps             | 47    |\n",
            "|    time_elapsed    | 295   |\n",
            "|    total_timesteps | 14160 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 244      |\n",
            "|    fps             | 48       |\n",
            "|    time_elapsed    | 307      |\n",
            "|    total_timesteps | 14750    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0102   |\n",
            "|    critic_loss     | 2.66e-06 |\n",
            "|    ent_coef        | 0.000138 |\n",
            "|    ent_coef_loss   | 0.0609   |\n",
            "|    learning_rate   | 0.0101   |\n",
            "|    n_updates       | 14240    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 248   |\n",
            "|    fps             | 48    |\n",
            "|    time_elapsed    | 307   |\n",
            "|    total_timesteps | 14750 |\n",
            "------------------------------\n",
            "Eval num_timesteps=15000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00567 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 15000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0104   |\n",
            "|    critic_loss     | 2.7e-06  |\n",
            "|    ent_coef        | 0.000115 |\n",
            "|    ent_coef_loss   | -0.146   |\n",
            "|    learning_rate   | 0.0101   |\n",
            "|    n_updates       | 14400    |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 252      |\n",
            "|    fps             | 48       |\n",
            "|    time_elapsed    | 318      |\n",
            "|    total_timesteps | 15340    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0102   |\n",
            "|    critic_loss     | 2.39e-06 |\n",
            "|    ent_coef        | 0.000126 |\n",
            "|    ent_coef_loss   | 0.109    |\n",
            "|    learning_rate   | 0.0101   |\n",
            "|    n_updates       | 14720    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 256   |\n",
            "|    fps             | 48    |\n",
            "|    time_elapsed    | 318   |\n",
            "|    total_timesteps | 15340 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 260   |\n",
            "|    fps             | 48    |\n",
            "|    time_elapsed    | 318   |\n",
            "|    total_timesteps | 15340 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 264      |\n",
            "|    fps             | 47       |\n",
            "|    time_elapsed    | 331      |\n",
            "|    total_timesteps | 15930    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0101   |\n",
            "|    critic_loss     | 2.37e-06 |\n",
            "|    ent_coef        | 0.000151 |\n",
            "|    ent_coef_loss   | -0.0814  |\n",
            "|    learning_rate   | 0.0101   |\n",
            "|    n_updates       | 15360    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 268   |\n",
            "|    fps             | 47    |\n",
            "|    time_elapsed    | 331   |\n",
            "|    total_timesteps | 15930 |\n",
            "------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.0105  |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 16000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 272      |\n",
            "|    fps             | 47       |\n",
            "|    time_elapsed    | 346      |\n",
            "|    total_timesteps | 16520    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0101   |\n",
            "|    critic_loss     | 2.46e-06 |\n",
            "|    ent_coef        | 0.000137 |\n",
            "|    ent_coef_loss   | 0.22     |\n",
            "|    learning_rate   | 0.0101   |\n",
            "|    n_updates       | 16000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 276   |\n",
            "|    fps             | 47    |\n",
            "|    time_elapsed    | 346   |\n",
            "|    total_timesteps | 16520 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 280   |\n",
            "|    fps             | 47    |\n",
            "|    time_elapsed    | 346   |\n",
            "|    total_timesteps | 16520 |\n",
            "------------------------------\n",
            "Eval num_timesteps=17000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00857 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 17000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0101   |\n",
            "|    critic_loss     | 2.24e-06 |\n",
            "|    ent_coef        | 0.000139 |\n",
            "|    ent_coef_loss   | 0.0899   |\n",
            "|    learning_rate   | 0.01     |\n",
            "|    n_updates       | 16480    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 284   |\n",
            "|    fps             | 47    |\n",
            "|    time_elapsed    | 357   |\n",
            "|    total_timesteps | 17110 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 288   |\n",
            "|    fps             | 47    |\n",
            "|    time_elapsed    | 357   |\n",
            "|    total_timesteps | 17110 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 292      |\n",
            "|    fps             | 47       |\n",
            "|    time_elapsed    | 370      |\n",
            "|    total_timesteps | 17700    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00994  |\n",
            "|    critic_loss     | 2.28e-06 |\n",
            "|    ent_coef        | 0.00014  |\n",
            "|    ent_coef_loss   | -0.104   |\n",
            "|    learning_rate   | 0.01     |\n",
            "|    n_updates       | 17120    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 296   |\n",
            "|    fps             | 47    |\n",
            "|    time_elapsed    | 370   |\n",
            "|    total_timesteps | 17700 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 300   |\n",
            "|    fps             | 47    |\n",
            "|    time_elapsed    | 370   |\n",
            "|    total_timesteps | 17700 |\n",
            "------------------------------\n",
            "Eval num_timesteps=18000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00523 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 18000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00984  |\n",
            "|    critic_loss     | 2.26e-06 |\n",
            "|    ent_coef        | 0.000138 |\n",
            "|    ent_coef_loss   | -0.0638  |\n",
            "|    learning_rate   | 0.01     |\n",
            "|    n_updates       | 17440    |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 304      |\n",
            "|    fps             | 47       |\n",
            "|    time_elapsed    | 385      |\n",
            "|    total_timesteps | 18290    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00982  |\n",
            "|    critic_loss     | 2.36e-06 |\n",
            "|    ent_coef        | 0.000138 |\n",
            "|    ent_coef_loss   | 0.0951   |\n",
            "|    learning_rate   | 0.01     |\n",
            "|    n_updates       | 17760    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 308   |\n",
            "|    fps             | 47    |\n",
            "|    time_elapsed    | 385   |\n",
            "|    total_timesteps | 18290 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 312      |\n",
            "|    fps             | 47       |\n",
            "|    time_elapsed    | 395      |\n",
            "|    total_timesteps | 18880    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00982  |\n",
            "|    critic_loss     | 2.43e-06 |\n",
            "|    ent_coef        | 0.00018  |\n",
            "|    ent_coef_loss   | -0.1     |\n",
            "|    learning_rate   | 0.01     |\n",
            "|    n_updates       | 18240    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 316   |\n",
            "|    fps             | 47    |\n",
            "|    time_elapsed    | 395   |\n",
            "|    total_timesteps | 18880 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 320   |\n",
            "|    fps             | 47    |\n",
            "|    time_elapsed    | 395   |\n",
            "|    total_timesteps | 18880 |\n",
            "------------------------------\n",
            "Eval num_timesteps=19000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00773 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 19000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0098   |\n",
            "|    critic_loss     | 2.45e-06 |\n",
            "|    ent_coef        | 0.000161 |\n",
            "|    ent_coef_loss   | -0.0601  |\n",
            "|    learning_rate   | 0.01     |\n",
            "|    n_updates       | 18400    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 324      |\n",
            "|    fps             | 47       |\n",
            "|    time_elapsed    | 409      |\n",
            "|    total_timesteps | 19470    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00958  |\n",
            "|    critic_loss     | 2.17e-06 |\n",
            "|    ent_coef        | 0.000152 |\n",
            "|    ent_coef_loss   | 0.026    |\n",
            "|    learning_rate   | 0.00999  |\n",
            "|    n_updates       | 18880    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 328   |\n",
            "|    fps             | 47    |\n",
            "|    time_elapsed    | 409   |\n",
            "|    total_timesteps | 19470 |\n",
            "------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00424 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 20000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00932  |\n",
            "|    critic_loss     | 2.14e-06 |\n",
            "|    ent_coef        | 0.000126 |\n",
            "|    ent_coef_loss   | 0.0887   |\n",
            "|    learning_rate   | 0.00998  |\n",
            "|    n_updates       | 19360    |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 332      |\n",
            "|    fps             | 47       |\n",
            "|    time_elapsed    | 422      |\n",
            "|    total_timesteps | 20060    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00927  |\n",
            "|    critic_loss     | 2.11e-06 |\n",
            "|    ent_coef        | 0.000136 |\n",
            "|    ent_coef_loss   | 0.301    |\n",
            "|    learning_rate   | 0.00997  |\n",
            "|    n_updates       | 19520    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 336   |\n",
            "|    fps             | 47    |\n",
            "|    time_elapsed    | 422   |\n",
            "|    total_timesteps | 20060 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 340   |\n",
            "|    fps             | 47    |\n",
            "|    time_elapsed    | 422   |\n",
            "|    total_timesteps | 20060 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 344      |\n",
            "|    fps             | 47       |\n",
            "|    time_elapsed    | 435      |\n",
            "|    total_timesteps | 20650    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00943  |\n",
            "|    critic_loss     | 2.47e-06 |\n",
            "|    ent_coef        | 0.00019  |\n",
            "|    ent_coef_loss   | -0.605   |\n",
            "|    learning_rate   | 0.00996  |\n",
            "|    n_updates       | 20160    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 348   |\n",
            "|    fps             | 47    |\n",
            "|    time_elapsed    | 435   |\n",
            "|    total_timesteps | 20650 |\n",
            "------------------------------\n",
            "Eval num_timesteps=21000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00619 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 21000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00916  |\n",
            "|    critic_loss     | 2.24e-06 |\n",
            "|    ent_coef        | 0.00014  |\n",
            "|    ent_coef_loss   | -0.119   |\n",
            "|    learning_rate   | 0.00995  |\n",
            "|    n_updates       | 20480    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 352      |\n",
            "|    fps             | 47       |\n",
            "|    time_elapsed    | 446      |\n",
            "|    total_timesteps | 21240    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0089   |\n",
            "|    critic_loss     | 2e-06    |\n",
            "|    ent_coef        | 0.000131 |\n",
            "|    ent_coef_loss   | -0.0508  |\n",
            "|    learning_rate   | 0.00995  |\n",
            "|    n_updates       | 20640    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 356   |\n",
            "|    fps             | 47    |\n",
            "|    time_elapsed    | 446   |\n",
            "|    total_timesteps | 21240 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 360   |\n",
            "|    fps             | 47    |\n",
            "|    time_elapsed    | 446   |\n",
            "|    total_timesteps | 21240 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 364      |\n",
            "|    fps             | 47       |\n",
            "|    time_elapsed    | 457      |\n",
            "|    total_timesteps | 21830    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00833  |\n",
            "|    critic_loss     | 1.75e-06 |\n",
            "|    ent_coef        | 0.000126 |\n",
            "|    ent_coef_loss   | 0.0245   |\n",
            "|    learning_rate   | 0.00993  |\n",
            "|    n_updates       | 21280    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 368   |\n",
            "|    fps             | 47    |\n",
            "|    time_elapsed    | 457   |\n",
            "|    total_timesteps | 21830 |\n",
            "------------------------------\n",
            "Eval num_timesteps=22000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.0089  |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 22000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00831  |\n",
            "|    critic_loss     | 1.74e-06 |\n",
            "|    ent_coef        | 0.000116 |\n",
            "|    ent_coef_loss   | -0.206   |\n",
            "|    learning_rate   | 0.00992  |\n",
            "|    n_updates       | 21440    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 372      |\n",
            "|    fps             | 47       |\n",
            "|    time_elapsed    | 470      |\n",
            "|    total_timesteps | 22420    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00794  |\n",
            "|    critic_loss     | 1.86e-06 |\n",
            "|    ent_coef        | 0.000111 |\n",
            "|    ent_coef_loss   | -0.0819  |\n",
            "|    learning_rate   | 0.00991  |\n",
            "|    n_updates       | 21920    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 376   |\n",
            "|    fps             | 47    |\n",
            "|    time_elapsed    | 470   |\n",
            "|    total_timesteps | 22420 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 380   |\n",
            "|    fps             | 47    |\n",
            "|    time_elapsed    | 470   |\n",
            "|    total_timesteps | 22420 |\n",
            "------------------------------\n",
            "Eval num_timesteps=23000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00283 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 23000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00781  |\n",
            "|    critic_loss     | 1.66e-06 |\n",
            "|    ent_coef        | 0.000116 |\n",
            "|    ent_coef_loss   | 0.232    |\n",
            "|    learning_rate   | 0.0099   |\n",
            "|    n_updates       | 22400    |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 384   |\n",
            "|    fps             | 48    |\n",
            "|    time_elapsed    | 479   |\n",
            "|    total_timesteps | 23010 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 388   |\n",
            "|    fps             | 48    |\n",
            "|    time_elapsed    | 479   |\n",
            "|    total_timesteps | 23010 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 392      |\n",
            "|    fps             | 48       |\n",
            "|    time_elapsed    | 489      |\n",
            "|    total_timesteps | 23600    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00741  |\n",
            "|    critic_loss     | 1.4e-06  |\n",
            "|    ent_coef        | 9.86e-05 |\n",
            "|    ent_coef_loss   | 0.226    |\n",
            "|    learning_rate   | 0.00988  |\n",
            "|    n_updates       | 23040    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 396   |\n",
            "|    fps             | 48    |\n",
            "|    time_elapsed    | 489   |\n",
            "|    total_timesteps | 23600 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 400   |\n",
            "|    fps             | 48    |\n",
            "|    time_elapsed    | 489   |\n",
            "|    total_timesteps | 23600 |\n",
            "------------------------------\n",
            "Eval num_timesteps=24000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00236 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 24000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00738  |\n",
            "|    critic_loss     | 1.41e-06 |\n",
            "|    ent_coef        | 0.000112 |\n",
            "|    ent_coef_loss   | 0.0843   |\n",
            "|    learning_rate   | 0.00987  |\n",
            "|    n_updates       | 23360    |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 404      |\n",
            "|    fps             | 48       |\n",
            "|    time_elapsed    | 503      |\n",
            "|    total_timesteps | 24190    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00739  |\n",
            "|    critic_loss     | 1.47e-06 |\n",
            "|    ent_coef        | 0.000103 |\n",
            "|    ent_coef_loss   | -0.116   |\n",
            "|    learning_rate   | 0.00986  |\n",
            "|    n_updates       | 23680    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 408   |\n",
            "|    fps             | 48    |\n",
            "|    time_elapsed    | 503   |\n",
            "|    total_timesteps | 24190 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 412      |\n",
            "|    fps             | 48       |\n",
            "|    time_elapsed    | 512      |\n",
            "|    total_timesteps | 24780    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0071   |\n",
            "|    critic_loss     | 1.36e-06 |\n",
            "|    ent_coef        | 8.25e-05 |\n",
            "|    ent_coef_loss   | 0.286    |\n",
            "|    learning_rate   | 0.00985  |\n",
            "|    n_updates       | 24160    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 416   |\n",
            "|    fps             | 48    |\n",
            "|    time_elapsed    | 512   |\n",
            "|    total_timesteps | 24780 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 420   |\n",
            "|    fps             | 48    |\n",
            "|    time_elapsed    | 512   |\n",
            "|    total_timesteps | 24780 |\n",
            "------------------------------\n",
            "Eval num_timesteps=25000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00302 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 25000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00709  |\n",
            "|    critic_loss     | 1.39e-06 |\n",
            "|    ent_coef        | 9.63e-05 |\n",
            "|    ent_coef_loss   | 0.018    |\n",
            "|    learning_rate   | 0.00984  |\n",
            "|    n_updates       | 24480    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 424      |\n",
            "|    fps             | 48       |\n",
            "|    time_elapsed    | 525      |\n",
            "|    total_timesteps | 25370    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00695  |\n",
            "|    critic_loss     | 1.25e-06 |\n",
            "|    ent_coef        | 8.53e-05 |\n",
            "|    ent_coef_loss   | -0.224   |\n",
            "|    learning_rate   | 0.00983  |\n",
            "|    n_updates       | 24800    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 428   |\n",
            "|    fps             | 48    |\n",
            "|    time_elapsed    | 525   |\n",
            "|    total_timesteps | 25370 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 432      |\n",
            "|    fps             | 47       |\n",
            "|    time_elapsed    | 541      |\n",
            "|    total_timesteps | 25960    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00727  |\n",
            "|    critic_loss     | 1.5e-06  |\n",
            "|    ent_coef        | 8.22e-05 |\n",
            "|    ent_coef_loss   | 0.352    |\n",
            "|    learning_rate   | 0.00981  |\n",
            "|    n_updates       | 25440    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 436   |\n",
            "|    fps             | 47    |\n",
            "|    time_elapsed    | 541   |\n",
            "|    total_timesteps | 25960 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 440   |\n",
            "|    fps             | 47    |\n",
            "|    time_elapsed    | 541   |\n",
            "|    total_timesteps | 25960 |\n",
            "------------------------------\n",
            "Eval num_timesteps=26000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.0029  |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 26000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 444      |\n",
            "|    fps             | 48       |\n",
            "|    time_elapsed    | 550      |\n",
            "|    total_timesteps | 26550    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00695  |\n",
            "|    critic_loss     | 1.43e-06 |\n",
            "|    ent_coef        | 7.56e-05 |\n",
            "|    ent_coef_loss   | 0.0446   |\n",
            "|    learning_rate   | 0.00979  |\n",
            "|    n_updates       | 25920    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 448   |\n",
            "|    fps             | 48    |\n",
            "|    time_elapsed    | 550   |\n",
            "|    total_timesteps | 26550 |\n",
            "------------------------------\n",
            "Eval num_timesteps=27000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00518 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 27000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0068   |\n",
            "|    critic_loss     | 1.24e-06 |\n",
            "|    ent_coef        | 9.65e-05 |\n",
            "|    ent_coef_loss   | 0.18     |\n",
            "|    learning_rate   | 0.00978  |\n",
            "|    n_updates       | 26400    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 452      |\n",
            "|    fps             | 48       |\n",
            "|    time_elapsed    | 562      |\n",
            "|    total_timesteps | 27140    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00679  |\n",
            "|    critic_loss     | 1.24e-06 |\n",
            "|    ent_coef        | 0.000103 |\n",
            "|    ent_coef_loss   | 0.11     |\n",
            "|    learning_rate   | 0.00977  |\n",
            "|    n_updates       | 26560    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 456   |\n",
            "|    fps             | 48    |\n",
            "|    time_elapsed    | 562   |\n",
            "|    total_timesteps | 27140 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 460   |\n",
            "|    fps             | 48    |\n",
            "|    time_elapsed    | 562   |\n",
            "|    total_timesteps | 27140 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 464      |\n",
            "|    fps             | 48       |\n",
            "|    time_elapsed    | 572      |\n",
            "|    total_timesteps | 27730    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00664  |\n",
            "|    critic_loss     | 1.16e-06 |\n",
            "|    ent_coef        | 0.000111 |\n",
            "|    ent_coef_loss   | 0.049    |\n",
            "|    learning_rate   | 0.00975  |\n",
            "|    n_updates       | 27200    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 468   |\n",
            "|    fps             | 48    |\n",
            "|    time_elapsed    | 572   |\n",
            "|    total_timesteps | 27730 |\n",
            "------------------------------\n",
            "Eval num_timesteps=28000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.0063  |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 28000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00663  |\n",
            "|    critic_loss     | 1.34e-06 |\n",
            "|    ent_coef        | 8.39e-05 |\n",
            "|    ent_coef_loss   | -0.324   |\n",
            "|    learning_rate   | 0.00974  |\n",
            "|    n_updates       | 27360    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 472      |\n",
            "|    fps             | 48       |\n",
            "|    time_elapsed    | 580      |\n",
            "|    total_timesteps | 28320    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00663  |\n",
            "|    critic_loss     | 1.33e-06 |\n",
            "|    ent_coef        | 0.000113 |\n",
            "|    ent_coef_loss   | -0.0529  |\n",
            "|    learning_rate   | 0.00973  |\n",
            "|    n_updates       | 27680    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 476   |\n",
            "|    fps             | 48    |\n",
            "|    time_elapsed    | 580   |\n",
            "|    total_timesteps | 28320 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 480   |\n",
            "|    fps             | 48    |\n",
            "|    time_elapsed    | 580   |\n",
            "|    total_timesteps | 28320 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 484      |\n",
            "|    fps             | 48       |\n",
            "|    time_elapsed    | 590      |\n",
            "|    total_timesteps | 28910    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00652  |\n",
            "|    critic_loss     | 1.19e-06 |\n",
            "|    ent_coef        | 6.39e-05 |\n",
            "|    ent_coef_loss   | -0.285   |\n",
            "|    learning_rate   | 0.00971  |\n",
            "|    n_updates       | 28320    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 488   |\n",
            "|    fps             | 48    |\n",
            "|    time_elapsed    | 590   |\n",
            "|    total_timesteps | 28910 |\n",
            "------------------------------\n",
            "Eval num_timesteps=29000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00244 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 29000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00643  |\n",
            "|    critic_loss     | 1.28e-06 |\n",
            "|    ent_coef        | 6.78e-05 |\n",
            "|    ent_coef_loss   | -0.0653  |\n",
            "|    learning_rate   | 0.00971  |\n",
            "|    n_updates       | 28480    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 492      |\n",
            "|    fps             | 49       |\n",
            "|    time_elapsed    | 600      |\n",
            "|    total_timesteps | 29500    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00639  |\n",
            "|    critic_loss     | 1.13e-06 |\n",
            "|    ent_coef        | 8.48e-05 |\n",
            "|    ent_coef_loss   | -0.0294  |\n",
            "|    learning_rate   | 0.00969  |\n",
            "|    n_updates       | 28960    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 496   |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 600   |\n",
            "|    total_timesteps | 29500 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 500   |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 600   |\n",
            "|    total_timesteps | 29500 |\n",
            "------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.000709 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 30000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 0.00623   |\n",
            "|    critic_loss     | 1.18e-06  |\n",
            "|    ent_coef        | 6.66e-05  |\n",
            "|    ent_coef_loss   | 0.326     |\n",
            "|    learning_rate   | 0.00967   |\n",
            "|    n_updates       | 29440     |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 504      |\n",
            "|    fps             | 49       |\n",
            "|    time_elapsed    | 611      |\n",
            "|    total_timesteps | 30090    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00622  |\n",
            "|    critic_loss     | 1.23e-06 |\n",
            "|    ent_coef        | 7.51e-05 |\n",
            "|    ent_coef_loss   | -0.0396  |\n",
            "|    learning_rate   | 0.00967  |\n",
            "|    n_updates       | 29600    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 508   |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 611   |\n",
            "|    total_timesteps | 30090 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 512      |\n",
            "|    fps             | 49       |\n",
            "|    time_elapsed    | 619      |\n",
            "|    total_timesteps | 30680    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00632  |\n",
            "|    critic_loss     | 1e-06    |\n",
            "|    ent_coef        | 8.23e-05 |\n",
            "|    ent_coef_loss   | 0.0104   |\n",
            "|    learning_rate   | 0.00965  |\n",
            "|    n_updates       | 30080    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 516   |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 619   |\n",
            "|    total_timesteps | 30680 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 520   |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 619   |\n",
            "|    total_timesteps | 30680 |\n",
            "------------------------------\n",
            "Eval num_timesteps=31000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00386 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 31000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00624  |\n",
            "|    critic_loss     | 1.23e-06 |\n",
            "|    ent_coef        | 7.96e-05 |\n",
            "|    ent_coef_loss   | -0.00743 |\n",
            "|    learning_rate   | 0.00964  |\n",
            "|    n_updates       | 30400    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 524      |\n",
            "|    fps             | 49       |\n",
            "|    time_elapsed    | 630      |\n",
            "|    total_timesteps | 31270    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00621  |\n",
            "|    critic_loss     | 1.15e-06 |\n",
            "|    ent_coef        | 9.47e-05 |\n",
            "|    ent_coef_loss   | 0.206    |\n",
            "|    learning_rate   | 0.00962  |\n",
            "|    n_updates       | 30720    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 528   |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 630   |\n",
            "|    total_timesteps | 31270 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 532      |\n",
            "|    fps             | 49       |\n",
            "|    time_elapsed    | 637      |\n",
            "|    total_timesteps | 31860    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00624  |\n",
            "|    critic_loss     | 1.1e-06  |\n",
            "|    ent_coef        | 9.61e-05 |\n",
            "|    ent_coef_loss   | -0.183   |\n",
            "|    learning_rate   | 0.0096   |\n",
            "|    n_updates       | 31360    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 536   |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 637   |\n",
            "|    total_timesteps | 31860 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 540   |\n",
            "|    fps             | 49    |\n",
            "|    time_elapsed    | 637   |\n",
            "|    total_timesteps | 31860 |\n",
            "------------------------------\n",
            "Eval num_timesteps=32000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00432 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 32000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 544      |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 647      |\n",
            "|    total_timesteps | 32450    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00636  |\n",
            "|    critic_loss     | 1.18e-06 |\n",
            "|    ent_coef        | 0.000122 |\n",
            "|    ent_coef_loss   | -0.0968  |\n",
            "|    learning_rate   | 0.00958  |\n",
            "|    n_updates       | 31840    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 548   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 647   |\n",
            "|    total_timesteps | 32450 |\n",
            "------------------------------\n",
            "Eval num_timesteps=33000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00337 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 33000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00627  |\n",
            "|    critic_loss     | 1.11e-06 |\n",
            "|    ent_coef        | 9.07e-05 |\n",
            "|    ent_coef_loss   | -0.0231  |\n",
            "|    learning_rate   | 0.00956  |\n",
            "|    n_updates       | 32480    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 552   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 658   |\n",
            "|    total_timesteps | 33040 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 556   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 658   |\n",
            "|    total_timesteps | 33040 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 560   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 658   |\n",
            "|    total_timesteps | 33040 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 564      |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 668      |\n",
            "|    total_timesteps | 33630    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00632  |\n",
            "|    critic_loss     | 1.03e-06 |\n",
            "|    ent_coef        | 9.62e-05 |\n",
            "|    ent_coef_loss   | -0.0298  |\n",
            "|    learning_rate   | 0.00953  |\n",
            "|    n_updates       | 33120    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 568   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 668   |\n",
            "|    total_timesteps | 33630 |\n",
            "------------------------------\n",
            "Eval num_timesteps=34000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00249 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 34000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00619  |\n",
            "|    critic_loss     | 1.06e-06 |\n",
            "|    ent_coef        | 8.78e-05 |\n",
            "|    ent_coef_loss   | -0.0156  |\n",
            "|    learning_rate   | 0.00952  |\n",
            "|    n_updates       | 33440    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 572      |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 676      |\n",
            "|    total_timesteps | 34220    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00619  |\n",
            "|    critic_loss     | 1.23e-06 |\n",
            "|    ent_coef        | 9.59e-05 |\n",
            "|    ent_coef_loss   | -0.0408  |\n",
            "|    learning_rate   | 0.00951  |\n",
            "|    n_updates       | 33600    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 576   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 676   |\n",
            "|    total_timesteps | 34220 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 580   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 676   |\n",
            "|    total_timesteps | 34220 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 584      |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 686      |\n",
            "|    total_timesteps | 34810    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00595  |\n",
            "|    critic_loss     | 9.85e-07 |\n",
            "|    ent_coef        | 8.5e-05  |\n",
            "|    ent_coef_loss   | 0.109    |\n",
            "|    learning_rate   | 0.00949  |\n",
            "|    n_updates       | 34240    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 588   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 686   |\n",
            "|    total_timesteps | 34810 |\n",
            "------------------------------\n",
            "Eval num_timesteps=35000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00268 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 35000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00601  |\n",
            "|    critic_loss     | 1.08e-06 |\n",
            "|    ent_coef        | 8.55e-05 |\n",
            "|    ent_coef_loss   | -0.142   |\n",
            "|    learning_rate   | 0.00948  |\n",
            "|    n_updates       | 34400    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 592      |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 698      |\n",
            "|    total_timesteps | 35400    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0059   |\n",
            "|    critic_loss     | 1.05e-06 |\n",
            "|    ent_coef        | 9.55e-05 |\n",
            "|    ent_coef_loss   | -0.0392  |\n",
            "|    learning_rate   | 0.00946  |\n",
            "|    n_updates       | 34880    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 596   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 698   |\n",
            "|    total_timesteps | 35400 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 600   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 698   |\n",
            "|    total_timesteps | 35400 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 604      |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 706      |\n",
            "|    total_timesteps | 35990    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00591  |\n",
            "|    critic_loss     | 9.65e-07 |\n",
            "|    ent_coef        | 8.99e-05 |\n",
            "|    ent_coef_loss   | 0.0963   |\n",
            "|    learning_rate   | 0.00944  |\n",
            "|    n_updates       | 35360    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 608   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 706   |\n",
            "|    total_timesteps | 35990 |\n",
            "------------------------------\n",
            "Eval num_timesteps=36000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00152 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 36000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 612      |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 717      |\n",
            "|    total_timesteps | 36580    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00588  |\n",
            "|    critic_loss     | 1.13e-06 |\n",
            "|    ent_coef        | 8.92e-05 |\n",
            "|    ent_coef_loss   | 0.327    |\n",
            "|    learning_rate   | 0.00941  |\n",
            "|    n_updates       | 36000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 616   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 717   |\n",
            "|    total_timesteps | 36580 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 620   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 717   |\n",
            "|    total_timesteps | 36580 |\n",
            "------------------------------\n",
            "Eval num_timesteps=37000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00738 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 37000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00585  |\n",
            "|    critic_loss     | 1.05e-06 |\n",
            "|    ent_coef        | 8.79e-05 |\n",
            "|    ent_coef_loss   | -0.357   |\n",
            "|    learning_rate   | 0.00939  |\n",
            "|    n_updates       | 36480    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 624      |\n",
            "|    fps             | 51       |\n",
            "|    time_elapsed    | 728      |\n",
            "|    total_timesteps | 37170    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00579  |\n",
            "|    critic_loss     | 1.06e-06 |\n",
            "|    ent_coef        | 7.81e-05 |\n",
            "|    ent_coef_loss   | -0.0217  |\n",
            "|    learning_rate   | 0.00938  |\n",
            "|    n_updates       | 36640    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 628   |\n",
            "|    fps             | 51    |\n",
            "|    time_elapsed    | 728   |\n",
            "|    total_timesteps | 37170 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 632      |\n",
            "|    fps             | 51       |\n",
            "|    time_elapsed    | 735      |\n",
            "|    total_timesteps | 37760    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00585  |\n",
            "|    critic_loss     | 1.01e-06 |\n",
            "|    ent_coef        | 9.38e-05 |\n",
            "|    ent_coef_loss   | -0.353   |\n",
            "|    learning_rate   | 0.00936  |\n",
            "|    n_updates       | 37120    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 636   |\n",
            "|    fps             | 51    |\n",
            "|    time_elapsed    | 735   |\n",
            "|    total_timesteps | 37760 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 640   |\n",
            "|    fps             | 51    |\n",
            "|    time_elapsed    | 735   |\n",
            "|    total_timesteps | 37760 |\n",
            "------------------------------\n",
            "Eval num_timesteps=38000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00606 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 38000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00574  |\n",
            "|    critic_loss     | 1.04e-06 |\n",
            "|    ent_coef        | 0.000102 |\n",
            "|    ent_coef_loss   | 0.0856   |\n",
            "|    learning_rate   | 0.00935  |\n",
            "|    n_updates       | 37440    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 644      |\n",
            "|    fps             | 51       |\n",
            "|    time_elapsed    | 746      |\n",
            "|    total_timesteps | 38350    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00569  |\n",
            "|    critic_loss     | 9.55e-07 |\n",
            "|    ent_coef        | 9.71e-05 |\n",
            "|    ent_coef_loss   | -0.233   |\n",
            "|    learning_rate   | 0.00933  |\n",
            "|    n_updates       | 37760    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 648   |\n",
            "|    fps             | 51    |\n",
            "|    time_elapsed    | 746   |\n",
            "|    total_timesteps | 38350 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 652      |\n",
            "|    fps             | 51       |\n",
            "|    time_elapsed    | 756      |\n",
            "|    total_timesteps | 38940    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00563  |\n",
            "|    critic_loss     | 1e-06    |\n",
            "|    ent_coef        | 9.23e-05 |\n",
            "|    ent_coef_loss   | -0.0145  |\n",
            "|    learning_rate   | 0.0093   |\n",
            "|    n_updates       | 38400    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 656   |\n",
            "|    fps             | 51    |\n",
            "|    time_elapsed    | 756   |\n",
            "|    total_timesteps | 38940 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 660   |\n",
            "|    fps             | 51    |\n",
            "|    time_elapsed    | 756   |\n",
            "|    total_timesteps | 38940 |\n",
            "------------------------------\n",
            "Eval num_timesteps=39000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00949 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 39000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 664      |\n",
            "|    fps             | 51       |\n",
            "|    time_elapsed    | 769      |\n",
            "|    total_timesteps | 39530    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0056   |\n",
            "|    critic_loss     | 1.13e-06 |\n",
            "|    ent_coef        | 8.83e-05 |\n",
            "|    ent_coef_loss   | -0.174   |\n",
            "|    learning_rate   | 0.00927  |\n",
            "|    n_updates       | 39040    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 668   |\n",
            "|    fps             | 51    |\n",
            "|    time_elapsed    | 769   |\n",
            "|    total_timesteps | 39530 |\n",
            "------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00344 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 40000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00576  |\n",
            "|    critic_loss     | 1.05e-06 |\n",
            "|    ent_coef        | 9.85e-05 |\n",
            "|    ent_coef_loss   | 0.142    |\n",
            "|    learning_rate   | 0.00926  |\n",
            "|    n_updates       | 39360    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 672      |\n",
            "|    fps             | 51       |\n",
            "|    time_elapsed    | 778      |\n",
            "|    total_timesteps | 40120    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00576  |\n",
            "|    critic_loss     | 8.6e-07  |\n",
            "|    ent_coef        | 0.000105 |\n",
            "|    ent_coef_loss   | -0.0549  |\n",
            "|    learning_rate   | 0.00925  |\n",
            "|    n_updates       | 39520    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 676   |\n",
            "|    fps             | 51    |\n",
            "|    time_elapsed    | 778   |\n",
            "|    total_timesteps | 40120 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 680   |\n",
            "|    fps             | 51    |\n",
            "|    time_elapsed    | 778   |\n",
            "|    total_timesteps | 40120 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 684      |\n",
            "|    fps             | 51       |\n",
            "|    time_elapsed    | 791      |\n",
            "|    total_timesteps | 40710    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0058   |\n",
            "|    critic_loss     | 8.81e-07 |\n",
            "|    ent_coef        | 0.000108 |\n",
            "|    ent_coef_loss   | 0.032    |\n",
            "|    learning_rate   | 0.00922  |\n",
            "|    n_updates       | 40160    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 688   |\n",
            "|    fps             | 51    |\n",
            "|    time_elapsed    | 791   |\n",
            "|    total_timesteps | 40710 |\n",
            "------------------------------\n",
            "Eval num_timesteps=41000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00217 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 41000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00581  |\n",
            "|    critic_loss     | 9.85e-07 |\n",
            "|    ent_coef        | 0.000103 |\n",
            "|    ent_coef_loss   | -0.0101  |\n",
            "|    learning_rate   | 0.00921  |\n",
            "|    n_updates       | 40480    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 692      |\n",
            "|    fps             | 51       |\n",
            "|    time_elapsed    | 804      |\n",
            "|    total_timesteps | 41300    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00575  |\n",
            "|    critic_loss     | 9.62e-07 |\n",
            "|    ent_coef        | 8.92e-05 |\n",
            "|    ent_coef_loss   | 0.365    |\n",
            "|    learning_rate   | 0.00919  |\n",
            "|    n_updates       | 40800    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 696   |\n",
            "|    fps             | 51    |\n",
            "|    time_elapsed    | 804   |\n",
            "|    total_timesteps | 41300 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 700   |\n",
            "|    fps             | 51    |\n",
            "|    time_elapsed    | 804   |\n",
            "|    total_timesteps | 41300 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 704      |\n",
            "|    fps             | 51       |\n",
            "|    time_elapsed    | 813      |\n",
            "|    total_timesteps | 41890    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00587  |\n",
            "|    critic_loss     | 1.14e-06 |\n",
            "|    ent_coef        | 0.000103 |\n",
            "|    ent_coef_loss   | -0.202   |\n",
            "|    learning_rate   | 0.00917  |\n",
            "|    n_updates       | 41280    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 708   |\n",
            "|    fps             | 51    |\n",
            "|    time_elapsed    | 813   |\n",
            "|    total_timesteps | 41890 |\n",
            "------------------------------\n",
            "Eval num_timesteps=42000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00191 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 42000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00586  |\n",
            "|    critic_loss     | 1e-06    |\n",
            "|    ent_coef        | 8.59e-05 |\n",
            "|    ent_coef_loss   | 0.0281   |\n",
            "|    learning_rate   | 0.00916  |\n",
            "|    n_updates       | 41440    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 712      |\n",
            "|    fps             | 51       |\n",
            "|    time_elapsed    | 827      |\n",
            "|    total_timesteps | 42480    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00592  |\n",
            "|    critic_loss     | 1e-06    |\n",
            "|    ent_coef        | 9.03e-05 |\n",
            "|    ent_coef_loss   | -0.137   |\n",
            "|    learning_rate   | 0.00914  |\n",
            "|    n_updates       | 41920    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 716   |\n",
            "|    fps             | 51    |\n",
            "|    time_elapsed    | 827   |\n",
            "|    total_timesteps | 42480 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 720   |\n",
            "|    fps             | 51    |\n",
            "|    time_elapsed    | 827   |\n",
            "|    total_timesteps | 42480 |\n",
            "------------------------------\n",
            "Eval num_timesteps=43000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.000441 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 43000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 0.0059    |\n",
            "|    critic_loss     | 9.69e-07  |\n",
            "|    ent_coef        | 0.000112  |\n",
            "|    ent_coef_loss   | 0.0631    |\n",
            "|    learning_rate   | 0.00911   |\n",
            "|    n_updates       | 42400     |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 724      |\n",
            "|    fps             | 51       |\n",
            "|    time_elapsed    | 842      |\n",
            "|    total_timesteps | 43070    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00594  |\n",
            "|    critic_loss     | 1.01e-06 |\n",
            "|    ent_coef        | 9.66e-05 |\n",
            "|    ent_coef_loss   | -0.0851  |\n",
            "|    learning_rate   | 0.00911  |\n",
            "|    n_updates       | 42560    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 728   |\n",
            "|    fps             | 51    |\n",
            "|    time_elapsed    | 842   |\n",
            "|    total_timesteps | 43070 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 732      |\n",
            "|    fps             | 51       |\n",
            "|    time_elapsed    | 851      |\n",
            "|    total_timesteps | 43660    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00588  |\n",
            "|    critic_loss     | 9.11e-07 |\n",
            "|    ent_coef        | 9.65e-05 |\n",
            "|    ent_coef_loss   | -0.103   |\n",
            "|    learning_rate   | 0.00908  |\n",
            "|    n_updates       | 43040    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 736   |\n",
            "|    fps             | 51    |\n",
            "|    time_elapsed    | 851   |\n",
            "|    total_timesteps | 43660 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 740   |\n",
            "|    fps             | 51    |\n",
            "|    time_elapsed    | 851   |\n",
            "|    total_timesteps | 43660 |\n",
            "------------------------------\n",
            "Eval num_timesteps=44000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 59        |\n",
            "|    mean_reward     | -0.000674 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 44000     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 0.00592   |\n",
            "|    critic_loss     | 1.01e-06  |\n",
            "|    ent_coef        | 0.000105  |\n",
            "|    ent_coef_loss   | -0.0137   |\n",
            "|    learning_rate   | 0.00906   |\n",
            "|    n_updates       | 43360     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 744      |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 868      |\n",
            "|    total_timesteps | 44250    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00584  |\n",
            "|    critic_loss     | 9.68e-07 |\n",
            "|    ent_coef        | 9.98e-05 |\n",
            "|    ent_coef_loss   | 0.0103   |\n",
            "|    learning_rate   | 0.00905  |\n",
            "|    n_updates       | 43680    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 748   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 868   |\n",
            "|    total_timesteps | 44250 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 752      |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 881      |\n",
            "|    total_timesteps | 44840    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00572  |\n",
            "|    critic_loss     | 9.2e-07  |\n",
            "|    ent_coef        | 9.09e-05 |\n",
            "|    ent_coef_loss   | 0.01     |\n",
            "|    learning_rate   | 0.00902  |\n",
            "|    n_updates       | 44320    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 756   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 881   |\n",
            "|    total_timesteps | 44840 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 760   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 881   |\n",
            "|    total_timesteps | 44840 |\n",
            "------------------------------\n",
            "Eval num_timesteps=45000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00498 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 45000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00574  |\n",
            "|    critic_loss     | 1.02e-06 |\n",
            "|    ent_coef        | 8.88e-05 |\n",
            "|    ent_coef_loss   | 0.141    |\n",
            "|    learning_rate   | 0.00901  |\n",
            "|    n_updates       | 44480    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 764      |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 892      |\n",
            "|    total_timesteps | 45430    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00575  |\n",
            "|    critic_loss     | 1.09e-06 |\n",
            "|    ent_coef        | 8.76e-05 |\n",
            "|    ent_coef_loss   | 0.321    |\n",
            "|    learning_rate   | 0.00899  |\n",
            "|    n_updates       | 44800    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 768   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 892   |\n",
            "|    total_timesteps | 45430 |\n",
            "------------------------------\n",
            "Eval num_timesteps=46000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.0016  |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 46000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00574  |\n",
            "|    critic_loss     | 1.13e-06 |\n",
            "|    ent_coef        | 9.22e-05 |\n",
            "|    ent_coef_loss   | 0.154    |\n",
            "|    learning_rate   | 0.00896  |\n",
            "|    n_updates       | 45440    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 772   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 906   |\n",
            "|    total_timesteps | 46020 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 776   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 906   |\n",
            "|    total_timesteps | 46020 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 780   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 906   |\n",
            "|    total_timesteps | 46020 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 784      |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 919      |\n",
            "|    total_timesteps | 46610    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00559  |\n",
            "|    critic_loss     | 1.13e-06 |\n",
            "|    ent_coef        | 8.34e-05 |\n",
            "|    ent_coef_loss   | 0.137    |\n",
            "|    learning_rate   | 0.00892  |\n",
            "|    n_updates       | 46080    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 788   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 919   |\n",
            "|    total_timesteps | 46610 |\n",
            "------------------------------\n",
            "Eval num_timesteps=47000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00291 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 47000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0056   |\n",
            "|    critic_loss     | 1.06e-06 |\n",
            "|    ent_coef        | 8.39e-05 |\n",
            "|    ent_coef_loss   | 0.26     |\n",
            "|    learning_rate   | 0.00891  |\n",
            "|    n_updates       | 46400    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 792      |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 930      |\n",
            "|    total_timesteps | 47200    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00561  |\n",
            "|    critic_loss     | 9.12e-07 |\n",
            "|    ent_coef        | 8.94e-05 |\n",
            "|    ent_coef_loss   | -0.243   |\n",
            "|    learning_rate   | 0.0089   |\n",
            "|    n_updates       | 46560    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 796   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 930   |\n",
            "|    total_timesteps | 47200 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 800   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 930   |\n",
            "|    total_timesteps | 47200 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 804      |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 942      |\n",
            "|    total_timesteps | 47790    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00568  |\n",
            "|    critic_loss     | 1e-06    |\n",
            "|    ent_coef        | 9.4e-05  |\n",
            "|    ent_coef_loss   | -0.111   |\n",
            "|    learning_rate   | 0.00886  |\n",
            "|    n_updates       | 47200    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 808   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 942   |\n",
            "|    total_timesteps | 47790 |\n",
            "------------------------------\n",
            "Eval num_timesteps=48000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00254 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 48000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00567  |\n",
            "|    critic_loss     | 9.22e-07 |\n",
            "|    ent_coef        | 0.000102 |\n",
            "|    ent_coef_loss   | 0.219    |\n",
            "|    learning_rate   | 0.00885  |\n",
            "|    n_updates       | 47360    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 812      |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 955      |\n",
            "|    total_timesteps | 48380    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00554  |\n",
            "|    critic_loss     | 9.57e-07 |\n",
            "|    ent_coef        | 8.16e-05 |\n",
            "|    ent_coef_loss   | 0.148    |\n",
            "|    learning_rate   | 0.00883  |\n",
            "|    n_updates       | 47840    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 816   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 955   |\n",
            "|    total_timesteps | 48380 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 820   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 955   |\n",
            "|    total_timesteps | 48380 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 824      |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 968      |\n",
            "|    total_timesteps | 48970    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00546  |\n",
            "|    critic_loss     | 8.98e-07 |\n",
            "|    ent_coef        | 8.06e-05 |\n",
            "|    ent_coef_loss   | -0.034   |\n",
            "|    learning_rate   | 0.00879  |\n",
            "|    n_updates       | 48480    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 828   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 968   |\n",
            "|    total_timesteps | 48970 |\n",
            "------------------------------\n",
            "Eval num_timesteps=49000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.003   |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 49000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 832      |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 980      |\n",
            "|    total_timesteps | 49560    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00537  |\n",
            "|    critic_loss     | 8.36e-07 |\n",
            "|    ent_coef        | 7.93e-05 |\n",
            "|    ent_coef_loss   | 0.253    |\n",
            "|    learning_rate   | 0.00877  |\n",
            "|    n_updates       | 48960    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 836   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 980   |\n",
            "|    total_timesteps | 49560 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 840   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 980   |\n",
            "|    total_timesteps | 49560 |\n",
            "------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00211 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 50000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0053   |\n",
            "|    critic_loss     | 8.73e-07 |\n",
            "|    ent_coef        | 7.87e-05 |\n",
            "|    ent_coef_loss   | 0.124    |\n",
            "|    learning_rate   | 0.00874  |\n",
            "|    n_updates       | 49440    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 844      |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 994      |\n",
            "|    total_timesteps | 50150    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0054   |\n",
            "|    critic_loss     | 9.98e-07 |\n",
            "|    ent_coef        | 7.88e-05 |\n",
            "|    ent_coef_loss   | -0.0743  |\n",
            "|    learning_rate   | 0.00873  |\n",
            "|    n_updates       | 49600    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 848   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 994   |\n",
            "|    total_timesteps | 50150 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 852      |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 1007     |\n",
            "|    total_timesteps | 50740    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00536  |\n",
            "|    critic_loss     | 9.57e-07 |\n",
            "|    ent_coef        | 8.96e-05 |\n",
            "|    ent_coef_loss   | -0.172   |\n",
            "|    learning_rate   | 0.00869  |\n",
            "|    n_updates       | 50240    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 856   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1007  |\n",
            "|    total_timesteps | 50740 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 860   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1007  |\n",
            "|    total_timesteps | 50740 |\n",
            "------------------------------\n",
            "Eval num_timesteps=51000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00421 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 51000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00534  |\n",
            "|    critic_loss     | 9.34e-07 |\n",
            "|    ent_coef        | 7.21e-05 |\n",
            "|    ent_coef_loss   | -0.0859  |\n",
            "|    learning_rate   | 0.00868  |\n",
            "|    n_updates       | 50400    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 864      |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 1017     |\n",
            "|    total_timesteps | 51330    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00534  |\n",
            "|    critic_loss     | 8.5e-07  |\n",
            "|    ent_coef        | 7.69e-05 |\n",
            "|    ent_coef_loss   | 0.000674 |\n",
            "|    learning_rate   | 0.00867  |\n",
            "|    n_updates       | 50720    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 868   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1017  |\n",
            "|    total_timesteps | 51330 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 872      |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 1029     |\n",
            "|    total_timesteps | 51920    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00512  |\n",
            "|    critic_loss     | 8.47e-07 |\n",
            "|    ent_coef        | 7.72e-05 |\n",
            "|    ent_coef_loss   | 0.0833   |\n",
            "|    learning_rate   | 0.00863  |\n",
            "|    n_updates       | 51360    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 876   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1029  |\n",
            "|    total_timesteps | 51920 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 880   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1029  |\n",
            "|    total_timesteps | 51920 |\n",
            "------------------------------\n",
            "Eval num_timesteps=52000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00223 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 52000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 884      |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 1043     |\n",
            "|    total_timesteps | 52510    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00519  |\n",
            "|    critic_loss     | 9.14e-07 |\n",
            "|    ent_coef        | 7.91e-05 |\n",
            "|    ent_coef_loss   | 0.276    |\n",
            "|    learning_rate   | 0.00859  |\n",
            "|    n_updates       | 52000    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 888   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1043  |\n",
            "|    total_timesteps | 52510 |\n",
            "------------------------------\n",
            "Eval num_timesteps=53000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00207 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 53000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0053   |\n",
            "|    critic_loss     | 8.64e-07 |\n",
            "|    ent_coef        | 8.76e-05 |\n",
            "|    ent_coef_loss   | -0.165   |\n",
            "|    learning_rate   | 0.00856  |\n",
            "|    n_updates       | 52480    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 892   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1054  |\n",
            "|    total_timesteps | 53100 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 896   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1054  |\n",
            "|    total_timesteps | 53100 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 900   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1054  |\n",
            "|    total_timesteps | 53100 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 904      |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 1066     |\n",
            "|    total_timesteps | 53690    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00544  |\n",
            "|    critic_loss     | 8.95e-07 |\n",
            "|    ent_coef        | 8.86e-05 |\n",
            "|    ent_coef_loss   | 0.477    |\n",
            "|    learning_rate   | 0.00853  |\n",
            "|    n_updates       | 53120    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 908   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1066  |\n",
            "|    total_timesteps | 53690 |\n",
            "------------------------------\n",
            "Eval num_timesteps=54000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00157 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 54000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00542  |\n",
            "|    critic_loss     | 8.57e-07 |\n",
            "|    ent_coef        | 0.000105 |\n",
            "|    ent_coef_loss   | -0.00466 |\n",
            "|    learning_rate   | 0.00851  |\n",
            "|    n_updates       | 53440    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 912      |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 1079     |\n",
            "|    total_timesteps | 54280    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00545  |\n",
            "|    critic_loss     | 9.7e-07  |\n",
            "|    ent_coef        | 7.59e-05 |\n",
            "|    ent_coef_loss   | -0.167   |\n",
            "|    learning_rate   | 0.00849  |\n",
            "|    n_updates       | 53760    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 916   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1079  |\n",
            "|    total_timesteps | 54280 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 920   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1079  |\n",
            "|    total_timesteps | 54280 |\n",
            "------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 924      |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 1088     |\n",
            "|    total_timesteps | 54870    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00538  |\n",
            "|    critic_loss     | 9.29e-07 |\n",
            "|    ent_coef        | 0.000105 |\n",
            "|    ent_coef_loss   | -0.0717  |\n",
            "|    learning_rate   | 0.00846  |\n",
            "|    n_updates       | 54240    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 928   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1088  |\n",
            "|    total_timesteps | 54870 |\n",
            "------------------------------\n",
            "Eval num_timesteps=55000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00147 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 55000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00534  |\n",
            "|    critic_loss     | 8.99e-07 |\n",
            "|    ent_coef        | 7.76e-05 |\n",
            "|    ent_coef_loss   | 0.0136   |\n",
            "|    learning_rate   | 0.00845  |\n",
            "|    n_updates       | 54400    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 932      |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 1104     |\n",
            "|    total_timesteps | 55460    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00531  |\n",
            "|    critic_loss     | 8.26e-07 |\n",
            "|    ent_coef        | 8.66e-05 |\n",
            "|    ent_coef_loss   | -0.209   |\n",
            "|    learning_rate   | 0.00842  |\n",
            "|    n_updates       | 54880    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 936   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1104  |\n",
            "|    total_timesteps | 55460 |\n",
            "------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 940   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1104  |\n",
            "|    total_timesteps | 55460 |\n",
            "------------------------------\n",
            "Eval num_timesteps=56000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 59.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59       |\n",
            "|    mean_reward     | -0.00162 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 56000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.0054   |\n",
            "|    critic_loss     | 8.23e-07 |\n",
            "|    ent_coef        | 9.04e-05 |\n",
            "|    ent_coef_loss   | 0.0788   |\n",
            "|    learning_rate   | 0.00839  |\n",
            "|    n_updates       | 55360    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 944      |\n",
            "|    fps             | 50       |\n",
            "|    time_elapsed    | 1116     |\n",
            "|    total_timesteps | 56050    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 0.00541  |\n",
            "|    critic_loss     | 8.7e-07  |\n",
            "|    ent_coef        | 9.6e-05  |\n",
            "|    ent_coef_loss   | -0.0709  |\n",
            "|    learning_rate   | 0.00838  |\n",
            "|    n_updates       | 55520    |\n",
            "---------------------------------\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    episodes        | 948   |\n",
            "|    fps             | 50    |\n",
            "|    time_elapsed    | 1116  |\n",
            "|    total_timesteps | 56050 |\n",
            "------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[19], line 59\u001b[0m\n\u001b[0;32m     48\u001b[0m eval_callback \u001b[38;5;241m=\u001b[39m EvalCallback(\n\u001b[0;32m     49\u001b[0m     eval_env,\n\u001b[0;32m     50\u001b[0m     best_model_save_path\u001b[38;5;241m=\u001b[39msave_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m     render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     56\u001b[0m )\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Train the model with the callbacks\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal_timesteps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m     62\u001b[0m model\u001b[38;5;241m.\u001b[39msave(save_dir)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stable_baselines3\\sac\\sac.py:307\u001b[0m, in \u001b[0;36mSAC.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfSAC,\n\u001b[0;32m    300\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    306\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfSAC:\n\u001b[1;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:347\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 347\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stable_baselines3\\sac\\sac.py:280\u001b[0m, in \u001b[0;36mSAC.train\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m# Optimize the actor\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 280\u001b[0m \u001b[43mactor_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# Update target networks\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.noise import OrnsteinUhlenbeckActionNoise\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "\n",
        "\n",
        "\n",
        "# Set the global seed for reproducibility\n",
        "set_global_seeds(config['seed'])\n",
        "\n",
        "# Number of parallel environments\n",
        "n_envs = 10\n",
        "\n",
        "# Create the parallel environments and get the disturbances\n",
        "env, disturbances_list = create_parallel_envs(n_envs, config['seed'])\n",
        "\n",
        "# Print the first disturbances\n",
        "print(\"First disturbance used in training:\", disturbances_list[0])\n",
        "\n",
        "# Initialize the SAC model\n",
        "model = SAC(\n",
        "    config['policy'],\n",
        "    env,\n",
        "    learning_rate=0.001,\n",
        "    # lambda progress: cosine_annealing_schedule(progress, min_lr=config['min_lr'], max_lr=config['max_lr']),  # Cosine schedule\n",
        "    # buffer_size=config['buffer_size'],\n",
        "    # batch_size=config['batch_size'],\n",
        "    # gamma=config['gamma'],\n",
        "    # tau=config['tau'],\n",
        "    # ent_coef=config['ent_coef'],\n",
        "    # train_freq=config['train_freq'],\n",
        "    # gradient_steps=config['gradient_steps'],\n",
        "    # learning_starts=config['learning_starts'],\n",
        "    # policy_kwargs=policy_kwargs,  # Use custom policy network structure\n",
        "    seed=seed,\n",
        "    verbose=1,\n",
        "    \n",
        "    # Adding the optimized SAC-specific parameters to the model\n",
        "\n",
        "    # derivative_penalty_weight=config['derivative_penalty_weight'],\n",
        "    # use_direct_penalty=config['use_direct_penalty'],\n",
        "    # allowed_increase_factor=config['allowed_increase_factor'],\n",
        ")\n",
        "\n",
        "# Create the evaluation environment\n",
        "eval_env = create_eval_env(seed)\n",
        "\n",
        "# Create evaluation callback\n",
        "eval_callback = EvalCallback(\n",
        "    eval_env,\n",
        "    best_model_save_path=save_dir,\n",
        "    log_path=save_dir,\n",
        "    eval_freq=config['check_freq'],\n",
        "    n_eval_episodes=config['n_eval_episodes'],\n",
        "    deterministic=True,\n",
        "    render=False\n",
        ")\n",
        "\n",
        "# Train the model with the callbacks\n",
        "model.learn(total_timesteps=config['total_timesteps'], callback=eval_callback)\n",
        "\n",
        "# Save the model\n",
        "model.save(save_dir)\n",
        "\n",
        "# Save the disturbances to a file for future reference\n",
        "disturbances_file_path = os.path.join(save_dir, \"disturbances_used_in_training.txt\")\n",
        "with open(disturbances_file_path, \"w\") as f:\n",
        "    for i, disturbance in enumerate(disturbances_list):\n",
        "        f.write(f\"Disturbance {i+1}: {disturbance}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "6x1u30y1uN2_"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02ab575e267f4264a325dcea50170518": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d397a3c58bdb4f09b7e774b30727d8ae",
              "IPY_MODEL_5e12a8eb338042f8b42b6f9a35813403"
            ],
            "layout": "IPY_MODEL_cc9fd941a0004a3ebb1d5d66a1013aaf"
          }
        },
        "15627f8991b54e8b961444ef8481086b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19a083985d3e466cb8130c07d692901c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29c77ed8695a4287956ec27672fe7b7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b907d9cb030400c8cd9308246d761c1",
              "IPY_MODEL_bf4a59f3065346d6af96d1fc1a2e6385"
            ],
            "layout": "IPY_MODEL_c341e0ddb94f4c0b9066e8dc99136430"
          }
        },
        "32fd32bd040841959acbae2d2e1b8365": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1a6dbe39e8e492baac9782ed9b7230e",
            "placeholder": "​",
            "style": "IPY_MODEL_b74942f112ea4933b0c5ba6176c6b142",
            "value": "0.013 MB of 0.013 MB uploaded\r"
          }
        },
        "35e1b6be6af8413d95e14d6ce68f3f98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32fd32bd040841959acbae2d2e1b8365",
              "IPY_MODEL_52dbfe8dcd8d4a968987b49e4643feeb"
            ],
            "layout": "IPY_MODEL_15627f8991b54e8b961444ef8481086b"
          }
        },
        "483c79836eed4cfab4cfc7320151ef66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "518bd8accca940b284dfc562ca9b823c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "52dbfe8dcd8d4a968987b49e4643feeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2ce0aa2308849fb9a56cbd94aad8fa9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b5526159b14423dbcc1228e4a1f7699",
            "value": 1
          }
        },
        "5b5526159b14423dbcc1228e4a1f7699": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b907d9cb030400c8cd9308246d761c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac046599192d45cfbf5ab150f9fb240d",
            "placeholder": "​",
            "style": "IPY_MODEL_483c79836eed4cfab4cfc7320151ef66",
            "value": "0.674 MB of 0.674 MB uploaded\r"
          }
        },
        "5e12a8eb338042f8b42b6f9a35813403": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae728950ce914f39a035a67662e99bad",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_518bd8accca940b284dfc562ca9b823c",
            "value": 1
          }
        },
        "6446e95478cf4113aad04353ba1c2030": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac046599192d45cfbf5ab150f9fb240d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae728950ce914f39a035a67662e99bad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b74942f112ea4933b0c5ba6176c6b142": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf4a59f3065346d6af96d1fc1a2e6385": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8afd67546254f249e488008aa76f0a8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6446e95478cf4113aad04353ba1c2030",
            "value": 1
          }
        },
        "c2ce0aa2308849fb9a56cbd94aad8fa9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c341e0ddb94f4c0b9066e8dc99136430": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc9fd941a0004a3ebb1d5d66a1013aaf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfb98e5737c9486fb426884c14e5ae1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1a6dbe39e8e492baac9782ed9b7230e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d397a3c58bdb4f09b7e774b30727d8ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19a083985d3e466cb8130c07d692901c",
            "placeholder": "​",
            "style": "IPY_MODEL_cfb98e5737c9486fb426884c14e5ae1f",
            "value": "0.013 MB of 0.013 MB uploaded\r"
          }
        },
        "e8afd67546254f249e488008aa76f0a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

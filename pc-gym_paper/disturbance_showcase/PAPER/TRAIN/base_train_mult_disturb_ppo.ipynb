{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-BcG1g4QbE_"
      },
      "source": [
        "# Cloning the Repository (pc-gym)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxWOGAsaECz2",
        "outputId": "899c63bc-c202-4dbc-eb5e-9de200616a0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Volume in drive C is Windows \n",
            " Volume Serial Number is F0B8-32A3\n",
            "\n",
            " Directory of c:\\Users\\Usuario\\Desktop\\imperial_projects\\VSCode\\pcgym2\\train\\base\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "File Not Found\n"
          ]
        }
      ],
      "source": [
        "!dir pc-gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVv4nhKnJZyQ",
        "outputId": "0c6c2f7d-37db-4485-a590-41299d74c51c"
      },
      "outputs": [],
      "source": [
        "# %cd pc-gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pXeY_oDVTPZ"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhR6YfBEbVRM",
        "outputId": "a67140ae-cfff-4a16-fbd8-dc7974929319"
      },
      "outputs": [],
      "source": [
        "# %cd src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %cd pcgym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Usuario\\Desktop\\imperial_projects\\VSCode\\pcgym2\\pc-gym\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\Desktop\\imperial_projects\\VSCode\\pcgym2\\new_venv\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "%cd C:/Users/Usuario/Desktop/imperial_projects/VSCode/pcgym2/pc-gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0xr48fylffH",
        "outputId": "504b6be1-dc60-4c93-f087-158249487b25"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\Desktop\\imperial_projects\\VSCode\\pcgym2\\new_venv\\Lib\\site-packages\\do_mpc\\sysid\\__init__.py:15: UserWarning: The ONNX feature is not available. Please install the full version of do-mpc to access this feature.\n",
            "  warnings.warn('The ONNX feature is not available. Please install the full version of do-mpc to access this feature.')\n",
            "c:\\Users\\Usuario\\Desktop\\imperial_projects\\VSCode\\pcgym2\\new_venv\\Lib\\site-packages\\do_mpc\\opcua\\__init__.py:14: UserWarning: The opcua feature is not available. Please install the full version of do-mpc to access this feature.\n",
            "  warnings.warn('The opcua feature is not available. Please install the full version of do-mpc to access this feature.')\n"
          ]
        }
      ],
      "source": [
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "from stable_baselines3 import PPO,SAC,DDPG,TD3\n",
        "import pcgym\n",
        "from pcgym import make_env\n",
        "import jax.numpy as jnp\n",
        "#Global params\n",
        "T = 26\n",
        "nsteps = 101\n",
        "# Global seed for reproducibility\n",
        "seed = 1990"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMPAn1SRd32f"
      },
      "source": [
        "# Saving and loading\n",
        "\n",
        "Saving and loading stable-baselines models is straightforward: you can directly call `.save()` and `.load()` on the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mph7ddv8iCjY",
        "outputId": "fc007c61-f2cd-4545-d8ee-15f900a0eb6c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Create save dir\n",
        "save_dir = \"C:/Users/Usuario/Desktop/imperial_projects/VSCode/pcgym2/train/base/ppo\"\n",
        "os.makedirs(save_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPlCXGwWFPTI"
      },
      "source": [
        "# Monitoring experiments with W&B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8sM9B5fOG_w",
        "outputId": "6c14e127-7339-4421-9a3b-f4c37cd16d6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Usuario\\.netrc\n"
          ]
        }
      ],
      "source": [
        "!wandb login 84af17cc9914cf1736f3a8e2733a2f361e4750bb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sWhTer-Wg7X"
      },
      "source": [
        "# 1.1 Reactor Case Study"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZklHwMtzkOAE",
        "outputId": "209ef879-e2cb-48ac-ce44-0c2300707462"
      },
      "outputs": [],
      "source": [
        "# @title Function to log the performance data\n",
        "def log_performance(performance, test_label, file_path):\n",
        "    with open(file_path, \"a\") as file:\n",
        "        file.write(f\"{test_label}: \\n\")\n",
        "        file.write(f\"scalarised_performance: {performance}\\n\\n\")\n",
        "\n",
        "file_path = f\"{save_dir}/lcb_metric_safe.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OtCnIncfray"
      },
      "source": [
        "### RL training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch as th\n",
        "from torch import nn  # Import torch.nn for activation functions\n",
        "\n",
        "# Learning rate decay schedule\n",
        "def cosine_annealing_schedule(progress_remaining: float, num_cycles=1, min_lr=0.005, max_lr=0.01):\n",
        "    progress = 1.0 - progress_remaining\n",
        "    lr = min_lr + (max_lr - min_lr) / 2 * (1 + np.cos(np.pi * num_cycles * progress))\n",
        "    return lr\n",
        "\n",
        "# Configuration for reinforcement learning model\n",
        "config = {\n",
        "    \"policy\": 'MlpPolicy',  # default: MlpPolicy\n",
        "    \"seed\": 1990,\n",
        "    \"check_freq\": 100,  # base: 12000 (~100 episodes)\n",
        "    \"n_eval_episodes\": 10,  # evaluate the agent over 100 episodes in the evaluation environment\n",
        "    \"positive_definiteness_penalty_weight\": 0,  # Set to 0 initially\n",
        "    \"derivative_penalty_weight\": 0,  # Set to 0 initially\n",
        "    'use_direct_penalty': False,  # choose between applying a penalty directly to the critic loss or adjusting the Q target values (derivative penalty)\n",
        "    'allowed_increase_factor': 1,  # max increase value for both methods (derivative penalty)\n",
        "    'total_timesteps': 500000,\n",
        "}\n",
        "\n",
        "# The best hyperparameters found in previous runs\n",
        "best_params = {\n",
        "    'min_lr': 1.790271571814614e-05,\n",
        "    'max_lr': 0.021830951495112202,\n",
        "    'pi_layer_0_units': 5,\n",
        "    'pi_layer_1_units': 3,\n",
        "    'qf_layer_0_units': 6,\n",
        "    'qf_layer_1_units': 2,\n",
        "    'activation_fn': 'ReLU',\n",
        "    'n_steps': 2048,\n",
        "    'batch_size': 128,\n",
        "    'n_epochs': 11,\n",
        "    'ent_coef': 2.847818037607558e-06,\n",
        "    'clip_range': 0.23324165337690744,\n",
        "    'gamma': 0.9095905120763823,\n",
        "    'gae_lambda': 0.8795346055547795,\n",
        "    'vf_coef': 0.17896972094727606,\n",
        "    'max_grad_norm': 0.49101636537830917,\n",
        "    'total_timesteps': 500000\n",
        "}\n",
        "\n",
        "# Update the config dictionary with the best parameters\n",
        "config.update(best_params)\n",
        "\n",
        "# Set the activation function directly based on best_params\n",
        "if best_params['activation_fn'] == 'Tanh':\n",
        "    activation_fn = th.nn.Tanh\n",
        "elif best_params['activation_fn'] == 'ReLU':\n",
        "    activation_fn = th.nn.ReLU\n",
        "elif best_params['activation_fn'] == 'LeakyReLU':\n",
        "    activation_fn = th.nn.LeakyReLU\n",
        "else:\n",
        "    raise ValueError(\"Unsupported activation function\")\n",
        "\n",
        "# # Create policy_kwargs with the fixed network architecture and activation function from best_params\n",
        "policy_kwargs = dict(\n",
        "    activation_fn=activation_fn,  # Use the activation function directly\n",
        "    net_arch=dict(\n",
        "        pi=[2 ** best_params['pi_layer_0_units'], 2 ** best_params['pi_layer_1_units']],\n",
        "        qf=[2 ** best_params['qf_layer_0_units'], 2 ** best_params['qf_layer_1_units']]\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9qGhyfYRQMZ",
        "outputId": "0ef37754-e355-49a2-9785-2604e4a68a71"
      },
      "outputs": [],
      "source": [
        "import torch as th\n",
        "import numpy as np\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.policies import ActorCriticPolicy\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "##################################################################################\n",
        "# Environment and RL Definition\n",
        "##################################################################################\n",
        "\n",
        "# Enter required setpoints for each state. Enter None for states without setpoints.\n",
        "SP = {\n",
        "    'T': [324.475443431599 for _ in range(5)] + [340.0 for _ in range(nsteps - 5)],\n",
        "}\n",
        "\n",
        "# Continuous box action space\n",
        "action_space = {\n",
        "    'low': np.array([250]),\n",
        "    'high': np.array([350])\n",
        "}\n",
        "\n",
        "# Continuous box observation space ([CA, T, CA_Setpoint, T_Setpoint])\n",
        "observation_space = {\n",
        "    'low': np.array([0.0, 200, 300]),\n",
        "    'high': np.array([1, 600, 400])\n",
        "}\n",
        "\n",
        "r_scale = {\n",
        "    'T': 1e-6  # Reward scale for each state,\n",
        "}\n",
        "\n",
        "# Define disturbance bounds\n",
        "disturbance_bounds = {\n",
        "    'low': np.array([310]),\n",
        "    'high': np.array([390])\n",
        "}\n",
        "\n",
        "# Environment parameters\n",
        "env_params_template = {\n",
        "    'Nx': 2,\n",
        "    'N': 100,\n",
        "    'tsim': 26,\n",
        "    'Nu': 1,\n",
        "    'SP': SP,\n",
        "    'o_space': observation_space,\n",
        "    'a_space': action_space,\n",
        "    'x0': np.array([0.87725294608097, 324.475443431599, 324.475443431599]),\n",
        "    'model': 'cstr',\n",
        "    'r_scale': r_scale,\n",
        "    'normalise_a': True,\n",
        "    'normalise_o': True,\n",
        "    'noise': True,\n",
        "    'integration_method': 'casadi',\n",
        "    'noise_percentage': 0.001,  # 0.001,\n",
        "    'disturbance_bounds': disturbance_bounds\n",
        "}\n",
        "\n",
        "# Add noise_percentage from env_params to config\n",
        "config['noise_percentage'] = env_params_template['noise_percentage']\n",
        "\n",
        "# Seed everything for reproducibility\n",
        "def set_global_seeds(seed):\n",
        "    np.random.seed(seed)\n",
        "    th.manual_seed(seed)\n",
        "    if th.cuda.is_available():\n",
        "        th.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "# Function to create random disturbances\n",
        "def create_random_disturbances(seed, nsteps, low=315, high=385):\n",
        "    # Set the global seed for reproducibility\n",
        "    set_global_seeds(seed)\n",
        "    values = np.random.uniform(low, high, 3)  # Generate three random disturbance values within the specified range\n",
        "    times = np.sort(np.random.choice(range(1, nsteps - 1), 2, replace=False))  # Select two unique time steps for disturbances\n",
        "    times = np.append(times, nsteps)  # Append the total number of steps to get three periods\n",
        "    times = np.diff([0] + times.tolist())  # Calculate the duration of each disturbance period\n",
        "    disturbances = {'Ti': np.repeat(values, times)}  # Repeat the disturbance values according to the calculated durations\n",
        "    return disturbances\n",
        "\n",
        "# Create multiple environments with different disturbances\n",
        "def create_parallel_envs(n_envs, seed):\n",
        "    set_global_seeds(seed)\n",
        "    envs = []\n",
        "    disturbances_list = []\n",
        "    for i in range(n_envs):\n",
        "        env_params = env_params_template.copy()\n",
        "        disturbances = create_random_disturbances(seed + i, nsteps)\n",
        "        env_params.update({'disturbances': disturbances})\n",
        "        disturbances_list.append(disturbances)\n",
        "        envs.append(lambda: make_env(env_params))\n",
        "    return DummyVecEnv(envs), disturbances_list\n",
        "\n",
        "# Create evaluation environment using DummyVecEnv\n",
        "def create_eval_env(seed, n_envs=1):\n",
        "    set_global_seeds(seed)\n",
        "    envs = []\n",
        "    for i in range(n_envs):\n",
        "        env_params = env_params_template.copy()\n",
        "        disturbances = create_random_disturbances(seed + i, nsteps)\n",
        "        env_params.update({'disturbances': disturbances})\n",
        "        envs.append(lambda: make_env(env_params))\n",
        "    return DummyVecEnv(envs)\n",
        "\n",
        "eval_env = create_eval_env(seed=config['seed'], n_envs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "29c77ed8695a4287956ec27672fe7b7a",
            "5b907d9cb030400c8cd9308246d761c1",
            "bf4a59f3065346d6af96d1fc1a2e6385",
            "c341e0ddb94f4c0b9066e8dc99136430",
            "ac046599192d45cfbf5ab150f9fb240d",
            "483c79836eed4cfab4cfc7320151ef66",
            "e8afd67546254f249e488008aa76f0a8",
            "6446e95478cf4113aad04353ba1c2030"
          ]
        },
        "id": "ZhPjWJ7AfjMs",
        "outputId": "eef1f821-6644-4a6c-9131-889a6f18b697"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjosetorraca\u001b[0m (\u001b[33mlades\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.18.2 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\Usuario\\Desktop\\imperial_projects\\VSCode\\pcgym2\\pc-gym\\wandb\\run-20240927_220336-imnmxfvd</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lades/pc-gym-paper-v3/runs/imnmxfvd' target=\"_blank\">train-base-PPO-mult-disturb-tot_500000</a></strong> to <a href='https://wandb.ai/lades/pc-gym-paper-v3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/lades/pc-gym-paper-v3' target=\"_blank\">https://wandb.ai/lades/pc-gym-paper-v3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/lades/pc-gym-paper-v3/runs/imnmxfvd' target=\"_blank\">https://wandb.ai/lades/pc-gym-paper-v3/runs/imnmxfvd</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First disturbance used in training: {'Ti': array([365.53840802, 365.53840802, 365.53840802, 365.53840802,\n",
            "       365.53840802, 365.53840802, 365.53840802, 365.53840802,\n",
            "       365.53840802, 365.53840802, 365.53840802, 365.53840802,\n",
            "       365.53840802, 365.53840802, 365.53840802, 365.53840802,\n",
            "       365.53840802, 320.42850404, 320.42850404, 320.42850404,\n",
            "       320.42850404, 320.42850404, 320.42850404, 320.42850404,\n",
            "       320.42850404, 320.42850404, 320.42850404, 320.42850404,\n",
            "       320.42850404, 320.42850404, 320.42850404, 320.42850404,\n",
            "       320.42850404, 320.42850404, 320.42850404, 320.42850404,\n",
            "       320.42850404, 320.42850404, 320.42850404, 320.42850404,\n",
            "       320.42850404, 320.42850404, 320.42850404, 320.42850404,\n",
            "       320.42850404, 320.42850404, 320.42850404, 320.42850404,\n",
            "       320.42850404, 320.42850404, 320.42850404, 320.42850404,\n",
            "       321.83357664, 321.83357664, 321.83357664, 321.83357664,\n",
            "       321.83357664, 321.83357664, 321.83357664, 321.83357664,\n",
            "       321.83357664, 321.83357664, 321.83357664, 321.83357664,\n",
            "       321.83357664, 321.83357664, 321.83357664, 321.83357664,\n",
            "       321.83357664, 321.83357664, 321.83357664, 321.83357664,\n",
            "       321.83357664, 321.83357664, 321.83357664, 321.83357664,\n",
            "       321.83357664, 321.83357664, 321.83357664, 321.83357664,\n",
            "       321.83357664, 321.83357664, 321.83357664, 321.83357664,\n",
            "       321.83357664, 321.83357664, 321.83357664, 321.83357664,\n",
            "       321.83357664, 321.83357664, 321.83357664, 321.83357664,\n",
            "       321.83357664, 321.83357664, 321.83357664, 321.83357664,\n",
            "       321.83357664, 321.83357664, 321.83357664, 321.83357664,\n",
            "       321.83357664])}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Usuario\\Desktop\\imperial_projects\\VSCode\\pcgym2\\new_venv\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval num_timesteps=1000, episode_reward=-0.11 +/- 0.01\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=2000, episode_reward=-0.11 +/- 0.01\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=3000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=4000, episode_reward=-0.11 +/- 0.01\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=5000, episode_reward=-0.11 +/- 0.01\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=6000, episode_reward=-0.12 +/- 0.01\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=7000, episode_reward=-0.12 +/- 0.01\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=8000, episode_reward=-0.11 +/- 0.01\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=9000, episode_reward=-0.11 +/- 0.01\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=10000, episode_reward=-0.11 +/- 0.01\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=11000, episode_reward=-0.11 +/- 0.01\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=12000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=13000, episode_reward=-0.12 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=14000, episode_reward=-0.12 +/- 0.01\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=15000, episode_reward=-0.12 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=16000, episode_reward=-0.11 +/- 0.01\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=17000, episode_reward=-0.11 +/- 0.01\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=18000, episode_reward=-0.11 +/- 0.01\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=19000, episode_reward=-0.11 +/- 0.01\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=20000, episode_reward=-0.11 +/- 0.01\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=21000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=22000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=23000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=24000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=25000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=26000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=27000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=28000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=29000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=30000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=31000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=32000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=33000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=34000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=35000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=36000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=37000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=38000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=39000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=40000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=41000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=42000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=43000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=44000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=45000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=46000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=47000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=48000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=49000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=50000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=51000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=52000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=53000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=54000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=55000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=56000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=57000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=58000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=59000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=60000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=61000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=62000, episode_reward=-0.17 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=63000, episode_reward=-0.17 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=64000, episode_reward=-0.17 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=65000, episode_reward=-0.17 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=66000, episode_reward=-0.17 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=67000, episode_reward=-0.17 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=68000, episode_reward=-0.17 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=69000, episode_reward=-0.17 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=70000, episode_reward=-0.17 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=71000, episode_reward=-0.17 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=72000, episode_reward=-0.17 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=73000, episode_reward=-0.17 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=74000, episode_reward=-0.17 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=75000, episode_reward=-0.17 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=76000, episode_reward=-0.17 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=77000, episode_reward=-0.17 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=78000, episode_reward=-0.17 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=79000, episode_reward=-0.17 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=80000, episode_reward=-0.17 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=81000, episode_reward=-0.17 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=82000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=83000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=84000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=85000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=86000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=87000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=88000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=89000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=90000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=91000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=92000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=93000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=94000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=95000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=96000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=97000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=98000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=99000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=100000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=101000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=102000, episode_reward=-0.16 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=103000, episode_reward=-0.15 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=104000, episode_reward=-0.15 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=105000, episode_reward=-0.15 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=106000, episode_reward=-0.15 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=107000, episode_reward=-0.15 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=108000, episode_reward=-0.15 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=109000, episode_reward=-0.15 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=110000, episode_reward=-0.15 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=111000, episode_reward=-0.15 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=112000, episode_reward=-0.15 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=113000, episode_reward=-0.15 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=114000, episode_reward=-0.15 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=115000, episode_reward=-0.15 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=116000, episode_reward=-0.15 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=117000, episode_reward=-0.15 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=118000, episode_reward=-0.15 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=119000, episode_reward=-0.15 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=120000, episode_reward=-0.15 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=121000, episode_reward=-0.15 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=122000, episode_reward=-0.15 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=123000, episode_reward=-0.13 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=124000, episode_reward=-0.13 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=125000, episode_reward=-0.13 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=126000, episode_reward=-0.13 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=127000, episode_reward=-0.13 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=128000, episode_reward=-0.13 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=129000, episode_reward=-0.13 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=130000, episode_reward=-0.13 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=131000, episode_reward=-0.13 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=132000, episode_reward=-0.13 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=133000, episode_reward=-0.13 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=134000, episode_reward=-0.13 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=135000, episode_reward=-0.13 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=136000, episode_reward=-0.13 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=137000, episode_reward=-0.13 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=138000, episode_reward=-0.13 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=139000, episode_reward=-0.13 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=140000, episode_reward=-0.13 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=141000, episode_reward=-0.13 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=142000, episode_reward=-0.13 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=143000, episode_reward=-0.13 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=144000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=145000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=146000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=147000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=148000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=149000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=150000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=151000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=152000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=153000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=154000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=155000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=156000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=157000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=158000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=159000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=160000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=161000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=162000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=163000, episode_reward=-0.11 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=164000, episode_reward=-0.08 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=165000, episode_reward=-0.08 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=166000, episode_reward=-0.08 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=167000, episode_reward=-0.08 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=168000, episode_reward=-0.08 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=169000, episode_reward=-0.08 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=170000, episode_reward=-0.08 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=171000, episode_reward=-0.08 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=172000, episode_reward=-0.08 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=173000, episode_reward=-0.08 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=174000, episode_reward=-0.08 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=175000, episode_reward=-0.08 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=176000, episode_reward=-0.08 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=177000, episode_reward=-0.08 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=178000, episode_reward=-0.08 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=179000, episode_reward=-0.08 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=180000, episode_reward=-0.08 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=181000, episode_reward=-0.08 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=182000, episode_reward=-0.08 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=183000, episode_reward=-0.08 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=184000, episode_reward=-0.08 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=185000, episode_reward=-0.06 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=186000, episode_reward=-0.06 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=187000, episode_reward=-0.06 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=188000, episode_reward=-0.06 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=189000, episode_reward=-0.06 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=190000, episode_reward=-0.06 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=191000, episode_reward=-0.06 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=192000, episode_reward=-0.06 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=193000, episode_reward=-0.06 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=194000, episode_reward=-0.06 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=195000, episode_reward=-0.06 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=196000, episode_reward=-0.06 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=197000, episode_reward=-0.06 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=198000, episode_reward=-0.06 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=199000, episode_reward=-0.06 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=200000, episode_reward=-0.06 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=201000, episode_reward=-0.06 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=202000, episode_reward=-0.06 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=203000, episode_reward=-0.06 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=204000, episode_reward=-0.06 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=205000, episode_reward=-0.04 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=206000, episode_reward=-0.04 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=207000, episode_reward=-0.04 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=208000, episode_reward=-0.04 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=209000, episode_reward=-0.04 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=210000, episode_reward=-0.04 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=211000, episode_reward=-0.04 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=212000, episode_reward=-0.04 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=213000, episode_reward=-0.04 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=214000, episode_reward=-0.04 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=215000, episode_reward=-0.04 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=216000, episode_reward=-0.04 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=217000, episode_reward=-0.04 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=218000, episode_reward=-0.04 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=219000, episode_reward=-0.04 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=220000, episode_reward=-0.04 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=221000, episode_reward=-0.04 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=222000, episode_reward=-0.04 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=223000, episode_reward=-0.04 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=224000, episode_reward=-0.04 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=225000, episode_reward=-0.04 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=226000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=227000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=228000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=229000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=230000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=231000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=232000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=233000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=234000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=235000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=236000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=237000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=238000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=239000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=240000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=241000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=242000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=243000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=244000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=245000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=246000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=247000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=248000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=249000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=250000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=251000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=252000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=253000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=254000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=255000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=256000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=257000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=258000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=259000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=260000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=261000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=262000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=263000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=264000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=265000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=266000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=267000, episode_reward=-0.02 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=268000, episode_reward=-0.02 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=269000, episode_reward=-0.02 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=270000, episode_reward=-0.02 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=271000, episode_reward=-0.02 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=272000, episode_reward=-0.02 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=273000, episode_reward=-0.02 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=274000, episode_reward=-0.02 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=275000, episode_reward=-0.02 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=276000, episode_reward=-0.02 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=277000, episode_reward=-0.02 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=278000, episode_reward=-0.02 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=279000, episode_reward=-0.02 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=280000, episode_reward=-0.02 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=281000, episode_reward=-0.02 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=282000, episode_reward=-0.02 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=283000, episode_reward=-0.02 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=284000, episode_reward=-0.02 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=285000, episode_reward=-0.02 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=286000, episode_reward=-0.02 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=287000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=288000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=289000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=290000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=291000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=292000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=293000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=294000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=295000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=296000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=297000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=298000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=299000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=300000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=301000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=302000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=303000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=304000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=305000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=306000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=307000, episode_reward=-0.03 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=308000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=309000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=310000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=311000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=312000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=313000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=314000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=315000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=316000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=317000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=318000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=319000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=320000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=321000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=322000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=323000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=324000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=325000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=326000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=327000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=328000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=329000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=330000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=331000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=332000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=333000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=334000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=335000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=336000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=337000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=338000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=339000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=340000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=341000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=342000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=343000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=344000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=345000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=346000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=347000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=348000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=349000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=350000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=351000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=352000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=353000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=354000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=355000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=356000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=357000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=358000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=359000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=360000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=361000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=362000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=363000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=364000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=365000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=366000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=367000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=368000, episode_reward=-0.01 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=369000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=370000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=371000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=372000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=373000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=374000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=375000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=376000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=377000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=378000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=379000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=380000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=381000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=382000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=383000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=384000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=385000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=386000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=387000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=388000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=389000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=390000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=391000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=392000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=393000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=394000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=395000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=396000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=397000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=398000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=399000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=400000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=401000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=402000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=403000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=404000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=405000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=406000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=407000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=408000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=409000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=410000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=411000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=412000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=413000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=414000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=415000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=416000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=417000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=418000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=419000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=420000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=421000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=422000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=423000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=424000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=425000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=426000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=427000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=428000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=429000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=430000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=431000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=432000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=433000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=434000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=435000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=436000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=437000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=438000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=439000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=440000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=441000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=442000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=443000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=444000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=445000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=446000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=447000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=448000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=449000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=450000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=451000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=452000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=453000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=454000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=455000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=456000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=457000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=458000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=459000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=460000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=461000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=462000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=463000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=464000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=465000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=466000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=467000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=468000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=469000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=470000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=471000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=472000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=473000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=474000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=475000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=476000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=477000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=478000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=479000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=480000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=481000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=482000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=483000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=484000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=485000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=486000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=487000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=488000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=489000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=490000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=491000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=492000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=493000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=494000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=495000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=496000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=497000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=498000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=499000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=500000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=501000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=502000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=503000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=504000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=505000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=506000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=507000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=508000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=509000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=510000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=511000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n",
            "Eval num_timesteps=512000, episode_reward=-0.00 +/- 0.00\n",
            "Episode length: 99.00 +/- 0.00\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_ep_length</td><td></td></tr><tr><td>eval/mean_reward</td><td></td></tr><tr><td>global_step</td><td></td></tr><tr><td>time/fps</td><td></td></tr><tr><td>train/approx_kl</td><td></td></tr><tr><td>train/clip_fraction</td><td></td></tr><tr><td>train/clip_range</td><td></td></tr><tr><td>train/entropy_loss</td><td></td></tr><tr><td>train/explained_variance</td><td></td></tr><tr><td>train/learning_rate</td><td></td></tr><tr><td>train/loss</td><td></td></tr><tr><td>train/policy_gradient_loss</td><td></td></tr><tr><td>train/std</td><td></td></tr><tr><td>train/value_function</td><td></td></tr><tr><td>train/value_function_derivative</td><td></td></tr><tr><td>train/value_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_ep_length</td><td>99.0</td></tr><tr><td>eval/mean_reward</td><td>-0.00166</td></tr><tr><td>global_step</td><td>512000</td></tr><tr><td>time/fps</td><td>549.0</td></tr><tr><td>train/approx_kl</td><td>0.01942</td></tr><tr><td>train/clip_fraction</td><td>0.14898</td></tr><tr><td>train/clip_range</td><td>0.23324</td></tr><tr><td>train/entropy_loss</td><td>0.85636</td></tr><tr><td>train/explained_variance</td><td>0.10615</td></tr><tr><td>train/learning_rate</td><td>3e-05</td></tr><tr><td>train/loss</td><td>-0.00188</td></tr><tr><td>train/policy_gradient_loss</td><td>-0.02665</td></tr><tr><td>train/std</td><td>0.10051</td></tr><tr><td>train/value_function</td><td>-0.00036</td></tr><tr><td>train/value_function_derivative</td><td>0.0</td></tr><tr><td>train/value_loss</td><td>0.0</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">train-base-PPO-mult-disturb-tot_500000</strong> at: <a href='https://wandb.ai/lades/pc-gym-paper-v3/runs/imnmxfvd' target=\"_blank\">https://wandb.ai/lades/pc-gym-paper-v3/runs/imnmxfvd</a><br/> View project at: <a href='https://wandb.ai/lades/pc-gym-paper-v3' target=\"_blank\">https://wandb.ai/lades/pc-gym-paper-v3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240927_220336-imnmxfvd\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import wandb\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "# Initialize W&B\n",
        "wandb.init(\n",
        "    config=config,\n",
        "    sync_tensorboard=True,  # Automatically upload SB3's tensorboard metrics\n",
        "    project=\"pc-gym-paper-v3\",\n",
        "    name=f\"train-base-PPO-mult-disturb-tot_{config['total_timesteps']}\",\n",
        ")\n",
        "\n",
        "# Set the global seed for reproducibility\n",
        "set_global_seeds(config['seed'])\n",
        "\n",
        "# Number of parallel environments\n",
        "n_envs = 10\n",
        "\n",
        "# Create the parallel environments and get the disturbances\n",
        "env, disturbances_list = create_parallel_envs(n_envs, config['seed'])\n",
        "\n",
        "# Print the first disturbances\n",
        "print(\"First disturbance used in training:\", disturbances_list[0])\n",
        "\n",
        "# Training the model\n",
        "model = PPO(\n",
        "    config['policy'],\n",
        "    env,\n",
        "    learning_rate=lambda progress: cosine_annealing_schedule(progress, min_lr=config['min_lr'], max_lr=config['max_lr']),  # Cosine schedule\n",
        "    n_steps=config['n_steps'],\n",
        "    batch_size=config['batch_size'],  # PPO uses batch_size for mini-batch updates\n",
        "    gamma=config['gamma'],\n",
        "    gae_lambda=config['gae_lambda'],\n",
        "    ent_coef=config['ent_coef'],\n",
        "    vf_coef=config['vf_coef'],\n",
        "    max_grad_norm=config['max_grad_norm'],\n",
        "    clip_range=config['clip_range'],\n",
        "    n_epochs=config['n_epochs'],  # Now optimizing number of epochs\n",
        "    policy_kwargs=policy_kwargs,  # Use custom policy network structure\n",
        "    seed=seed,\n",
        "    verbose=0,\n",
        "    tensorboard_log=f\"runs/ppo\",\n",
        "    # Adding the optimized PPO-specific parameters to the model\n",
        "    positive_definiteness_penalty_weight=config['positive_definiteness_penalty_weight'],\n",
        "    derivative_penalty_weight=config['derivative_penalty_weight'],\n",
        ")\n",
        "\n",
        "# Create the evaluation environment\n",
        "eval_env = create_eval_env(seed)\n",
        "\n",
        "# Create evaluation callback\n",
        "eval_callback = EvalCallback(\n",
        "    eval_env,\n",
        "    best_model_save_path=save_dir,\n",
        "    log_path=save_dir,\n",
        "    eval_freq=config['check_freq'],\n",
        "    n_eval_episodes=config['n_eval_episodes'],\n",
        "    deterministic=True,\n",
        "    render=False\n",
        ")\n",
        "\n",
        "# Train the model with the callbacks\n",
        "model.learn(total_timesteps=config['total_timesteps'], callback=eval_callback)\n",
        "\n",
        "# Save the model\n",
        "model.save(save_dir)\n",
        "\n",
        "# Save the disturbances to a file for future reference\n",
        "disturbances_file_path = os.path.join(save_dir, \"disturbances_used_in_training.txt\")\n",
        "with open(disturbances_file_path, \"w\") as f:\n",
        "    for i, disturbance in enumerate(disturbances_list):\n",
        "        f.write(f\"Disturbance {i+1}: {disturbance}\\n\")\n",
        "\n",
        "# Finish the W&B run\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "6x1u30y1uN2_"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02ab575e267f4264a325dcea50170518": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d397a3c58bdb4f09b7e774b30727d8ae",
              "IPY_MODEL_5e12a8eb338042f8b42b6f9a35813403"
            ],
            "layout": "IPY_MODEL_cc9fd941a0004a3ebb1d5d66a1013aaf"
          }
        },
        "15627f8991b54e8b961444ef8481086b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19a083985d3e466cb8130c07d692901c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29c77ed8695a4287956ec27672fe7b7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b907d9cb030400c8cd9308246d761c1",
              "IPY_MODEL_bf4a59f3065346d6af96d1fc1a2e6385"
            ],
            "layout": "IPY_MODEL_c341e0ddb94f4c0b9066e8dc99136430"
          }
        },
        "32fd32bd040841959acbae2d2e1b8365": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1a6dbe39e8e492baac9782ed9b7230e",
            "placeholder": "",
            "style": "IPY_MODEL_b74942f112ea4933b0c5ba6176c6b142",
            "value": "0.013 MB of 0.013 MB uploaded\r"
          }
        },
        "35e1b6be6af8413d95e14d6ce68f3f98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32fd32bd040841959acbae2d2e1b8365",
              "IPY_MODEL_52dbfe8dcd8d4a968987b49e4643feeb"
            ],
            "layout": "IPY_MODEL_15627f8991b54e8b961444ef8481086b"
          }
        },
        "483c79836eed4cfab4cfc7320151ef66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "518bd8accca940b284dfc562ca9b823c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "52dbfe8dcd8d4a968987b49e4643feeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2ce0aa2308849fb9a56cbd94aad8fa9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b5526159b14423dbcc1228e4a1f7699",
            "value": 1
          }
        },
        "5b5526159b14423dbcc1228e4a1f7699": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b907d9cb030400c8cd9308246d761c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac046599192d45cfbf5ab150f9fb240d",
            "placeholder": "",
            "style": "IPY_MODEL_483c79836eed4cfab4cfc7320151ef66",
            "value": "0.674 MB of 0.674 MB uploaded\r"
          }
        },
        "5e12a8eb338042f8b42b6f9a35813403": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae728950ce914f39a035a67662e99bad",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_518bd8accca940b284dfc562ca9b823c",
            "value": 1
          }
        },
        "6446e95478cf4113aad04353ba1c2030": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac046599192d45cfbf5ab150f9fb240d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae728950ce914f39a035a67662e99bad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b74942f112ea4933b0c5ba6176c6b142": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf4a59f3065346d6af96d1fc1a2e6385": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8afd67546254f249e488008aa76f0a8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6446e95478cf4113aad04353ba1c2030",
            "value": 1
          }
        },
        "c2ce0aa2308849fb9a56cbd94aad8fa9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c341e0ddb94f4c0b9066e8dc99136430": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc9fd941a0004a3ebb1d5d66a1013aaf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfb98e5737c9486fb426884c14e5ae1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1a6dbe39e8e492baac9782ed9b7230e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d397a3c58bdb4f09b7e774b30727d8ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19a083985d3e466cb8130c07d692901c",
            "placeholder": "",
            "style": "IPY_MODEL_cfb98e5737c9486fb426884c14e5ae1f",
            "value": "0.013 MB of 0.013 MB uploaded\r"
          }
        },
        "e8afd67546254f249e488008aa76f0a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
